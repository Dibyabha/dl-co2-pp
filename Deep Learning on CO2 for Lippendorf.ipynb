{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd610b0-81ea-47b5-a02b-64c6a3d0ea70",
   "metadata": {},
   "source": [
    "Replicating the paper using CNN regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0cf096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "This cell is the main part of our work where we have replicated the results that are obtained by\n",
    "'Deep learning applied to CO2 power plant emissions quantification using simulated satellite images' (Joffrey Dumont Le Brazidec et al.). Our main\n",
    "aim is to ensure that we have the understanding of the model (which they have mentioned in their paper) and replicate the results. We understand that absolute\n",
    "replication is very difficult as mentioned by author of the paper due to the random initialization of weights, parameters and hyperparameters by optimization\n",
    "algorithms and hence have followed the similar model ensembling approach as mentioned. Note for the reader : In the paper, authors mentioned running the\n",
    "model at a particular location for a particular additional input (None, Segmentation, NO2) for 3 times. Due to computation related issues, we\n",
    "ran it for 2 times and have got reasonable results to believe that our work is quite satisfactorily close to the results mentioned in Table 2 of the paper.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Importing different packages that we will use.\n",
    "\n",
    "\"\"\"\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset as NetCDFDataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Dropout, BatchNormalization, MaxPool2D, Flatten, Dense, GaussianNoise\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "We define our Custom Dataset which will read the .nc files (train, valid, test) and extract the XCO2, u, v, NO2 data from the .nc files.\n",
    "Note to the reader : For our comparision, we are going to consider Lippendorf location and as additional input we will consider NO2 images.\n",
    "\n",
    "\"\"\"\n",
    "class CustomDataset:\n",
    "    \n",
    "    def __init__(self, file_path, variable = None):\n",
    "        self.ncfile = NetCDFDataset(file_path, 'r') # Reading the .nc file\n",
    "        self.variable = variable # Variable in our comparison is no2_noisy\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ncfile.variables['xco2_noisy'].shape[0] # The total number of images or data we will have which is different for train (25152), valid (4608), test (6289)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Reading the necessary variables (xco2_noisy, u, v)\n",
    "        xco2_read = self.ncfile.variables['xco2_noisy'][idx] # Unit : ppmv (parts per million by volume), Shape : 64, 64\n",
    "        u_read = self.ncfile.variables['u'][idx] # Unit : m/s, Shape : 64, 64\n",
    "        v_read = self.ncfile.variables['v'][idx] # Unit : m/s, Shape : 64, 64\n",
    "        emiss_read = self.ncfile.variables['emiss'][idx] # emiss is the true output data that we need to train our model. Unit : Mt/yr (Million tonnes/ year)\n",
    "                                                         # Shape : 3,\n",
    "        # Converting into numpy arrays with data type float32\n",
    "        xco2_arr = np.array(xco2_read.data).astype('float32') \n",
    "        u_arr = np.array(u_read.data).astype('float32')\n",
    "        v_arr = np.array(v_read.data).astype('float32')\n",
    "        emiss_arr = np.array(emiss_read.data).astype('float32')\n",
    "\n",
    "        #noise = GaussianNoise(stddev = 0.7) # Addition of Gaussian Noise with std of 0.7\n",
    "        #xco2 = noise(xco2_arr).numpy() \n",
    "        if self.variable:\n",
    "            var_read = self.ncfile.variables[self.variable][idx] # Unit : molec/cm^2, Shape : 64, 64\n",
    "            var_arr = np.array(var_read.data).astype('float32') # Converting into numpy array after reading the variable\n",
    "            #var_noise = noise(var_tensor).numpy()\n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr, var_arr], axis = -1) # Stacking the inputs to get the desired input shape = (64, 64, 4) where 64, 64\n",
    "                                                                            # represents height, width and 4 represents the total number of features. Each pixel\n",
    "                                                                            # has a spatial resolution of 2km, i.e. 1*1 in pixel refers to an area of 2km*2km.\n",
    "        else: \n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr], axis = -1) # Stacking the inputs to get the shape = (64, 64, 3) in case when variable is None.\n",
    "        \n",
    "        mean = inputs.mean(axis = (0, 1), keepdims = True) # Calculating the mean of the 4 features separately and keeping the same shape as that of inputs.\n",
    "        \n",
    "        std = inputs.std(axis = (0, 1), keepdims = True) # Calculating the std of the 4 features separately and keeping the same shape as that of inputs.\n",
    "        \n",
    "        std[std==0] = 1 # If std becomes 0, we change it 1 to avoid division by 0. Multiple other methods can also be employed such as choosing an \n",
    "                        # appropriate epsilon such as 1e-5, 1e-6 or 1e-7\n",
    "        \n",
    "        inputs = (inputs - mean)/std # Normalization of the inputs separately for each feature.\n",
    "        \n",
    "        # emiss data is of shape (3,), from which we will only consider the middle value for our comparison. Hence, we multiply weights with emiss to\n",
    "        # get the middle value and round it off to 3 decimal places.\n",
    "        weights = np.array([0.0, 1.0, 0.0]) \n",
    "        weighted = np.round(np.sum(weights * emiss_arr), 3)\n",
    "        outputs = np.array([weighted], dtype = np.float32)\n",
    "        \n",
    "        return inputs, outputs # returns our inputs (64, 64, 4), outputs (1,)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.ncfile.close() # Closing the ncfile to avoid corruption of the file.\n",
    "\n",
    "\"\"\"\n",
    "The model architecture is taking from the paper which consists of 2D convolution, Maxpooling, Dropout, Batch Normalization, Flatten and finally Dense.\n",
    "\n",
    "\"\"\"\n",
    "def model_arch(input_shape): # input_shape = 64, 64, 4\n",
    "    model = Sequential()\n",
    "    \n",
    "    # The first convolution with kernel size of (3,3), total number of filters = 32, activation function = elu (Exponential Linear Unit), strides = 1, \n",
    "    # padding = valid (no padding).\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1, input_shape = input_shape)) # Shape after conv : 62, 62, 32\n",
    "    \n",
    "    # Adding a Dropout of 0.1\n",
    "    model.add(Dropout(0.1)) # No change in shape\n",
    "    \n",
    "    # Second convolution with kernel size of (3,3), total number of filters = 32, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 60, 60, 32\n",
    "    \n",
    "    # Maxpool with pool size of 2, 2 and strides = 2.\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2)) # Shape after maxpool : 30, 30, 32\n",
    "    \n",
    "    model.add(BatchNormalization()) # Batch Normalization which does not affect the shape\n",
    "    \n",
    "    # Third convolution with kernel size of (3,3), total number of filters = 32, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 28, 28, 32\n",
    "    \n",
    "    # Adding a Dropout of 0.2\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Fourth convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 26, 26, 64\n",
    "    \n",
    "    model.add(BatchNormalization()) # Batch Normalization which does not affect the shape\n",
    "    \n",
    "    # Fifth convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 24, 24, 64\n",
    "    \n",
    "    # Adding a Dropout of 0.2\n",
    "    model.add(Dropout(0.2)) # No change in shape\n",
    "    \n",
    "    # Sixth convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 22, 22, 64\n",
    "    \n",
    "    # Maxpool with pool size of 2, 2 and strides = 2.\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2)) # Shape after maxpool : 11, 11, 64\n",
    "    \n",
    "    # Seventh convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 9, 9, 64\n",
    "    \n",
    "    # Adding a Dropout of 0.2\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Eighth convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 7, 7, 64\n",
    "    \n",
    "    # Maxpool with pool size of 2, 2 and strides = 2.\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2)) # Shape after maxpool : 3, 3, 64\n",
    "    \n",
    "    model.add(Flatten()) # Total = 3*3*64 = 576\n",
    "    \n",
    "    model.add(Dense(1)) # Fully Connected Layer to get a single output.\n",
    "    \n",
    "    model.add(LeakyReLU(alpha = 0.3)) # The output goes through activation function = Leaky Rectified Linear Unit with negative slope = 0.3.\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creating and checking if saved_models folder exists or not. If not, then create a folder where we will store our weights.h5 file.\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "best_model_filepath = os.path.join(checkpoint_dir, 'best_model_weights.h5')\n",
    "# Callbacks can be of best weights or last weights. We can do either one. We went with saving the weights after every epoch. \n",
    "best_model_checkpoint_callback = ModelCheckpoint(filepath = best_model_filepath, save_weights_only = True, verbose = 1)\n",
    "\n",
    "# Applying data augmentation to Training data as specified by the paper.\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "# Filepaths for Train, Valid, Test. From the paper we know that for Train, Valid we will choose data points from everywhere except the location in which\n",
    "# we will test our model's performance. Location = Lippendorf\n",
    "test_set = 'curated/lip_test_dataset.nc'\n",
    "valid_set = 'curated/lip_valid_dataset.nc'\n",
    "train_set = 'curated/lip_train_dataset.nc'\n",
    "\n",
    "# Here, we have generated our custom dataset.\n",
    "train_dataset = CustomDataset(train_set, variable = 'no2_noisy')\n",
    "valid_dataset = CustomDataset(valid_set, variable = 'no2_noisy')\n",
    "test_dataset = CustomDataset(test_set, variable = 'no2_noisy')\n",
    "\n",
    "# To store the inputs and outputs from our dataset to feed it to the model during training, validation and inference.\n",
    "train_inputs = []\n",
    "train_outputs = []\n",
    "val_inputs = []\n",
    "val_outputs = []\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "\n",
    "for inputs, outputs in train_dataset:\n",
    "    train_inputs.append(inputs)\n",
    "    train_outputs.append(outputs)\n",
    "for inputs, outputs in valid_dataset:\n",
    "    val_inputs.append(inputs)\n",
    "    val_outputs.append(outputs)\n",
    "for inputs, outputs in test_dataset:\n",
    "    test_inputs.append(inputs)\n",
    "    test_outputs.append(outputs)\n",
    "\n",
    "# Converting into numpy arrays\n",
    "train_inputs = np.array(train_inputs) # train_inputs shape : (25152, 64, 64, 4)\n",
    "train_outputs = np.array(train_outputs) # train_outputs shape : (25152, 1)\n",
    "val_inputs = np.array(val_inputs) # val_inputs shape : (4608, 64, 64, 4)\n",
    "val_outputs = np.array(val_outputs) # val_outputs shape : (4608, 1)\n",
    "test_inputs = np.array(test_inputs) # test_inputs shape : (6289, 64, 64, 4)\n",
    "test_outputs = np.array(test_outputs) # test_outputs shape : (6289, 1)\n",
    "\n",
    "model = model_arch(input_shape = (64, 64, 4)) # Declaring the model\n",
    "\n",
    "#model.load_weights('saved_models/best_model_weights.h5') # If we have already stored the weights, then uncomment this line to load.\n",
    "\n",
    "optimizer = Adam(learning_rate = 1e-3) # Taking Adam optimizer with lr = 1e-3\n",
    "\n",
    "# We will update our lr if we reach a plateau. For this we use ReduceLROnPlateau which will monitor val_loss, wait for 20 epochs before changing lr and measures \n",
    "# min_delta threshold, reduces lr by 0.5 and lower bound on lr is 5e-5.\n",
    "learning_rate_monitor_callback = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 20, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-5)\n",
    "\n",
    "# Loss function for this task is MeanAbsoluteError and metrics is MeanAbsolutePercentageError.\n",
    "model.compile(optimizer = optimizer, loss = 'mae', metrics = ['mape'])\n",
    "\n",
    "# Generate the summary of the model. Around 186000 trainable paramaters are there.\n",
    "model.summary()\n",
    "\n",
    "# Total epochs = 500, batch size for training set = 32, steps will be 25152/32 = 786, validation will not have any augmentation with batch size of 32 and validating\n",
    "# on the entire validation set.\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 500, steps_per_epoch = len(train_dataset)//32,\n",
    "validation_data = (val_inputs, val_outputs), validation_batch_size = 32, validation_steps = None, callbacks = [learning_rate_monitor_callback, best_model_checkpoint_callback])\n",
    "\n",
    "# To test the model's performance on test set.\n",
    "#test_loss, test_mae = model.evaluate(test_inputs, test_outputs, batch_size = None) # Uncomment these lines to infer model's performance.\n",
    "#print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')\n",
    "\n",
    "# To store the true_emission and predicted emissions for the .csv file.\n",
    "true_emissions = []\n",
    "predicted_emissions = []\n",
    "for inputs, targets in test_dataset:\n",
    "    inputs = np.expand_dims(inputs, axis = 0)\n",
    "    outputs = model.predict(inputs) # Predicting the output based on the model's learning.\n",
    "    print(\"True value : \", targets)\n",
    "    print(\"Predicted value : \", outputs)\n",
    "    true_emissions.append(targets) # Append the true emissions into true_emissions list\n",
    "    predicted_emissions.append(outputs) # Append the predicted emissions into predicted_emissions list\n",
    "\n",
    "# Converting into numpy arrays\n",
    "true_emissions = np.array(true_emissions) # Shape : 6289, 3\n",
    "predicted_emissions = np.array(predicted_emissions) # Shape : 6289, 1, 3\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape) # After reshaping : 6289, 1, 3\n",
    "\n",
    "# Storing the true emissions and predicted emissions in a csv file. \n",
    "df = pd.DataFrame({'True Emissions': true_emissions.flatten(),'Predicted Emissions': predicted_emissions.flatten()}, index = np.arange(1, len(true_emissions) + 1))\n",
    "df.to_csv('emissions_data_lip.csv')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26121d79-dfa4-4448-9766-27274caff1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv('emissions_data_lip.csv') # Reading emissions_data\n",
    "\n",
    "# Creating new column error which is true-pred.\n",
    "data_df['error'] = df['True Emissions'] - df['Predicted Emissions'] \n",
    "\n",
    "# Creating new column absolute_error which is the absolute value of error rounded off to 3 decimal places.\n",
    "data_df['absolute_error'] = (abs(df['error'])).round(3)\n",
    "\n",
    "# Creating new column relative_error which is the relative value of error rounded off to 3 decimal places with epsilon = 1e-15.\n",
    "data_df['relative_error'] = ((df['error'] / (df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "# Updates the csv file\n",
    "data_df.to_csv('emissions_data_lip.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d03465-5383-40cb-a58d-5fe67658d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emissions_data_lip.csv') # Reading emissions_data\n",
    "\n",
    "# We want to know the Q1, Q2, Q3 values or (25%, 50%, 75%) values from our absolute_error distribution.\n",
    "percentiles = df['absolute_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 2.123, 50% : 4.058, 75% : 6.426 (All are in Mt/yr)\n",
    "\n",
    "# Stated in the paper, the median absolute error is 3 Mt/yr. We got 4 Mt/ yr which means we are at a 1 Mt/yr deviation which is quite reasonable.\n",
    "\n",
    "# The true percentile values as mentioned in the paper.\n",
    "true_percentiles = [1.3, 2.7, 4.5] # (All are in Mt/yr)\n",
    "\n",
    "# Absolute error between our percentile values and the true percentile values rounded off to 3 decimal places.\n",
    "error = [round(abs(true - pred), 3) for true, pred in zip(true_percentiles, percentiles)] # 0.823, 1.358, 1.926\n",
    "\n",
    "# This means that our percentile values are within +2 Mt/yr range of the true percentile values. This is pretty reasonable considering the random nature of\n",
    "# initialization of weights, parameters and hyperparameters by the optimization algorithms. \n",
    "\n",
    "print(percentiles)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a609b2b-aec8-4fdb-8461-019c1db90beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emissions_data_lip.csv') # Reading emissions_data\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "# We want to know the Q1, Q2, Q3 values or (25%, 50%, 75%) values from our relative_error distribution.\n",
    "percentiles = df['abs_rel_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "\n",
    "# Only the percentiles are of impportant to us.\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 8.151, 50% : 19.183, 75% : 37.049 (Unitless)\n",
    "\n",
    "# Stated in the paper, the median absolute relative paper is around 20%. We have got 19.183% which can be approximated to 20%.\n",
    "\n",
    "# The true percentile values as mentioned in the paper.\n",
    "true_percentiles = [8.6, 18.1, 30.3] # Note : These values are percentage values as the relative errors are calculated in terms of percentage.\n",
    "\n",
    "# Absolute error between our percentile values and the true percentile values rounded off to 3 decimal places.\n",
    "error = [round(abs(true - pred), 3) for true, pred in zip(true_percentiles, percentiles)] # 0.449, 1.083, 6.749\n",
    "\n",
    "# Again we notice a +2% range deviation (barring only one value) which can be associated to the fact that the model ran for only 2 times compared to 3 times\n",
    "# mentioned in the paper. Also, the random initialization of weights, parameters and hyperparameters by the optimization algorithms may have played a role here.\n",
    "print(percentiles)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41799f3-f7c0-44da-a59c-f38c3f7eff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This cell is for plotting the Kernel Density Estimation (KDE) plots using seaborn of absolute_error and relative_error. \"\"\"\n",
    "\n",
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('emissions_data_lip.csv') # Reading emissions_data\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['absolute_error'], shade = True, color = 'b') # Using seaborn's kdeplot. \n",
    "plt.title('Kernel Density Estimate of Absolute Error')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('absolute_error_plot.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['relative_error'], shade = True, color = 'r') # Using seaborn's kdeplot.\n",
    "plt.title('Kernel Density Estimate of Relative Error (%)')\n",
    "plt.xlabel('Relative Error (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.savefig('relative_error_plot.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad36ac-9dc4-4389-bbdf-5e670e36856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our absolute errors lie in (0-2) Mt/yr range, in (2-5) Mt/yr range, in (5-10) Mt/yr range\n",
    "and in 10 Mt/yr above. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emissions_data.csv')\n",
    "\n",
    "# useful column\n",
    "abs = df['absolute_error']\n",
    "\n",
    "# get the required info\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 1477\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 2358\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 2172\n",
    "print(\"above 10 : \", ab10.sum()) # 282\n",
    "print(\"mean : \", mean) # 4.474\n",
    "print(\"median : \", median) # 4.058\n",
    "print(\"std : \", std) # 2.967\n",
    "\n",
    "# According to the paper, \" Majority of errors are concentrated below 10 Mt/yr.\" which we can see is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e14b6-d51e-42b4-8fbc-0d478ae9764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our relative errors lie less than -150%, in (-150 to -100)% range, in (-100 to -50)% range,\n",
    "in (-50 to 0) %, in (0 to 50) % range, in (50 to 100) % and above 100%. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emissions_data.csv')\n",
    "\n",
    "# useful column\n",
    "rel = df['relative_error']\n",
    "\n",
    "# get the required info\n",
    "lessneg150 = (rel <= -150)\n",
    "neg150_100 = (rel > -150) & (rel <= -100)\n",
    "neg100_50 = (rel > -100) & (rel <= -50)\n",
    "neg50_0 = (rel > -50) & (rel <= 0)\n",
    "in0_50 = (rel > 0) & (rel <= 50)\n",
    "in50_100 = (rel > 50) & (rel <= 100)\n",
    "ab100 = rel > 100\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"less than - 150% : \", lessneg150.sum()) # 1\n",
    "print(\"in between -150% and -100% : \", neg150_100.sum()) # 38\n",
    "print(\"in between -100% and -50% : \", neg100_50.sum()) # 257\n",
    "print(\"in between -50% and 0% : \", neg50_0.sum()) # 1662\n",
    "print(\"in between 0% and 50% : \", in0_50.sum()) # 3892\n",
    "print(\"in between 50% and 100% : \", in50_100.sum()) # 439\n",
    "print(\"above 100% : \", ab100.sum()) # 0\n",
    "print(\"mean : \", mean) # 11.938\n",
    "print(\"median : \", median) # 19.183\n",
    "print(\"std : \", std) # 32.561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437f4d4-0b26-418a-a08f-43ec6448e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our absolute relative errors lie in (0 to 20) % range, in (20 to 50)% range, in (50 to 100) %, in (100 to 150)% \n",
    "and above 150%. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emissions_data.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "# useful column\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "# get the required info\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 1\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 38\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 696\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 3404\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 2150\n",
    "print(\"mean : \", mean) # 29.325\n",
    "print(\"median : \", median) # 28.288\n",
    "print(\"std : \", std) # 18.511\n",
    "\n",
    "# According to the paper, \" Majority of errors are concentrated below 10 Mt/yr or 50% with very few exceeding 100%.\" which we can again see is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6c294-b7b5-4971-9797-c62411ec92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code gets us a few statistics about true emissions and predicted emissions. \"\"\"\n",
    "\n",
    "# necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# read\n",
    "df = pd.read_csv('emissions_data.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Pred_emiss']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 7.493\n",
    "print(\"max : \", max_true) # 24.112\n",
    "print(\"mean : \", mean_true) # 15.256\n",
    "print(\"std : \", std_true) # 3.527\n",
    "print(\"median : \", median_true) # 15.267\n",
    "print(\"range : \", range_true) # 16.619\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 0.739\n",
    "print(\"max : \", max_pred) # 34.250\n",
    "print(\"mean : \", mean_pred) # 13.198\n",
    "print(\"std : \", std_pred) # 5.202\n",
    "print(\"median : \", median_pred) # 11.537\n",
    "print(\"range : \", range_pred) # 33.511\n",
    "\n",
    "# The mean true emission = 15.2 Mt/yr and mean pred emission = 13.1 Mt/yr which tantamounts to the fact that our model has closely reciprocated the results\n",
    "# that was stated by Joffrey Dumont Le Brazidec et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fcbf6a",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis of our Train, Valid, Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This cell is only meant for those who are interested in knowing what are the different variables present in our .nc files. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "fn = 'curated/lip_train_dataset.nc' # filepath to .nc file\n",
    "nc = Dataset(fn, 'r') # Reading the nc file\n",
    "print(\"Variables in the NetCDF file:\") \n",
    "print(nc.variables.keys()) # printing the different variables.\n",
    "nc.close() # Closing the nc file to avoid corruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93ba7c-c054-409b-aac0-0f398cd7756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is for us to plot the distribution of different variables that we have used so far to understand our data better. \"\"\"\n",
    "\n",
    "# importing necessary packages\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Declaration of filepaths and opening using xarray.\n",
    "test_set = xr.open_dataset('curated/lip_test_dataset.nc')\n",
    "valid_set = xr.open_dataset('curated/lip_valid_dataset.nc')\n",
    "train_set = xr.open_dataset('curated/lip_train_dataset.nc')\n",
    "\n",
    "datasets = [test_set, valid_set, train_set]\n",
    "dataset_names = ['Test set', 'Validation set', 'Training set']\n",
    "variables = ['point_source', 'time', 'xco2_noisy', 'no2_noisy', 'u', 'v', 'seg_pred_no2'] # Variables that are of interest to us. \n",
    "\n",
    "for ds, name in zip(datasets, dataset_names): # repeating it for all the sets\n",
    "    for var in variables: # repeating it for all the variables\n",
    "        flat_data = ds[var].values.reshape(ds[var].shape[0], -1)\n",
    "        plt.figure(figsize = (12, 8))\n",
    "        sns.histplot(flat_data.flatten(), bins = 80, kde = True) # Histogram plot using 80 bins\n",
    "        plt.title(f\"{name} set: {var} distribution\")\n",
    "        plt.xlabel(var)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig(f\"{name}_{var}_distribution.png\")\n",
    "        plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3219a6b-9406-41ad-8589-f34c2aaa33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code allows us to understand the temporal variations in our datasets.\"\"\"\n",
    "\n",
    "# importing necessary packages\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Filepaths opened using xarray\n",
    "test_set = xr.open_dataset('curated/lip_test_dataset.nc')\n",
    "valid_set = xr.open_dataset('curated/lip_valid_dataset.nc')\n",
    "train_set = xr.open_dataset('curated/lip_train_dataset.nc')\n",
    "\n",
    "datasets = [test_set, valid_set, train_set]\n",
    "dataset_names = ['Test set', 'Validation set', 'Training set']\n",
    "\n",
    "for dataset, name in zip(datasets, dataset_names): # Repeating it for all the sets.\n",
    "    time_values = dataset['time'] # extracts the time values\n",
    "    time_index = pd.to_datetime(time_values.values) # Converting into a readable format.\n",
    "    if name == 'Test set':\n",
    "        hour_intervals = np.arange(0, 24, 1) # In test set, the temporal interval is 1 hour.\n",
    "        xtick_labels = [f'{i}-{i+1}' for i in hour_intervals]\n",
    "        \n",
    "    else:\n",
    "        hour_intervals = np.arange(0, 24, 2) # In train and valid set, the temporal interval is 2 hours.\n",
    "        xtick_labels = [f'{i}-{i+2}' for i in hour_intervals]\n",
    "    \n",
    "    # Hourly Distribution plot\n",
    "    plt.figure(figsize = (15, 8))\n",
    "    hours = time_index.hour.value_counts().sort_index()\n",
    "    bars = plt.bar(hours.index, hours, color = plt.cm.get_cmap('viridis', len(hour_intervals))(np.arange(len(hour_intervals)))) # plotting a bar plot \n",
    "    for bar in bars:\n",
    "        yval = bar.get_height() # getting the exact value of the bar\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval), va = 'bottom', ha = 'center', fontsize = 8) # Placing the value at the top of the bar.\n",
    "    plt.title(f\"{name} time distribution by hour\")\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(hour_intervals, xtick_labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{name}_hour_distribution.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Weekly Distribution plot\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    days = time_index.dayofweek.value_counts().sort_index()\n",
    "    plt.bar(range(7), days, color = plt.cm.get_cmap('viridis', 7)(np.arange(7))) # plotting a bar plot\n",
    "    plt.title(f\"{name} time distribution by day of week\")\n",
    "    plt.xlabel('Day of Week')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "    for i, value in enumerate(days):\n",
    "        plt.text(i, value + 0.5, str(value), ha = 'center', va = 'bottom', fontsize = 8, color = 'black') # Placing the value at the top of the bar.\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{name}_day_distribution.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Monthly Distribution plot\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    months = time_index.month.value_counts().sort_index()\n",
    "    plt.bar(range(1, 13), months, color = plt.cm.get_cmap('viridis', 12)(np.arange(12))) # plotting a bar plot\n",
    "    plt.title(f\"{name} time distribution by month\")\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    for i, value in enumerate(months):\n",
    "        plt.text(i + 1, value + 0.5, str(value), ha = 'center', va = 'bottom', fontsize = 8, color = 'black') # Placing the value at the top of the bar.\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{name}_month_distribution.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834de9f-d020-4cf1-be60-6c55ffbf69d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code helps us in identifying from which sources we got our datasets.\"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import xarray as xr\n",
    "\n",
    "# Filepaths opened using xarray\n",
    "test_set = xr.open_dataset('Datasets/data_paper_inv_pp/lippendorf/test_dataset.nc')\n",
    "valid_set = xr.open_dataset('Datasets/data_paper_inv_pp/lippendorf/valid_dataset.nc')\n",
    "train_set = xr.open_dataset('Datasets/data_paper_inv_pp/lippendorf/train_dataset.nc')\n",
    "\n",
    "# Only need point_source values\n",
    "test_point_sources = test_set['point_source'].values\n",
    "valid_point_sources = valid_set['point_source'].values\n",
    "train_point_sources = train_set['point_source'].values\n",
    "\n",
    "unique_test_point_sources = set(test_point_sources.tolist())\n",
    "unique_valid_point_sources = set(valid_point_sources.tolist())\n",
    "unique_train_point_sources = set(train_point_sources.tolist())\n",
    "\n",
    "# Print these unique point sources\n",
    "print(\"Unique point sources in Test set:\")\n",
    "print(unique_test_point_sources) # Only has data from Lippendorf\n",
    "print()\n",
    "\n",
    "# Have data from everywhere except Lippendorf\n",
    "print(\"Unique point sources in Validation set:\")\n",
    "print(unique_valid_point_sources)\n",
    "print()\n",
    "\n",
    "print(\"Unique point sources in Training set:\")\n",
    "print(unique_train_point_sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33f423-da6e-4854-8bd2-6f7e48e74e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code helps us in understanding temporal variations better and also look at emission data.\"\"\"\n",
    "\n",
    "# importing necessary packages\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_set = xr.open_dataset('curated/lip_test_dataset.nc')\n",
    "valid_set = xr.open_dataset('curated/lip_valid_dataset.nc')\n",
    "train_set = xr.open_dataset('curated/lip_train_dataset.nc')\n",
    "\n",
    "#variables = ['xco2_noisy', 'no2_noisy', 'u', 'v', 'seg_pred_no2', 'emiss']\n",
    "variables = ['emiss']\n",
    "datasets = [test_set, valid_set, train_set]\n",
    "dataset_names = ['Test set', 'Validation set', 'Training set']\n",
    "\n",
    "for dataset, name in zip(datasets, dataset_names):\n",
    "    #time_values = dataset['time']\n",
    "    \n",
    "    #print(f\"{name} time distribution:\")\n",
    "    #print(f\"Min time: {time_values.min().values}\")\n",
    "    #print(f\"Max time: {time_values.max().values}\")\n",
    "    #time_diff_1_2 = (time_values[1] - time_values[0]).astype('timedelta64[ns]').values*(10**(-9))\n",
    "    #time_diff_2_3 = (time_values[2] - time_values[1]).astype('timedelta64[ns]').values*(10**(-9)) \n",
    "    #time_diff_3_4 = (time_values[3] - time_values[2]).astype('timedelta64[ns]').values*(10**(-9))\n",
    "    #time_range = (time_values.max() - time_values.min()).astype('timedelta64[ns]').values*(10**(-9))\n",
    "    \n",
    "    #print(f\"Time difference between 1st and 2nd observation: {int(time_diff_1_2/3600)} h\") # 1 h if test, 2 h if train or valid\n",
    "    #print(f\"Time difference between 2nd and 3rd observation: {int(time_diff_2_3/3600)} h\") # 1 h if test, 2 h if train or valid\n",
    "    #print(f\"Time difference between 3rd and 4th observation: {int(time_diff_3_4/3600)} h\") # 1 h if test, 2 h if train or valid\n",
    "    #print()\n",
    "\n",
    "    print(f\"{name} set summary statistics:\")\n",
    "    for var in variables:\n",
    "        if var == 'emiss': # only for emiss data\n",
    "            emiss_data = dataset['emiss'].values\n",
    "            modified_data = emiss_data * [0, 1, 0] # only considering the middle value\n",
    "            emiss_data = np.round(modified_data, decimals = 3) # rounded off to 3 decimal places\n",
    "            mean = np.mean(emiss_data, axis = None) # Calculating mean \n",
    "            max = np.max(emiss_data, axis = None) # Getting the max\n",
    "            min = np.min(emiss_data, axis = None) # Getting the min\n",
    "            std = np.std(emiss_data, axis = None) # Calculating the std\n",
    "        \n",
    "            print(f\"mean : {mean}\")\n",
    "            print(f\"max : {max}\")\n",
    "            print(f\"min : {min}\")\n",
    "            print(f\"std : {std}\")\n",
    "            print()\n",
    "        \n",
    "        #else: # for all variables except emiss\n",
    "            #print(f\"Variable {var}:\")\n",
    "            #print(f\"mean : {dataset[var].mean().values}\") # Calculating mean \n",
    "            #print(f\"std dev : {dataset[var].std().values}\") # Calculating the std\n",
    "            #print(f\"min : {dataset[var].min().values}\") # Getting the min\n",
    "            #print(f\"max : {dataset[var].max().values}\") # Getting the max\n",
    "            #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550844d-bb79-4fe7-b419-b94a9b4df5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code helps us in understanding the emiss distribution.\"\"\"\n",
    "\n",
    "# importing necessary packages\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Opened using xarray\n",
    "test_set = xr.open_dataset('curated/lip_test_dataset.nc')\n",
    "valid_set = xr.open_dataset('curated/lip_valid_dataset.nc')\n",
    "train_set = xr.open_dataset('curated/lip_train_dataset.nc')\n",
    "names = ['Test', 'Train', 'Val']\n",
    "\n",
    "# Function Declaration\n",
    "def process_emiss_data(name, dataset):\n",
    "    emiss_data = dataset['emiss'].values\n",
    "    modified_data = emiss_data * [0, 1, 0] # only considering the middle value\n",
    "    emiss_data = np.round(modified_data, decimals = 3) # rounded off to 3 decimal places\n",
    "    data = emiss_data.flatten()\n",
    "    \n",
    "    plt.figure(figsize = (10, 6))\n",
    "    sns.histplot(data, kde = True, bins = 50, edgecolor = 'black') # Histogram plot to understand the emiss distribution\n",
    "    plt.title('Histogram of Emiss Data')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True)\n",
    "    #plt.savefig(f\"Figures/EDA/{name}/{name}_set_emiss_distribution.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Calling the function\n",
    "process_emiss_data(names[0], test_set)\n",
    "process_emiss_data(names[2], valid_set)\n",
    "process_emiss_data(names[1], train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0770c-f658-4212-835d-349985420c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# filepath declaration and opening with xarray\n",
    "test_set = xr.open_dataset('curated/box_test_dataset.nc')\n",
    "valid_set = xr.open_dataset('curated/box_valid_dataset.nc')\n",
    "train_set = xr.open_dataset('curated/box_train_dataset.nc')\n",
    "names = ['Test', 'Train', 'Val']\n",
    "datasets = [test_set, train_set, valid_set]\n",
    "\n",
    "# define the bins\n",
    "bins = [(0, 2), (2, 5), (5, 10), (10, 15), (15, 20), (20, 25), (25, np.inf)]\n",
    "\n",
    "for name, dataset in zip(names, datasets):\n",
    "    print(f\"Counts for {name} set:\")\n",
    "    emiss = dataset['emiss'].values\n",
    "    mod = emiss[:, 1] \n",
    "    print(f\"Shape of mod: {mod.shape}\")\n",
    "    total = len(mod)\n",
    "    print(f\"Total: {total}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    min_val = np.min(mod)\n",
    "    max_val = np.max(mod)\n",
    "    range_val = max_val - min_val\n",
    "    mean_val = np.mean(mod)\n",
    "    median_val = np.median(mod)\n",
    "    std_val = np.std(mod)\n",
    "    \n",
    "    print(f\"Min: {min_val:.2f}\")\n",
    "    print(f\"Max: {max_val:.2f}\")\n",
    "    print(f\"Range: {range_val:.2f}\")\n",
    "    print(f\"Mean: {mean_val:.2f}\")\n",
    "    print(f\"Median: {median_val:.2f}\")\n",
    "    print(f\"Standard Deviation: {std_val:.2f}\")\n",
    "    \n",
    "    # loop through the bins and count the values\n",
    "    for bin in bins:\n",
    "        count = np.sum((mod >= bin[0]) & (mod < bin[1]))  # Use mod instead of data\n",
    "        percentage = (count / total) * 100  # Calculate percentage\n",
    "        if bin[1] == np.inf:\n",
    "            print(f\"{bin[0]} and above: {count} ({percentage:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"{bin[0]}-{bin[1]}: {count} ({percentage:.2f}%)\")\n",
    "    \n",
    "    '''# Plot histogram\n",
    "    plt.hist(mod, bins=50, alpha=0.5, label=name)\n",
    "    plt.xlabel('Emissivity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Histogram of Emissivity for {name} set')\n",
    "    plt.savefig(f\"Figures/EDA/{name}/{name}_set_emiss_distribution_actual.png\")\n",
    "    plt.show()'''\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f767d2-6119-4832-a362-10a5fc7fb2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for Train set:\n",
      "Min: 2.88\n",
      "Max: 52.66\n",
      "Mean: 13.99\n",
      "Median: 10.48\n",
      "Standard Deviation: 9.12\n",
      "0-2: 0 (0.00%)\n",
      "2-5: 1189 (5.07%)\n",
      "5-10: 9909 (42.29%)\n",
      "10-15: 4131 (17.63%)\n",
      "15-20: 3107 (13.26%)\n",
      "20-25: 2298 (9.81%)\n",
      "25 and above: 2797 (11.94%)\n",
      "\n",
      "Statistics for Validation set:\n",
      "Min: 2.95\n",
      "Max: 52.66\n",
      "Mean: 14.01\n",
      "Median: 10.69\n",
      "Standard Deviation: 9.00\n",
      "0-2: 0 (0.00%)\n",
      "2-5: 285 (5.27%)\n",
      "5-10: 2218 (41.02%)\n",
      "10-15: 1008 (18.64%)\n",
      "15-20: 691 (12.78%)\n",
      "20-25: 560 (10.36%)\n",
      "25 and above: 645 (11.93%)\n",
      "\n",
      "Statistics for Test set:\n",
      "Min: 2.86\n",
      "Max: 52.66\n",
      "Mean: 14.04\n",
      "Median: 10.50\n",
      "Standard Deviation: 9.20\n",
      "0-2: 0 (0.00%)\n",
      "2-5: 394 (5.46%)\n",
      "5-10: 3037 (42.12%)\n",
      "10-15: 1270 (17.61%)\n",
      "15-20: 952 (13.20%)\n",
      "20-25: 661 (9.17%)\n",
      "25 and above: 897 (12.44%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset as NetCDFDataset\n",
    "\n",
    "class CustomDataset:\n",
    "    \n",
    "    def __init__(self, file_path, variable = None):\n",
    "        self.ncfile = NetCDFDataset(file_path, 'r') # Reading the .nc file\n",
    "        self.variable = variable # Variable in our comparison is no2_noisy\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ncfile.variables['xco2_noisy'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        \n",
    "        emiss_read = self.ncfile.variables['emiss'][idx]\n",
    "        \n",
    "        emiss_arr = np.array(emiss_read.data).astype('float32')\n",
    "        weights = np.array([0.0, 1.0, 0.0]) \n",
    "        weighted = np.round(np.sum(weights * emiss_arr), 3)\n",
    "        outputs = np.array([weighted], dtype = np.float32)\n",
    "        \n",
    "        return outputs # returns our inputs (64, 64, 4), outputs (1,)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.ncfile.close() # Closing the ncfile \n",
    "        \n",
    "# Define paths to dataset files\n",
    "test_set = 'curated/box_test_dataset.nc'\n",
    "valid_set = 'curated/box_valid_dataset.nc'\n",
    "train_set = 'curated/box_train_dataset.nc'\n",
    "\n",
    "# Load custom datasets\n",
    "train_dataset = CustomDataset(train_set, variable='no2_noisy')\n",
    "valid_dataset = CustomDataset(valid_set, variable='no2_noisy')\n",
    "test_dataset = CustomDataset(test_set, variable='no2_noisy')\n",
    "\n",
    "# Initialize lists to store inputs and outputs\n",
    "\n",
    "combine_outputs = []\n",
    "\n",
    "# Combine inputs and outputs from all datasets\n",
    "for dataset in [train_dataset, valid_dataset, test_dataset]:\n",
    "    for outputs in dataset:\n",
    "        \n",
    "        combine_outputs.append(outputs)\n",
    "\n",
    "\n",
    "combine_outputs = np.array(combine_outputs)  # Shape: (36049, 1)\n",
    "\n",
    "# Shuffle dataset\n",
    "indices = np.arange(combine_outputs.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "combine_outputs = combine_outputs[indices]\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.65 * combine_outputs.shape[0])\n",
    "valid_size = int(0.15 * combine_outputs.shape[0])\n",
    "test_size = combine_outputs.shape[0] - train_size - valid_size\n",
    "\n",
    "train_outputs, val_outputs, test_outputs = np.split(combine_outputs, [train_size, train_size + valid_size])\n",
    "\n",
    "# Clean up unnecessary variables\n",
    "del combine_outputs, train_dataset, valid_dataset, test_dataset, indices\n",
    "\n",
    "# Function to calculate statistics\n",
    "def calculate_statistics(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    mean_val = np.mean(data)\n",
    "    median_val = np.median(data)\n",
    "    std_val = np.std(data)\n",
    "    return min_val, max_val, mean_val, median_val, std_val\n",
    "\n",
    "# Calculate and print statistics for each dataset\n",
    "datasets = [train_outputs, val_outputs, test_outputs]\n",
    "names = ['Train', 'Validation', 'Test']\n",
    "\n",
    "for name, dataset in zip(names, datasets):\n",
    "    # Flatten dataset for analysis\n",
    "    mod = dataset.flatten()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    min_val, max_val, mean_val, median_val, std_val = calculate_statistics(mod)\n",
    "    \n",
    "    print(f\"\\nStatistics for {name} set:\")\n",
    "    print(f\"Min: {min_val:.2f}\")\n",
    "    print(f\"Max: {max_val:.2f}\")\n",
    "    print(f\"Mean: {mean_val:.2f}\")\n",
    "    print(f\"Median: {median_val:.2f}\")\n",
    "    print(f\"Standard Deviation: {std_val:.2f}\")\n",
    "    \n",
    "    # Define bins for histogram analysis\n",
    "    bins = [(0, 2), (2, 5), (5, 10), (10, 15), (15, 20), (20, 25), (25, np.inf)]\n",
    "    \n",
    "    # Histogram analysis\n",
    "    total = len(mod)\n",
    "    for bin_range in bins:\n",
    "        count = np.sum((mod >= bin_range[0]) & (mod < bin_range[1]))\n",
    "        percentage = (count / total) * 100\n",
    "        if bin_range[1] == np.inf:\n",
    "            print(f\"{bin_range[0]} and above: {count} ({percentage:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"{bin_range[0]}-{bin_range[1]}: {count} ({percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ce670-5a2a-4178-9f65-4b5571f819b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code helps us in plotting a few random plots of xco2, no2, seg images.\"\"\"\n",
    "\n",
    "# importing necessary packages\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# filepath declaration and opening with xarray\n",
    "test_set = xr.open_dataset('curated/lip_test_dataset.nc')\n",
    "valid_set = xr.open_dataset('curated/lip_valid_dataset.nc')\n",
    "train_set = xr.open_dataset('curated/lip_train_dataset.nc')\n",
    "names = ['Test', 'Train', 'Val']\n",
    "\n",
    "# Function declaration\n",
    "def plot(name, dataset):\n",
    "    \n",
    "    # Get the values from the variables that are of interest\n",
    "    no2_noisy = dataset['no2_noisy'].values\n",
    "    xco2_noisy = dataset['xco2_noisy'].values\n",
    "    u = dataset['u'].values\n",
    "    v = dataset['v'].values\n",
    "    seg_pred_no2 = dataset['seg_pred_no2'].values\n",
    "    num_images = 4 # total number of images we want to plot for each.\n",
    "    \n",
    "    # PLotting NO2 images\n",
    "    fig, axs = plt.subplots(1, num_images, figsize = (12, 8))\n",
    "    for i in range(num_images):\n",
    "        idx = np.random.randint(0, len(no2_noisy)) # generates a random number in between 0 and length of the column-1\n",
    "        img = axs[i].imshow(no2_noisy[idx], cmap = 'viridis')\n",
    "        axs[i].set_title(f'no2_noisy Image {idx}') \n",
    "        axs[i].axis('off')\n",
    "        cbar = fig.colorbar(img, ax = axs[i], orientation = 'vertical', fraction = 0.05)\n",
    "        cbar.set_label('Intensity', rotation = 270, labelpad = 25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Figures/EDA/{name}/{name}_set_no2.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # PLotting XCO2 images \n",
    "    fig, axs = plt.subplots(1, num_images, figsize = (12, 8))\n",
    "    for i in range(num_images):\n",
    "        idx = np.random.randint(0, len(xco2_noisy)) # generates a random number in between 0 and length of the column-1\n",
    "        img = axs[i].imshow(xco2_noisy[idx], cmap = 'viridis')\n",
    "        axs[i].set_title(f'xco2_noisy Image {idx}')\n",
    "        axs[i].axis('off')\n",
    "        cbar = fig.colorbar(img, ax = axs[i], orientation = 'vertical', fraction = 0.05)\n",
    "        cbar.set_label('Intensity', rotation = 270, labelpad = 15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Figures/EDA/{name}/{name}_set_xco2.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # PLotting seg images\n",
    "    fig, axs = plt.subplots(1, num_images, figsize = (12, 8))\n",
    "    for i in range(num_images):\n",
    "        idx = np.random.randint(0, len(seg_pred_no2)) # generates a random number in between 0 and length of the column-1\n",
    "        img = axs[i].imshow(seg_pred_no2[idx], cmap = 'viridis')\n",
    "        axs[i].set_title(f'seg_pred_no2 Image {idx}')\n",
    "        axs[i].axis('off')\n",
    "        cbar = fig.colorbar(img, ax = axs[i], orientation = 'vertical', fraction = 0.05)\n",
    "        cbar.set_label('Intensity', rotation = 270, labelpad = 15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Figures/EDA/{name}/{name}_set_seg.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Calling the function\n",
    "plot(names[0], test_set)\n",
    "plot(names[2], valid_set)\n",
    "plot(names[1], train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa21558-4f58-4e1b-a979-ef830f278b9d",
   "metadata": {},
   "source": [
    "Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326d5b0-a39e-4529-bdb8-ed1676f00c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Let us understand our input data and target data for segmentation work. \"\"\"\n",
    "\n",
    "# necessary packages\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# filepath declaration and opening with xarray\n",
    "test_set = xr.open_dataset('curated/lip_test_dataset.nc')\n",
    "valid_set = xr.open_dataset('curated/lip_valid_dataset.nc')\n",
    "train_set = xr.open_dataset('curated/lip_train_dataset.nc')\n",
    "names = ['Test', 'Train', 'Val']\n",
    "\n",
    "# Function declaration\n",
    "def plot(name, dataset):\n",
    "    \n",
    "    # Get the random value from the variables\n",
    "    idx = np.random.randint(0, len(dataset['no2']))\n",
    "    \n",
    "    no2 = dataset['no2'][idx].values\n",
    "    plume = dataset['no2_plume'][idx].values\n",
    "    bool_perf_seg = dataset['plume'][idx].values\n",
    "    seg_pred_no2 = dataset['seg_pred_no2'][idx].values\n",
    "    w_perf_seg = dataset['w_perf_seg'][idx].values\n",
    "\n",
    "    fig, axs = plt.subplots(1, 5, figsize = (18, 6))\n",
    "\n",
    "    axs[0].imshow(no2, cmap = 'viridis')\n",
    "    axs[0].set_title(f'{name} - NO2')\n",
    "    axs[1].imshow(plume, cmap = 'viridis')\n",
    "    axs[1].set_title(f'{name} - plume')\n",
    "    #fig.colorbar(im, ax = axs[1])\n",
    "    axs[2].imshow(bool_perf_seg, cmap = 'viridis')\n",
    "    axs[2].set_title(f'{name} - Bool Perf Seg')\n",
    "    axs[3].imshow(seg_pred_no2, cmap = 'viridis')\n",
    "    axs[3].set_title(f'{name} - Seg Pred NO2')\n",
    "    axs[4].imshow(w_perf_seg, cmap = 'viridis')\n",
    "    axs[4].set_title(f'{name} - W Perf Seg')\n",
    "    \n",
    "\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(f'Figures/EDA/{name}/comp.png')\n",
    "    plt.show()\n",
    "\n",
    "# Function calling\n",
    "for name, dataset in zip(names, [test_set, train_set, valid_set]):\n",
    "    plot(name, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db96b6f-5965-4040-9549-ac688915cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Segmentation Code for NO2 data \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Importing different packages that we will use.\n",
    "\n",
    "\"\"\"\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset as NetCDFDataset\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, BatchNormalization, MaxPool2D, Input, Concatenate, Lambda, Activation, UpSampling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "\n",
    "\"\"\"\n",
    "We define our Custom Dataset which will read the .nc files (train, valid, test) and extract the no2, plume data from the .nc files.\n",
    "Note to the reader : For our comparision, we are going to consider Lippendorf location.\n",
    "\n",
    "\"\"\"\n",
    "class CustomDataset:\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.ncfile = NetCDFDataset(file_path, 'r') # Reading the .nc file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ncfile.variables['no2'].shape[0] # The total number of images or data we will have which is different for train (25152), valid (4608), test (6289)\n",
    "\n",
    "    # To preprocess plume data as mentioned by \"Segmentation of XCO2 images with deep learning: application to synthetic plumes from cities and power plants\" by Joffrey Dumont Le Brazidec et al.\n",
    "    def calculate_weighted_plume(self, plume, min_w, max_w, curve = \"linear\", param_curve = 1):\n",
    "        y_min = 0.05\n",
    "        y_max = np.quantile(plume, q = 0.99)\n",
    "    \n",
    "        weight_min = min_w\n",
    "        weight_max = max_w\n",
    "    \n",
    "        y_data = np.zeros_like(plume, dtype = np.float32)\n",
    "    \n",
    "        if curve == \"linear\":\n",
    "            y_data[plume <= y_min] = 0\n",
    "            y_data[(plume > y_min) & (plume < y_max)] = weight_min + ((weight_max - weight_min) / (y_max - y_min)) * (plume[(plume > y_min) & (plume < y_max)] - y_min)\n",
    "            y_data[plume >= y_max] = weight_max\n",
    "    \n",
    "        return y_data\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Reading the necessary variables (no2, w_perf_seg, seg_pred_no2)\n",
    "        no2_read = self.ncfile.variables['no2'][idx] # Unit : molec/cm^2, Shape : 64, 64\n",
    "        plume_read = self.ncfile.variables['plume'][idx] # Unit : ppmv, Shape : 64, 64\n",
    "        \n",
    "        # Converting into numpy arrays\n",
    "        no2_arr = np.array(no2_read.data).astype('float32')\n",
    "        plume_arr = np.array(plume_read.data).astype('float32')\n",
    "        \n",
    "        # Normalization\n",
    "        inputs = (no2_arr - no2_arr.mean(axis = (0, 1), keepdims = True)) / np.where(no2_arr.std(axis = (0, 1), keepdims = True) == 0, 1, no2_arr.std(axis = (0, 1), keepdims = True))\n",
    "        truths = self.calculate_weighted_plume(plume_arr, min_w = 0.01, max_w = 4.0)\n",
    "        inputs = inputs.reshape((64, 64, 1))\n",
    "        truths = truths.reshape((64, 64, 1))\n",
    "        \n",
    "        return inputs, truths\n",
    "\n",
    "    def __del__(self):\n",
    "        self.ncfile.close()\n",
    "\n",
    "\"\"\" Our Unet model builder function \n",
    "Note to the reader :  The paper \"Segmentation of XCO2 images with deep learning: application to synthetic plumes from cities and power plants\" by Joffrey Dumont Le Brazidec et al.\n",
    "used EfficientNetB0 as its encoder in the Unet architecture which resulted 10 million params. The code is taken from the paper (Github). \"\"\"\n",
    "\n",
    "# Two-Dimensional Convolutional operation with Batch Normalization\n",
    "def Conv2dBn(filters, kernel_size, strides = (1, 1), padding = 'valid', data_format = None, dilation_rate = (1, 1), activation = None, \n",
    "             kernel_initializer = 'glorot_uniform', bias_initializer = 'zeros', kernel_regularizer = None, bias_regularizer = None, \n",
    "             activity_regularizer = None, kernel_constraint = None, bias_constraint = None, use_batchnorm = False, **kwargs):\n",
    "    def wrapper(input_tensor):\n",
    "        x = Conv2D(\n",
    "            filters = filters,\n",
    "            kernel_size = kernel_size,\n",
    "            strides = strides,\n",
    "            padding = padding,\n",
    "            data_format = data_format,\n",
    "            dilation_rate = dilation_rate,\n",
    "            activation = None,\n",
    "            use_bias = not (use_batchnorm),\n",
    "            kernel_initializer = kernel_initializer,\n",
    "            bias_initializer = bias_initializer,\n",
    "            kernel_regularizer = kernel_regularizer,\n",
    "            bias_regularizer = bias_regularizer,\n",
    "            activity_regularizer = activity_regularizer,\n",
    "            kernel_constraint = kernel_constraint,\n",
    "            bias_constraint = bias_constraint,\n",
    "            )(input_tensor)\n",
    "\n",
    "        if use_batchnorm:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "# Convolution operation that calls the previous defined function with activation as ReLU.\n",
    "def Conv3x3BnReLU(filters, use_batchnorm, name = None):\n",
    "    def wrapper(input_tensor):\n",
    "        return Conv2dBn(\n",
    "            filters,\n",
    "            kernel_size = 3,\n",
    "            activation = 'relu',\n",
    "            kernel_initializer = 'he_uniform',\n",
    "            padding = 'same',\n",
    "            use_batchnorm = use_batchnorm,\n",
    "            name = name,\n",
    "        )(input_tensor)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "# For the Decoder part of Unet\n",
    "def DecoderUpsamplingX2Block(filters, stage, use_batchnorm = False):\n",
    "    up_name = 'decoder_stage{}_upsampling'.format(stage)\n",
    "    conv1_name = 'decoder_stage{}a'.format(stage)\n",
    "    conv2_name = 'decoder_stage{}b'.format(stage)\n",
    "    concat_name = 'decoder_stage{}_concat'.format(stage)\n",
    "\n",
    "    def wrapper(input_tensor, skip = None):\n",
    "        x = UpSampling2D(size = 2, name = up_name)(input_tensor)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = Concatenate(axis = 3, name = concat_name)([x, skip])\n",
    "\n",
    "        x = Conv3x3BnReLU(filters, use_batchnorm, name = conv1_name)(x)\n",
    "        x = Conv3x3BnReLU(filters, use_batchnorm, name = conv2_name)(x)\n",
    "        return x\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def build_unet(backbone, decoder_block, skip_connection_layers, \n",
    "               decoder_filters = (160, 80, 40, 20, 10), n_upsample_blocks = 5, \n",
    "               classes = 1, activation = 'sigmoid', use_batchnorm = True):\n",
    "    input_ = backbone.input\n",
    "    x = backbone.output\n",
    "\n",
    "    x = Lambda(lambda x: x)(x)\n",
    "\n",
    "    # extract skip connections\n",
    "    skips = [backbone.get_layer(name = i).output if isinstance(i, str) \n",
    "             else backbone.get_layer(index = i).output for i in skip_connection_layers]\n",
    "\n",
    "    # building decoder blocks\n",
    "    for i in range(n_upsample_blocks):\n",
    "        if i < len(skips):\n",
    "            skip = skips[i]\n",
    "        else:\n",
    "            skip = None\n",
    "\n",
    "        x = decoder_block(decoder_filters[i], stage = i, use_batchnorm = use_batchnorm)(x, skip)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters = classes,\n",
    "        kernel_size = (3, 3),\n",
    "        padding = 'same',\n",
    "        use_bias = True,\n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "        name = 'final_conv',\n",
    "    )(x)\n",
    "    x = Activation(activation, name = activation)(x)\n",
    "\n",
    "    model = Model(input_, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def Unet_model(input_shape, num_classes, backbone_trainable = True):\n",
    "    inputs = Input(shape = input_shape)\n",
    "    #x = Conv2D(3, (3, 3), activation = 'relu', padding = 'same')(inputs)\n",
    "    backbone = keras.applications.EfficientNetB0(include_top = False, input_tensor = inputs, weights = None, drop_connect_rate = 0.2)\n",
    "    backbone.trainable = backbone_trainable\n",
    "    model = build_unet(\n",
    "        backbone = backbone,\n",
    "        decoder_block = DecoderUpsamplingX2Block,\n",
    "        skip_connection_layers = ['block6a_expand_activation', 'block4a_expand_activation',\n",
    "                                'block3a_expand_activation', 'block2a_expand_activation'],\n",
    "        decoder_filters = (256, 128, 64, 32, 16),\n",
    "        classes = num_classes,\n",
    "        activation = 'sigmoid',\n",
    "        use_batchnorm = True,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Custom Loss function adapted from \"Segmentation of XCO2 images with deep learning: application to synthetic plumes from cities and power plants\" by Joffrey Dumont Le Brazidec et al.\n",
    "def weighted_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        y_bin_true = tf.cast(y_true > 0, y_true.dtype)\n",
    "        loss_val = tf.keras.losses.binary_crossentropy(y_bin_true, y_pred)\n",
    "        weights = tf.where(y_true > 0, y_true, 1.0)\n",
    "        loss_val = tf.squeeze(weights, axis = -1) * loss_val\n",
    "        return K.mean(loss_val)\n",
    "    return loss\n",
    "\n",
    "# Creating and checking if saved_models folder exists or not. If not, then create a folder where we will store our weights.h5 file.\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "best_model_filepath = os.path.join(checkpoint_dir, 'seg_weights.h5')\n",
    "# Callbacks can be of best weights or last weights. We can do either one. We went with saving the weights after every epoch. \n",
    "best_model_checkpoint_callback = ModelCheckpoint(filepath = best_model_filepath, save_weights_only = True, verbose = 1)\n",
    "\n",
    "# Filepaths for Train, Valid, Test. From the paper we know that for Train, Valid we will choose data points from everywhere except the location in which\n",
    "# we will test our model's performance. Location = Lippendorf\n",
    "test_set = 'curated/lip_train_dataset.nc'\n",
    "valid_set = 'curated/lip_valid_dataset.nc'\n",
    "train_set = 'curated/lip_train_dataset.nc'\n",
    "\n",
    "# Here, we have generated our custom dataset.\n",
    "train_dataset = CustomDataset(train_set)\n",
    "valid_dataset = CustomDataset(valid_set)\n",
    "test_dataset = CustomDataset(test_set)\n",
    "\n",
    "# Applying data augmentation to Training data.\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 45, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "# To store the inputs and outputs from our dataset to feed it to the model during training, validation and inference.\n",
    "train_inputs = []\n",
    "train_truths = []\n",
    "val_inputs = []\n",
    "val_truths = []\n",
    "test_inputs = []\n",
    "test_truths = []\n",
    "\n",
    "for inputs, truths in train_dataset:\n",
    "    train_inputs.extend([inputs])\n",
    "    train_truths.extend([truths])\n",
    "for inputs, truths in valid_dataset:\n",
    "    val_inputs.extend([inputs])\n",
    "    val_truths.extend([truths])\n",
    "for inputs, truths in test_dataset:\n",
    "    test_inputs.extend([inputs])\n",
    "    test_truths.extend([truths])\n",
    "\n",
    "# Converting into numpy arrays\n",
    "train_inputs = np.array(train_inputs) # train_inputs shape : (25152, 64, 64, 1) \n",
    "train_truths = np.array(train_truths) # train_truths shape : (25152, 64, 64, 1)\n",
    "val_inputs = np.array(val_inputs) # val_inputs shape : (4608, 64, 64, 1) \n",
    "val_truths = np.array(val_truths) # val_truths shape : (4608, 64, 64, 1)\n",
    "test_inputs = np.array(test_inputs) # test_inputs shape : (6289, 64, 64, 1)\n",
    "test_truths = np.array(test_truths) # test_truths shape : (6289, 64, 64, 1)\n",
    "\n",
    "# Model Declaration\n",
    "model = Unet_model(input_shape = (64, 64, 1), num_classes = 1)\n",
    "\n",
    "#model.load_weights('saved_models/seg_weights.h5') # If we have already stored the weights, then uncomment this line to load.\n",
    "\n",
    "optimizer = Adam(learning_rate = 1e-3) # Taking Adam optimizer with lr = 1e-3\n",
    "\n",
    "# We will update our lr if we reach a plateau. For this we use ReduceLROnPlateau which will monitor val_loss, wait for 20 epochs before changing lr and measures \n",
    "# min_delta threshold, reduces lr by 0.5 and lower bound on lr is 5e-5.\n",
    "learning_rate_monitor_callback = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 20, verbose = 0, min_delta = 5e-3, cooldown = 0, min_lr = 5e-5)\n",
    "\n",
    "# Loss function for this task is Custom Weighted BCE.\n",
    "model.compile(optimizer = optimizer, loss = weighted_loss(), metrics = ['accuracy'])\n",
    "\n",
    "# Generate the summary of the model. Around 5 million trainable paramaters are there.\n",
    "model.summary()\n",
    "\n",
    "# Total epochs = 500, batch size for training set = 32, steps will be 25152/32 = 786, validation will not have any augmentation with batch size of 32 and validating\n",
    "# on the entire validation set.\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_truths, batch_size = 32, shuffle = True), epochs = 500, steps_per_epoch = len(train_inputs)//32,\n",
    "validation_data = (val_inputs, val_truths), validation_batch_size = 32, validation_steps = None, callbacks = [learning_rate_monitor_callback, best_model_checkpoint_callback])\n",
    "\n",
    "# To test the model's performance on test set.\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_truths, batch_size = None)\n",
    "print(f'Test Loss: {test_loss:.4f}') # 0.1471\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}') # 0.7907\n",
    "\n",
    "# Plotting loss and accuracy curves\n",
    "plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label = 'Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()\n",
    "#plt.savefig('Figures/Seg/check_loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f034a-c9a4-4925-832a-e36acdce22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Check for model performance \"\"\"\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset as NetCDFDataset\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, BatchNormalization, MaxPool2D, Input, Concatenate, Lambda, Activation, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "\n",
    "class CustomDataset:\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.ncfile = NetCDFDataset(file_path, 'r') # Reading the .nc file\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ncfile.variables['no2'].shape[0] # The total number of images or data we will have which is different for train (25152), valid (4608), test (6289)\n",
    "\n",
    "    # To preprocess plume data as mentioned by \"Segmentation of XCO2 images with deep learning: application to synthetic plumes from cities and power plants\" by Joffrey Dumont Le Brazidec et al.\n",
    "    def calculate_weighted_plume(self, plume, min_w, max_w, curve = \"linear\", param_curve = 1):\n",
    "        y_min = 0.05\n",
    "        y_max = np.quantile(plume, q = 0.99)\n",
    "    \n",
    "        weight_min = min_w\n",
    "        weight_max = max_w\n",
    "    \n",
    "        y_data = np.zeros_like(plume, dtype = np.float32)\n",
    "    \n",
    "        if curve == \"linear\":\n",
    "            y_data[plume <= y_min] = 0\n",
    "            y_data[(plume > y_min) & (plume < y_max)] = weight_min + ((weight_max - weight_min) / (y_max - y_min)) * (plume[(plume > y_min) & (plume < y_max)] - y_min)\n",
    "            y_data[plume >= y_max] = weight_max\n",
    "    \n",
    "        return y_data\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Reading the necessary variables (no2, w_perf_seg, seg_pred_no2)\n",
    "        no2_read = self.ncfile.variables['no2'][idx] # Unit : molec/cm^2, Shape : 64, 64\n",
    "        plume_read = self.ncfile.variables['plume'][idx] # Unit : ppmv, Shape : 64, 64\n",
    "        seg_read = self.ncfile.variables['seg_pred_no2'][idx] # Unit : bool, Shape :  64, 64\n",
    "        \n",
    "        # Converting into numpy arrays\n",
    "        no2_arr = np.array(no2_read.data).astype('float32')\n",
    "        plume_arr = np.array(plume_read.data).astype('float32')\n",
    "        seg_arr = np.array(seg_read.data).astype('float32')\n",
    "        \n",
    "        # Normalization\n",
    "        inputs = (no2_arr - no2_arr.mean(axis = (0, 1), keepdims = True)) / np.where(no2_arr.std(axis = (0, 1), keepdims = True) == 0, 1, no2_arr.std(axis = (0, 1), keepdims = True))\n",
    "        seg = (seg_arr - seg_arr.mean(axis = (0, 1), keepdims = True))/np.where(seg_arr.std(axis = (0, 1), keepdims = True) == 0, 1, seg_arr.std(axis = (0, 1), keepdims = True))\n",
    "        truths = self.calculate_weighted_plume(plume_arr, min_w = 0.01, max_w = 4.0)\n",
    "        inputs = inputs.reshape((64, 64, 1))\n",
    "        truths = truths.reshape((64, 64, 1))\n",
    "        seg = seg.reshape((64, 64, 1))\n",
    "        \n",
    "        return inputs, truths, seg\n",
    "\n",
    "    def __del__(self):\n",
    "        self.ncfile.close()\n",
    "\n",
    "# Two-Dimensional Convolutional operation with Batch Normalization\n",
    "def Conv2dBn(filters, kernel_size, strides = (1, 1), padding = 'valid', data_format = None, dilation_rate = (1, 1), activation = None, \n",
    "             kernel_initializer = 'glorot_uniform', bias_initializer = 'zeros', kernel_regularizer = None, bias_regularizer = None, \n",
    "             activity_regularizer = None, kernel_constraint = None, bias_constraint = None, use_batchnorm = False, **kwargs):\n",
    "    def wrapper(input_tensor):\n",
    "        x = Conv2D(\n",
    "            filters = filters,\n",
    "            kernel_size = kernel_size,\n",
    "            strides = strides,\n",
    "            padding = padding,\n",
    "            data_format = data_format,\n",
    "            dilation_rate = dilation_rate,\n",
    "            activation = None,\n",
    "            use_bias = not (use_batchnorm),\n",
    "            kernel_initializer = kernel_initializer,\n",
    "            bias_initializer = bias_initializer,\n",
    "            kernel_regularizer = kernel_regularizer,\n",
    "            bias_regularizer = bias_regularizer,\n",
    "            activity_regularizer = activity_regularizer,\n",
    "            kernel_constraint = kernel_constraint,\n",
    "            bias_constraint = bias_constraint,\n",
    "            )(input_tensor)\n",
    "\n",
    "        if use_batchnorm:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "# Convolution operation that calls the previous defined function with activation as ReLU.\n",
    "def Conv3x3BnReLU(filters, use_batchnorm, name = None):\n",
    "    def wrapper(input_tensor):\n",
    "        return Conv2dBn(\n",
    "            filters,\n",
    "            kernel_size = 3,\n",
    "            activation = 'relu',\n",
    "            kernel_initializer = 'he_uniform',\n",
    "            padding = 'same',\n",
    "            use_batchnorm = use_batchnorm,\n",
    "            name = name,\n",
    "        )(input_tensor)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "# For the Decoder part of Unet\n",
    "def DecoderUpsamplingX2Block(filters, stage, use_batchnorm = False):\n",
    "    up_name = 'decoder_stage{}_upsampling'.format(stage)\n",
    "    conv1_name = 'decoder_stage{}a'.format(stage)\n",
    "    conv2_name = 'decoder_stage{}b'.format(stage)\n",
    "    concat_name = 'decoder_stage{}_concat'.format(stage)\n",
    "\n",
    "    def wrapper(input_tensor, skip = None):\n",
    "        x = UpSampling2D(size = 2, name = up_name)(input_tensor)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = Concatenate(axis = 3, name = concat_name)([x, skip])\n",
    "\n",
    "        x = Conv3x3BnReLU(filters, use_batchnorm, name = conv1_name)(x)\n",
    "        x = Conv3x3BnReLU(filters, use_batchnorm, name = conv2_name)(x)\n",
    "        return x\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def build_unet(backbone, decoder_block, skip_connection_layers, \n",
    "               decoder_filters = (160, 80, 40, 20, 10), n_upsample_blocks = 5, \n",
    "               classes = 1, activation = 'sigmoid', use_batchnorm = True):\n",
    "    input_ = backbone.input\n",
    "    x = backbone.output\n",
    "\n",
    "    x = Lambda(lambda x: x)(x)\n",
    "\n",
    "    # extract skip connections\n",
    "    skips = [backbone.get_layer(name = i).output if isinstance(i, str) \n",
    "             else backbone.get_layer(index = i).output for i in skip_connection_layers]\n",
    "\n",
    "    # building decoder blocks\n",
    "    for i in range(n_upsample_blocks):\n",
    "        if i < len(skips):\n",
    "            skip = skips[i]\n",
    "        else:\n",
    "            skip = None\n",
    "\n",
    "        x = decoder_block(decoder_filters[i], stage = i, use_batchnorm = use_batchnorm)(x, skip)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters = classes,\n",
    "        kernel_size = (3, 3),\n",
    "        padding = 'same',\n",
    "        use_bias = True,\n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "        name = 'final_conv',\n",
    "    )(x)\n",
    "    x = Activation(activation, name = activation)(x)\n",
    "\n",
    "    model = Model(input_, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def Unet_model(input_shape, num_classes, backbone_trainable = True):\n",
    "    inputs = Input(shape = input_shape)\n",
    "    #x = Conv2D(3, (3, 3), activation = 'relu', padding = 'same')(inputs)\n",
    "    backbone = keras.applications.EfficientNetB0(include_top = False, input_tensor = inputs, weights = None, drop_connect_rate = 0.2)\n",
    "    backbone.trainable = backbone_trainable\n",
    "    model = build_unet(\n",
    "        backbone = backbone,\n",
    "        decoder_block = DecoderUpsamplingX2Block,\n",
    "        skip_connection_layers = ['block6a_expand_activation', 'block4a_expand_activation',\n",
    "                                'block3a_expand_activation', 'block2a_expand_activation'],\n",
    "        decoder_filters = (256, 128, 64, 32, 16),\n",
    "        classes = num_classes,\n",
    "        activation = 'sigmoid',\n",
    "        use_batchnorm = True,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def weighted_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        y_bin_true = tf.cast(y_true > 0, y_true.dtype)\n",
    "        loss_val = tf.keras.losses.binary_crossentropy(y_bin_true, y_pred)\n",
    "        weights = tf.where(y_true > 0, y_true, 1.0)\n",
    "        loss_val = tf.squeeze(weights, axis = -1) * loss_val\n",
    "        return K.mean(loss_val)\n",
    "    return loss\n",
    "    \n",
    "test_set = 'curated/lip_test_dataset.nc'\n",
    "test_dataset = CustomDataset(test_set)\n",
    "\n",
    "test_inputs = []\n",
    "test_truths = []\n",
    "test_seg = []\n",
    "for inputs, truths, seg in test_dataset:\n",
    "    test_inputs.extend([inputs])\n",
    "    test_truths.extend([truths])\n",
    "    test_seg.extend([seg])\n",
    "test_inputs = np.array(test_inputs) # test_inputs shape : (6289, 64, 64, 1)\n",
    "test_truths = np.array(test_truths) # test_truths shape : (6289, 64, 64, 1)\n",
    "test_seg = np.array(test_seg) # test_seg shape :  (6289, 64, 64, 1)\n",
    "\n",
    "# Model Declaration\n",
    "model = Unet_model(input_shape = (64, 64, 1), num_classes = 1)\n",
    "model.load_weights('saved_models/seg_weights.h5')\n",
    "optimizer = Adam(learning_rate = 1e-3)\n",
    "model.compile(optimizer = optimizer, loss = weighted_loss(), metrics = ['accuracy'])\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_truths, batch_size = None)\n",
    "print(f'Test Loss: {test_loss:.4f}') # 0.1471\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}') # 0.7907\n",
    "\n",
    "# Plots\n",
    "test_pred = model.predict(test_inputs)\n",
    "#test_pred = test_pred*4\n",
    "idx = np.random.choice(test_inputs.shape[0], 25, replace = False)\n",
    "for i in idx:\n",
    "    plt.figure(figsize = (12, 4))\n",
    "    plt.subplot(1, 4, 1)\n",
    "    im1 = plt.imshow(test_inputs[i, :, :, 0], cmap = 'viridis')\n",
    "    plt.title('Actual Image')\n",
    "    plt.colorbar(im1, ax = plt.gca(), fraction = 0.046, pad = 0.04)\n",
    "    plt.subplot(1, 4, 2)\n",
    "    im2 = plt.imshow(test_truths[i, :, :, 0], cmap = 'viridis')\n",
    "    plt.title('Mask used')\n",
    "    plt.colorbar(im2, ax = plt.gca(), fraction = 0.046, pad = 0.04)\n",
    "    plt.subplot(1, 4, 3)\n",
    "    im3 = plt.imshow(test_pred[i, :, :, 0], cmap = 'viridis')\n",
    "    plt.title('Predicted Mask')\n",
    "    plt.colorbar(im3, ax = plt.gca(), fraction = 0.046, pad = 0.04)\n",
    "    plt.subplot(1, 4, 4)\n",
    "    im4 = plt.imshow(test_seg[i, :, :, 0], cmap = 'viridis')\n",
    "    plt.title('Mask given')\n",
    "    plt.colorbar(im4, ax = plt.gca(), fraction = 0.046, pad = 0.04)\n",
    "    #plt.savefig(f'Figures/Seg/test_seg_ex_{i}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c9b41-d0c5-4650-85c8-f73509a66267",
   "metadata": {},
   "source": [
    "U-Net Regression Model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4907d5f-94e3-40a4-96f2-9ed256a909a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This marks the beginning of our novel approach that is using U-Net for regression task. \"\"\"\n",
    "\"\"\"\n",
    "Importing different packages that we will use.\n",
    "\n",
    "\"\"\"\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset as NetCDFDataset\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dropout, BatchNormalization, MaxPool2D, Concatenate, Conv2DTranspose, Flatten, Dense, UpSampling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "We define our Custom Dataset which will read the .nc files (train, valid, test) and extract the XCO2, u, v, NO2 data from the .nc files.\n",
    "Note to the reader : For our comparision, we are going to consider Lippendorf location and as additional input we will consider NO2 images.\n",
    "\n",
    "\"\"\"\n",
    "class CustomDataset:\n",
    "    \n",
    "    def __init__(self, file_path, variable = None):\n",
    "        self.ncfile = NetCDFDataset(file_path, 'r') # Reading the .nc file\n",
    "        self.variable = variable # Variable in our comparison is no2_noisy\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ncfile.variables['xco2_noisy'].shape[0] # The total number of images or data we will have which is different for train (25152), valid (4608), test (6289)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Reading the necessary variables (xco2_noisy, u, v)\n",
    "        xco2_read = self.ncfile.variables['xco2_noisy'][idx] # Unit : ppmv (parts per million by volume), Shape : 64, 64\n",
    "        u_read = self.ncfile.variables['u'][idx] # Unit : m/s, Shape : 64, 64\n",
    "        v_read = self.ncfile.variables['v'][idx] # Unit : m/s, Shape : 64, 64\n",
    "        emiss_read = self.ncfile.variables['emiss'][idx] # emiss is the true output data that we need to train our model. Unit : Mt/yr (Million tonnes/ year)\n",
    "                                                         # Shape : 3,\n",
    "        # Converting into numpy arrays with data type float32\n",
    "        xco2_arr = np.array(xco2_read.data).astype('float32') \n",
    "        u_arr = np.array(u_read.data).astype('float32')\n",
    "        v_arr = np.array(v_read.data).astype('float32')\n",
    "        emiss_arr = np.array(emiss_read.data).astype('float32')\n",
    "\n",
    "        #noise = GaussianNoise(stddev = 0.7) # Addition of Gaussian Noise with std of 0.7\n",
    "        #xco2 = noise(xco2_arr).numpy() \n",
    "        if self.variable:\n",
    "            var_read = self.ncfile.variables[self.variable][idx] # Unit : molec/cm^2, Shape : 64, 64\n",
    "            var_arr = np.array(var_read.data).astype('float32') # Converting into numpy array after reading the variable\n",
    "            #var_noise = noise(var_tensor).numpy()\n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr, var_arr], axis = -1) # Stacking the inputs to get the desired input shape = (64, 64, 4) where 64, 64\n",
    "                                                                            # represents height, width and 4 represents the total number of features. Each pixel\n",
    "                                                                            # has a spatial resolution of 2km, i.e. 1*1 in pixel refers to an area of 2km*2km.\n",
    "        else: \n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr], axis = -1) # Stacking the inputs to get the shape = (64, 64, 3) in case when variable is None.\n",
    "        \n",
    "        mean = inputs.mean(axis = (0, 1), keepdims = True) # Calculating the mean of the 4 features separately and keeping the same shape as that of inputs.\n",
    "        \n",
    "        std = inputs.std(axis = (0, 1), keepdims = True) # Calculating the std of the 4 features separately and keeping the same shape as that of inputs.\n",
    "        \n",
    "        std[std==0] = 1 # If std becomes 0, we change it 1 to avoid division by 0. Multiple other methods can also be employed such as choosing an \n",
    "                        # appropriate epsilon such as 1e-5, 1e-6 or 1e-7\n",
    "        \n",
    "        inputs = (inputs - mean)/std # Normalization of the inputs separately for each feature.\n",
    "        \n",
    "        # emiss data is of shape (3,), from which we will only consider the middle value for our comparison. Hence, we multiply weights with emiss to\n",
    "        # get the middle value and round it off to 3 decimal places.\n",
    "        weights = np.array([0.0, 1.0, 0.0]) \n",
    "        weighted = np.round(np.sum(weights * emiss_arr), 3)\n",
    "        outputs = np.array([weighted], dtype = np.float32)\n",
    "        \n",
    "        return inputs, outputs # returns our inputs (64, 64, 4), outputs (1,)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.ncfile.close() # Closing the ncfile to avoid corruption of the file.\n",
    "\n",
    "\"\"\" The U-Net regression Model \"\"\"\n",
    "\n",
    "def unetreg(input_shape, dropout_rate = 0.2):\n",
    "    inputs = Input(input_shape) # If we use no2 then shape : (64, 64, 4), if we don't use no2 then shape : (64, 64, 3)\n",
    "    \n",
    "    # Encoder (downsampling path)\n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(inputs) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c1) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    d1 = Dropout(dropout_rate)(c1) \n",
    "    p1 = MaxPool2D(pool_size = (2, 2))(d1) # Shape : (32, 32, 64) \n",
    "    \n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p1) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c2) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    d2 = Dropout(dropout_rate)(c2)\n",
    "    p2 = MaxPool2D(pool_size = (2, 2))(d2) # Shape : (16, 16, 128)\n",
    "    \n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p2) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c3) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    d3 = Dropout(dropout_rate)(c3)\n",
    "    p3 = MaxPool2D(pool_size = (2, 2))(d3) # Shape : (8, 8, 256)\n",
    "\n",
    "    # Bottleneck\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p3) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c4) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    d4 = Dropout(dropout_rate)(c4)\n",
    "    \n",
    "    # Decoder (upsampling path)\n",
    "    u5 = Conv2DTranspose(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d4) # Shape : (8, 8, 256)\n",
    "    u5 = UpSampling2D((2, 2))(u5) # Shape : (16, 16, 256)\n",
    "    concat5 = Concatenate()([u5, c3]) # Shape : (16, 16, 256)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    d5 = Dropout(dropout_rate)(c5)\n",
    "    \n",
    "    u6 = Conv2DTranspose(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d5) # Shape : (16, 16, 128)\n",
    "    u6 = UpSampling2D((2, 2))(u6) # Shape : (32, 32, 128)\n",
    "    concat6 = Concatenate()([u6, c2]) # Shape : (32, 32, 128)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    d6 = Dropout(dropout_rate)(c6)\n",
    "    \n",
    "    u7 = Conv2DTranspose(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d6) # Shape : (32, 32, 64)\n",
    "    u7 = UpSampling2D((2, 2))(u7) # Shape : (64, 64, 64)\n",
    "    concat7 = Concatenate()([u7, c1])  # Shape : (64, 64, 64)\n",
    "    c7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat7) # Shape : (64, 64, 64)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Conv2D(1, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_uniform')(c7) # Shape : (64, 64, 1)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    \n",
    "    # Output layer\n",
    "    flat = Flatten()(c7) # Shape : 64*64*1\n",
    "    output = Dense(1, activation = 'linear')(flat) # Shape : 1\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creating and checking if saved_models folder exists or not. If not, then create a folder where we will store our weights.h5 file.\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "best_model_filepath = os.path.join(checkpoint_dir, 'modelweights_lip.h5')\n",
    "# Callbacks can be of best weights or last weights. We can do either one. We went with saving the weights after every epoch. \n",
    "best_model_checkpoint_callback = ModelCheckpoint(filepath = best_model_filepath, save_weights_only = True, verbose = 1)\n",
    "\n",
    "# Applying data augmentation to Training data.\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "# Filepaths for Train, Valid, Test. From the paper we know that for Train, Valid we will choose data points from everywhere except the location in which\n",
    "# we will test our model's performance. Location = Lippendorf\n",
    "test_set = 'curated/lip_test_dataset.nc'\n",
    "valid_set = 'curated/lip_valid_dataset.nc'\n",
    "train_set = 'curated/lip_train_dataset.nc'\n",
    "\n",
    "# Here, we have generated our custom dataset.\n",
    "train_dataset = CustomDataset(train_set, variable = 'no2_noisy')\n",
    "valid_dataset = CustomDataset(valid_set, variable = 'no2_noisy')\n",
    "test_dataset = CustomDataset(test_set, variable = 'no2_noisy')\n",
    "\n",
    "# To store the inputs and outputs from our dataset to feed it to the model during training, validation and inference.\n",
    "train_inputs = []\n",
    "train_outputs = []\n",
    "val_inputs = []\n",
    "val_outputs = []\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "\n",
    "for inputs, outputs in train_dataset:\n",
    "    train_inputs.append(inputs)\n",
    "    train_outputs.append(outputs)\n",
    "for inputs, outputs in valid_dataset:\n",
    "    val_inputs.append(inputs)\n",
    "    val_outputs.append(outputs)\n",
    "for inputs, outputs in test_dataset:\n",
    "    test_inputs.append(inputs)\n",
    "    test_outputs.append(outputs)\n",
    "\n",
    "# Converting into numpy arrays\n",
    "train_inputs = np.array(train_inputs) # train_inputs shape : (25152, 64, 64, 4)\n",
    "train_outputs = np.array(train_outputs) # train_outputs shape : (25152, 1)\n",
    "val_inputs = np.array(val_inputs) # val_inputs shape : (4608, 64, 64, 4)\n",
    "val_outputs = np.array(val_outputs) # val_outputs shape : (4608, 1)\n",
    "test_inputs = np.array(test_inputs) # test_inputs shape : (6289, 64, 64, 4)\n",
    "test_outputs = np.array(test_outputs) # test_outputs shape : (6289, 1)\n",
    "\n",
    "model = unetreg(input_shape = (64, 64, 4), dropout_rate = 0.2) # Declaring the model\n",
    "#model.load_weights('saved_models/modelweights_lip.h5') # If we have already stored the weights, then uncomment this line to load.\n",
    "\n",
    "optimizer = Adam(learning_rate = 1e-3) # Taking Adam optimizer with lr = 1e-3\n",
    "\n",
    "# We will update our lr if we reach a plateau. For this we use ReduceLROnPlateau which will monitor val_loss, wait for 20 epochs before changing lr and measures \n",
    "# min_delta threshold, reduces lr by 0.5 and lower bound on lr is 5e-5.\n",
    "learning_rate_monitor_callback = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 20, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-5)\n",
    "\n",
    "# Loss function for this task is MeanAbsoluteError and metrics are MeanAbsolutePercentageError, MeanSquaredError.\n",
    "model.compile(optimizer = optimizer, loss = 'mae', metrics = ['mape'])\n",
    "\n",
    "# Generate the summary of the model. Around 8 Million trainable paramaters are there.\n",
    "model.summary()\n",
    "\n",
    "# Total epochs = 200, batch size for training set = 32, steps will be 25152/32 = 786, validation will not have any augmentation with batch size of 32 and validating\n",
    "# on the entire validation set.\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 120, steps_per_epoch = len(train_inputs)//32,\n",
    "validation_data = (val_inputs, val_outputs), validation_batch_size = 32, validation_steps = None, callbacks = [learning_rate_monitor_callback, best_model_checkpoint_callback])\n",
    "\n",
    "# To test the model's performance on test set.\n",
    "test_loss, test_metrics = model.evaluate(test_inputs, test_outputs, batch_size = None) # Uncomment these lines to infer model's performance.\n",
    "print(f'Test Loss: {test_loss}, Test MAPE: {test_metrics}')\n",
    "\n",
    "# To store the true_emission and predicted emissions for the .csv file.\n",
    "true_emissions = []\n",
    "predicted_emissions = []\n",
    "for inputs, targets in test_dataset:\n",
    "    inputs = np.expand_dims(inputs, axis = 0)\n",
    "    outputs = model.predict(inputs) # Predicting the output based on the model's learning.\n",
    "    print(\"True value : \", targets)\n",
    "    print(\"Predicted value : \", outputs)\n",
    "    true_emissions.append(targets) # Append the true emissions into true_emissions list\n",
    "    predicted_emissions.append(outputs) # Append the predicted emissions into predicted_emissions list\n",
    "\n",
    "# Converting into numpy arrays\n",
    "true_emissions = np.array(true_emissions) # Shape : 6289, 3\n",
    "predicted_emissions = np.array(predicted_emissions) # Shape : 6289, 1, 3\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape) # After reshaping : 6289, 1, 3\n",
    "\n",
    "# Storing the true emissions and predicted emissions in a csv file. \n",
    "df = pd.DataFrame({'True Emissions': true_emissions.flatten(), 'Predicted Emissions': predicted_emissions.flatten()})\n",
    "\n",
    "df.to_csv('emissions_results_lip.csv')\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffc09f-b21e-45d5-970d-dce428abc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" After doing it twice, we now look at error, absolute error, relative errors. \"\"\" \n",
    "results_df = pd.read_csv('emissions_results_lip.csv') # Reading emissions_results\n",
    "\n",
    "# Creating new column error which is true-pred.\n",
    "results_df['error'] = (results_df['True Emissions'] - results_df['Average_Emissions']).round(3)\n",
    "\n",
    "# Creating new column absolute_error which is the absolute value of error rounded off to 3 decimal places.\n",
    "results_df['absolute_error'] = (abs(results_df['error'])).round(3)\n",
    "\n",
    "# Creating new column relative_error which is the relative value of error rounded off to 3 decimal places with epsilon = 1e-15.\n",
    "results_df['relative_error'] = ((results_df['error'] / (results_df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "# Updates the csv file\n",
    "results_df.to_csv('emissions_results_lip.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89511016-3310-42dd-ae98-8a5a21de4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emissions_results_lip.csv') # Reading emissions_results\n",
    "\n",
    "# We want to know the Q1, Q2, Q3 values or (25%, 50%, 75%) values from our absolute_error distribution.\n",
    "percentiles = df['absolute_error'].describe()\n",
    "#percentiles = [0.25, 0.5, 0.75]\n",
    "percentiles['25%'] = abs(percentiles['25%'])\n",
    "percentiles['50%'] = abs(percentiles['50%'])\n",
    "percentiles['75%'] = abs(percentiles['75%'])\n",
    "\n",
    "# Only the percentiles are of impportant to us.\n",
    "#percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 2.53, 50% : 5.07, 75% : 7.94 (All are in Mt/yr)\n",
    "\n",
    "# Stated in the paper, the median absolute error is 3 Mt/yr. We got 2.3 Mt/ yr which means we are at a -0.74 Mt/yr deviation which is better than the paper.\n",
    "\n",
    "# The true percentile values as mentioned in the paper.\n",
    "true_percentiles = [1.3, 2.7, 4.5] # (All are in Mt/yr)\n",
    "\n",
    "# Absolute error between our percentile values and the true percentile values rounded off to 3 decimal places.\n",
    "error = [round(abs(true - pred), 3) for true, pred in zip(true_percentiles, percentiles)] # 1.23, 2.37, 3.4\n",
    "\n",
    "print(percentiles)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0fef0-db66-4d60-bbae-38e7de36d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emissions_results_lip.csv') # Reading emissions_results\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "# We want to know the Q1, Q2, Q3 values or (25%, 50%, 75%) values from our relative_error distribution.\n",
    "percentiles = df['abs_rel_error'].describe()\n",
    "#percentiles = [0.25, 0.5, 0.75]\n",
    "# Only the percentiles are of impportant to us.\n",
    "#percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 17.1, 50% : 34.8, 75% : 52.0 (Unitless)\n",
    "\n",
    "# Stated in the paper, the median absolute relative paper is around 20%. We have got 14.76% which can be approximated to 15%.\n",
    "# This means our model performed better.\n",
    "\n",
    "# The true percentile values as mentioned in the paper.\n",
    "true_percentiles = [8.6, 18.1, 30.3] # Note : These values are percentage values as the relative errors are calculated in terms of percentage.\n",
    "\n",
    "# Absolute error between our percentile values and the true percentile values rounded off to 3 decimal places.\n",
    "error = [round(abs(true - pred), 3) for true, pred in zip(true_percentiles, percentiles)] # 3.6, 6.35, 10.38 \n",
    "#df.to_csv('emissions_results.csv', index = False)\n",
    "print(percentiles)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bdc09-02e5-44fc-be77-e7cb0bbde715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This cell is for plotting the Kernel Density Estimation (KDE) plots using seaborn of absolute_error and relative_error. \"\"\"\n",
    "\n",
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.read_csv('emissions_results_lip.csv') # Reading emissions_results\n",
    "df2 = pd.read_csv('emissions_data_lip.csv') # Reading emissions_data\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df1['absolute_error'], fill = True, color = 'b', label = 'Using U-Net regression') # Using seaborn's kdeplot.\n",
    "sns.kdeplot(df2['absolute_error'], fill = True, color = 'r', label = 'Using CNN regression')\n",
    "plt.title('Kernel Density Estimate of Absolute Error for Lippendorf')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.xlim(0,25)\n",
    "plt.savefig('absolute_error.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df1['relative_error'], fill = True, color = 'b', label = 'Using U-Net regression') # Using seaborn's kdeplot.\n",
    "sns.kdeplot(df2['relative_error'], fill = True, color = 'r', label = 'Using CNN regression')\n",
    "plt.title('Kernel Density Estimate of Relative Error (%) for Lippendorf')\n",
    "plt.xlabel('Relative Error (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.xlim(-110,120)\n",
    "plt.savefig('relative_error.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6a4b0-8ea2-4607-aad5-93150c48b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our absolute errors lie in (0-2) Mt/yr range, in (2-5) Mt/yr range, in (5-10) Mt/yr range\n",
    "and in 10 Mt/yr above. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emissions_results_lip.csv')\n",
    "\n",
    "# useful column\n",
    "abs = df['absolute_error']\n",
    "\n",
    "# get the required info\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 1222\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 1888\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 2425\n",
    "print(\"above 10 : \", ab10.sum()) # 754\n",
    "print(\"mean : \", mean) # 5.55\n",
    "print(\"median : \", median) # 5.071\n",
    "print(\"std : \", std) # 3.77\n",
    "\n",
    "# According to the paper, \" Majority of errors are concentrated below 10 Mt/yr.\" which we can see is true. In our case, we can say that majority of errors are\n",
    "# below 5 Mt/yr which is a significant increase in performance from 10 Mt/yr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9295a-5ad6-42c7-a8d9-2bd868aebf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our relative errors lie less than -150%, in (-150 to -100)% range, in (-100 to -50)% range,\n",
    "in (-50 to 0) %, in (0 to 50) % range, in (50 to 100) % and above 100%. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emissions_results_lip.csv')\n",
    "\n",
    "# useful column\n",
    "rel = df['relative_error']\n",
    "\n",
    "# get the required info\n",
    "lessneg150 = (rel <= -150)\n",
    "neg150_100 = (rel > -150) & (rel <= -100)\n",
    "neg100_50 = (rel > -100) & (rel <= -50)\n",
    "neg50_0 = (rel > -50) & (rel <= 0)\n",
    "in0_50 = (rel > 0) & (rel <= 50)\n",
    "in50_100 = (rel > 50) & (rel <= 100)\n",
    "ab100 = rel > 100\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"less than -150% : \", lessneg150.sum()) # 33\n",
    "print(\"in between -150% and -100% : \", neg150_100.sum()) # 164\n",
    "print(\"in between -100% and -50% : \", neg100_50.sum()) # 778\n",
    "print(\"in between -50% and 0% : \", neg50_0.sum()) # 2627\n",
    "print(\"in between 0% and 50% : \", in0_50.sum()) # 2627\n",
    "print(\"in between 50% and 100% : \", in50_100.sum()) # 776\n",
    "print(\"above 100% : \", ab100.sum()) # 0\n",
    "print(\"mean : \", mean) # -1.56\n",
    "print(\"median : \", median) # 5.38\n",
    "print(\"std : \", std) # 46.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c624af-1db7-44d7-84de-f8af2ba2061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our absolute relative errors lie in (0 to 20) % range, in (20 to 50)% range, in (50 to 100) %, in (100 to 150)% \n",
    "and above 150%. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emissions_results_lip.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "# useful column\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "# get the required info\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 33\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 163\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 1545\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 2734\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 1814\n",
    "print(\"mean : \", mean) # 37.7\n",
    "print(\"median : \", median) # 34.8\n",
    "print(\"std : \", std) # 27.27\n",
    "\n",
    "# According to the paper, \" Majority of errors are concentrated below 10 Mt/yr or 50% with very few exceeding 100%.\" which we can again see is true\n",
    "# In our model also, Majority of the errors are below 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085474a1-af38-449b-b0ca-21a832fa081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code gets us a few statistics about true emissions and predicted emissions. \"\"\"\n",
    "\n",
    "# necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# read\n",
    "df = pd.read_csv('emissions_results_lip.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Average_Emissions']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 7.493\n",
    "print(\"max : \", max_true) # 24.112\n",
    "print(\"mean : \", mean_true) # 15.256\n",
    "print(\"std : \", std_true) # 3.527\n",
    "print(\"median : \", median_true) # 15.264\n",
    "print(\"range : \", range_true) # 16.619\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 3.59\n",
    "print(\"max : \", max_pred) # 41.75\n",
    "print(\"mean : \", mean_pred) # 15.23\n",
    "print(\"std : \", std_pred) # 7.1\n",
    "print(\"median : \", median_pred) # 14.23\n",
    "print(\"range : \", range_pred) # 38.16\n",
    "\n",
    "# The mean true emission = 15.2 Mt/yr and mean pred emission = 14.3 Mt/yr which tantamounts to the fact that our model has come closer to the real true emission values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bfc68d-ca12-49a6-a83c-71fe0e7850b3",
   "metadata": {},
   "source": [
    "Redistributing the dataset to get our own datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eba732-561d-4b46-ae04-aaeff132654e",
   "metadata": {},
   "source": [
    "Working with CNN regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565205e-bd5b-4e19-a65c-761419bc2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing different packages that we will use.\n",
    "\n",
    "\"\"\"\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset as NetCDFDataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Dropout, BatchNormalization, MaxPool2D, Flatten, Dense, GaussianNoise\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "We define our Custom Dataset which will read the .nc files (train, valid, test) and extract the XCO2, u, v, NO2 data from the .nc files.\n",
    "Note to the reader : For our comparision, we are going to consider Lippendorf location and as additional input we will consider NO2 images.\n",
    "\n",
    "\"\"\"\n",
    "class CustomDataset:\n",
    "    \n",
    "    def __init__(self, file_path, variable = None):\n",
    "        self.ncfile = NetCDFDataset(file_path, 'r') # Reading the .nc file\n",
    "        self.variable = variable # Variable in our comparison is no2_noisy\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ncfile.variables['xco2_noisy'].shape[0] # The total number of images or data we will have which is different for train (25152), valid (4608), test (6289)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Reading the necessary variables (xco2_noisy, u, v)\n",
    "        xco2_read = self.ncfile.variables['xco2_noisy'][idx] # Unit : ppmv (parts per million by volume), Shape : 64, 64\n",
    "        u_read = self.ncfile.variables['u'][idx] # Unit : m/s, Shape : 64, 64\n",
    "        v_read = self.ncfile.variables['v'][idx] # Unit : m/s, Shape : 64, 64\n",
    "        emiss_read = self.ncfile.variables['emiss'][idx] # emiss is the true output data that we need to train our model. Unit : Mt/yr (Million tonnes/ year)\n",
    "                                                         # Shape : 3,\n",
    "        # Converting into numpy arrays with data type float32\n",
    "        xco2_arr = np.array(xco2_read.data).astype('float32') \n",
    "        u_arr = np.array(u_read.data).astype('float32')\n",
    "        v_arr = np.array(v_read.data).astype('float32')\n",
    "        emiss_arr = np.array(emiss_read.data).astype('float32')\n",
    "\n",
    "        #noise = GaussianNoise(stddev = 0.7) # Addition of Gaussian Noise with std of 0.7\n",
    "        #xco2 = noise(xco2_arr).numpy() \n",
    "        if self.variable:\n",
    "            var_read = self.ncfile.variables[self.variable][idx] # Unit : molec/cm^2, Shape : 64, 64\n",
    "            var_arr = np.array(var_read.data).astype('float32') # Converting into numpy array after reading the variable\n",
    "            #var_noise = noise(var_tensor).numpy()\n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr, var_arr], axis = -1) # Stacking the inputs to get the desired input shape = (64, 64, 4) where 64, 64\n",
    "                                                                            # represents height, width and 4 represents the total number of features. Each pixel\n",
    "                                                                            # has a spatial resolution of 2km, i.e. 1*1 in pixel refers to an area of 2km*2km.\n",
    "        else: \n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr], axis = -1) # Stacking the inputs to get the shape = (64, 64, 3) in case when variable is None.\n",
    "        \n",
    "        mean = inputs.mean(axis = (0, 1), keepdims = True) # Calculating the mean of the 4 features separately and keeping the same shape as that of inputs.\n",
    "        \n",
    "        std = inputs.std(axis = (0, 1), keepdims = True) # Calculating the std of the 4 features separately and keeping the same shape as that of inputs.\n",
    "        \n",
    "        std[std==0] = 1 # If std becomes 0, we change it 1 to avoid division by 0. Multiple other methods can also be employed such as choosing an \n",
    "                        # appropriate epsilon such as 1e-5, 1e-6 or 1e-7\n",
    "        \n",
    "        inputs = (inputs - mean)/std # Normalization of the inputs separately for each feature.\n",
    "        \n",
    "        # emiss data is of shape (3,), from which we will only consider the middle value for our comparison. Hence, we multiply weights with emiss to\n",
    "        # get the middle value and round it off to 3 decimal places.\n",
    "        weights = np.array([0.0, 1.0, 0.0]) \n",
    "        weighted = np.round(np.sum(weights * emiss_arr), 3)\n",
    "        outputs = np.array([weighted], dtype = np.float32)\n",
    "        \n",
    "        return inputs, outputs # returns our inputs (64, 64, 4), outputs (1,)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.ncfile.close() # Closing the ncfile to avoid corruption of the file.\n",
    "\n",
    "\"\"\"\n",
    "The model architecture is taking from the paper which consists of 2D convolution, Maxpooling, Dropout, Batch Normalization, Flatten and finally Dense.\n",
    "\n",
    "\"\"\"\n",
    "def model_arch(input_shape): # input_shape = 64, 64, 4\n",
    "    model = Sequential()\n",
    "    \n",
    "    # The first convolution with kernel size of (3,3), total number of filters = 32, activation function = elu (Exponential Linear Unit), strides = 1, \n",
    "    # padding = valid (no padding).\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1, input_shape = input_shape)) # Shape after conv : 62, 62, 32\n",
    "    \n",
    "    # Adding a Dropout of 0.1\n",
    "    model.add(Dropout(0.1)) # No change in shape\n",
    "    \n",
    "    # Second convolution with kernel size of (3,3), total number of filters = 32, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 60, 60, 32\n",
    "    \n",
    "    # Maxpool with pool size of 2, 2 and strides = 2.\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2)) # Shape after maxpool : 30, 30, 32\n",
    "    \n",
    "    model.add(BatchNormalization()) # Batch Normalization which does not affect the shape\n",
    "    \n",
    "    # Third convolution with kernel size of (3,3), total number of filters = 32, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 28, 28, 32\n",
    "    \n",
    "    # Adding a Dropout of 0.2\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Fourth convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 26, 26, 64\n",
    "    \n",
    "    model.add(BatchNormalization()) # Batch Normalization which does not affect the shape\n",
    "    \n",
    "    # Fifth convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 24, 24, 64\n",
    "    \n",
    "    # Adding a Dropout of 0.2\n",
    "    model.add(Dropout(0.2)) # No change in shape\n",
    "    \n",
    "    # Sixth convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 22, 22, 64\n",
    "    \n",
    "    # Maxpool with pool size of 2, 2 and strides = 2.\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2)) # Shape after maxpool : 11, 11, 64\n",
    "    \n",
    "    # Seventh convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 9, 9, 64\n",
    "    \n",
    "    # Adding a Dropout of 0.2\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Eighth convolution with kernel size of (3,3), total number of filters = 64, activation function = elu (Exponential Linear Unit), strides = 1,\n",
    "    # padding = valid (no padding)\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1)) # Shape after conv : 7, 7, 64\n",
    "    \n",
    "    # Maxpool with pool size of 2, 2 and strides = 2.\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2)) # Shape after maxpool : 3, 3, 64\n",
    "    \n",
    "    model.add(Flatten()) # Total = 3*3*64 = 576\n",
    "    \n",
    "    model.add(Dense(1)) # Fully Connected Layer to get a single output.\n",
    "    \n",
    "    model.add(LeakyReLU(alpha = 0.3)) # The output goes through activation function = Leaky Rectified Linear Unit with negative slope = 0.3.\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creating and checking if saved_models folder exists or not. If not, then create a folder where we will store our weights.h5 file.\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "best_model_filepath = os.path.join(checkpoint_dir, 'best_model_weights_new.h5')\n",
    "# Callbacks can be of best weights or last weights. We can do either one. We went with saving the weights after every epoch. \n",
    "best_model_checkpoint_callback = ModelCheckpoint(filepath = best_model_filepath, save_weights_only = True, verbose = 1)\n",
    "\n",
    "# Applying data augmentation to Training data as specified by the paper.\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "# Filepaths for Train, Valid, Test. From the paper we know that for Train, Valid we will choose data points from everywhere except the location in which\n",
    "# we will test our model's performance. Location = Lippendorf\n",
    "test_set = 'curated/lip_test_dataset.nc'\n",
    "valid_set = 'curated/lip_valid_dataset.nc'\n",
    "train_set = 'curated/lip_train_dataset.nc'\n",
    "\n",
    "# Here, we have generated our custom dataset.\n",
    "train_dataset = CustomDataset(train_set, variable = 'no2_noisy')\n",
    "valid_dataset = CustomDataset(valid_set, variable = 'no2_noisy')\n",
    "test_dataset = CustomDataset(test_set, variable = 'no2_noisy')\n",
    "\n",
    "# To store the inputs and outputs from our dataset.\n",
    "combine_inputs = []\n",
    "combine_outputs = []\n",
    "\n",
    "for dataset in [train_dataset, valid_dataset, test_dataset]:\n",
    "    for inputs, outputs in dataset:\n",
    "        combine_inputs.append(inputs)\n",
    "        combine_outputs.append(outputs)\n",
    "\n",
    "# Converting into numpy arrays\n",
    "combine_inputs = np.array(combine_inputs) # Shape : (36049, 64, 64, 4)\n",
    "combine_outputs = np.array(combine_outputs) # Shape : (36049, 1)\n",
    "\n",
    "indices = np.arange(combine_inputs.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "combine_inputs = combine_inputs[indices]\n",
    "combine_outputs = combine_outputs[indices]\n",
    "\n",
    "# Train, Val, Test lists\n",
    "train_size = int(0.65 * combine_inputs.shape[0])\n",
    "valid_size = int(0.15 * combine_inputs.shape[0])\n",
    "test_size = combine_inputs.shape[0] - train_size - valid_size\n",
    "\n",
    "train_inputs, val_inputs, test_inputs = np.split(combine_inputs, [train_size, train_size + valid_size])\n",
    "train_outputs, val_outputs, test_outputs = np.split(combine_outputs, [train_size, train_size + valid_size])\n",
    "\n",
    "# Delete unnecessary variables to free up memory\n",
    "del combine_inputs\n",
    "del combine_outputs\n",
    "del train_dataset\n",
    "del valid_dataset\n",
    "del test_dataset\n",
    "del indices\n",
    "del train_size\n",
    "del valid_size\n",
    "del test_size\n",
    "\n",
    "# Shapes for our data : \n",
    "# Train input Shape : (23431, 64, 64, 4), Train output Shape : (23431, 1)\n",
    "# Valid input Shape : (5407, 64, 64, 4), Valid output Shape :  (5407, 1)\n",
    "# Test input Shape : (7211, 64, 64, 4), Test output Shape : (7211, 1)\n",
    "\n",
    "model = model_arch(input_shape = (64, 64, 4)) # Declaring the model\n",
    "\n",
    "model.load_weights('saved_models/best_model_weights_new.h5') # If we have already stored the weights, then uncomment this line to load.\n",
    "\n",
    "optimizer = Adam(learning_rate = 1e-3) # Taking Adam optimizer with lr = 1e-3\n",
    "\n",
    "# We will update our lr if we reach a plateau. For this we use ReduceLROnPlateau which will monitor val_loss, wait for 20 epochs before changing lr and measures \n",
    "# min_delta threshold, reduces lr by 0.5 and lower bound on lr is 5e-5.\n",
    "learning_rate_monitor_callback = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 20, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-5)\n",
    "\n",
    "# Loss function for this task is MeanAbsoluteError and metrics is MeanAbsolutePercentageError.\n",
    "model.compile(optimizer = optimizer, loss = 'mae', metrics = ['mape'])\n",
    "\n",
    "# Generate the summary of the model. Around 186000 trainable paramaters are there.\n",
    "model.summary()\n",
    "\n",
    "# Total epochs = 500, batch size for training set = 32, steps will be 23431/32 = 732, validation will not have any augmentation with batch size of 32 and validating\n",
    "# on the entire validation set.\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 500, steps_per_epoch = len(train_inputs)//32,\n",
    "validation_data = (val_inputs, val_outputs), validation_batch_size = 32, validation_steps = None, callbacks = [learning_rate_monitor_callback, best_model_checkpoint_callback])\n",
    "\n",
    "# To test the model's performance on test set.\n",
    "test_loss, test_mae = model.evaluate(test_inputs, test_outputs, batch_size = None) # Uncomment these lines to infer model's performance.\n",
    "print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')\n",
    "\n",
    "# To store the true_emission and predicted emissions for the .csv file.\n",
    "true_emissions = []\n",
    "predicted_emissions = []\n",
    "for i in range(len(test_inputs)):\n",
    "    inputs = np.expand_dims(test_inputs[i], axis = 0)\n",
    "    outputs = model.predict(inputs)  # Predicting the output based on the model's learning.\n",
    "    print(\"True value : \", test_outputs[i])\n",
    "    print(\"Predicted value : \", outputs)\n",
    "    true_emissions.append(test_outputs[i])  # Append the true emissions into true_emissions list\n",
    "    predicted_emissions.append(outputs)  # Append the predicted emissions into predicted_emissions list\n",
    "\n",
    "# Converting into numpy arrays\n",
    "true_emissions = np.array(true_emissions)  # Shape : 7211, 3\n",
    "predicted_emissions = np.array(predicted_emissions)  # Shape : 7211, 1, 3\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape)  # After reshaping : 7211, 1, 3\n",
    "\n",
    "# Storing the true emissions and predicted emissions in a csv file.\n",
    "df = pd.DataFrame({'True Emissions': true_emissions.flatten(), 'Predicted Emissions': predicted_emissions.flatten()}, index = np.arange(1, len(true_emissions) + 1))\n",
    "df.to_csv('emiss_data.csv')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567cfeb7-1f5d-468e-b7f7-4fb30cc63c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_outputs = np.load('train_comb_outputs.npy')\n",
    "valid_outputs = np.load('valid_comb_outputs.npy')\n",
    "test_outputs = np.load('test_comb_outputs.npy')\n",
    "\n",
    "def compute_statistics(data):\n",
    "    \n",
    "    \n",
    "    total_min = np.min(data)\n",
    "    total_max = np.max(data) \n",
    "    total_range = total_max - total_min\n",
    "    total_mean = np.mean(data)\n",
    "    total_median = np.median(data)\n",
    "    \n",
    "    return {\n",
    "        \n",
    "        'total_min': total_min,\n",
    "        'total_max': total_max,\n",
    "        'total_range': total_range,\n",
    "        'total_mean': total_mean,\n",
    "        'total_median': total_median\n",
    "    }\n",
    "\n",
    "train_stats = compute_statistics(train_outputs)\n",
    "valid_stats = compute_statistics(valid_outputs)\n",
    "test_stats = compute_statistics(test_outputs)\n",
    "\n",
    "print(\"Train Outputs Statistics:\")\n",
    "for key, value in train_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nValidation Outputs Statistics:\")\n",
    "for key, value in valid_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nTest Outputs Statistics:\")\n",
    "for key, value in test_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "bins = np.concatenate([np.arange(0, 5.5, 0.5), np.arange(5, 25, 2), [np.inf]])\n",
    "\n",
    "def calculate_percentage(data, bins):\n",
    "    counts, _ = np.histogram(data, bins = bins)\n",
    "    total_count = np.sum(counts)\n",
    "    print(total_count)\n",
    "    percentages = (counts / total_count) * 100\n",
    "    return counts, percentages\n",
    "\n",
    "train_counts, train_percentages = calculate_percentage(train_outputs.flatten(), bins)\n",
    "valid_counts, valid_percentages = calculate_percentage(valid_outputs.flatten(), bins)\n",
    "test_counts, test_percentages = calculate_percentage(test_outputs.flatten(), bins)\n",
    "\n",
    "def stats(name, counts, percentages, bins):\n",
    "    print(f'\\n{name} Data Histogram:')\n",
    "    for i in range(len(counts)):\n",
    "        print(f'Bin range {bins[i]:.1f} to {bins[i+1]:.1f} (or {bins[i]} to {bins[i+1]}):')\n",
    "        print(f'  Count: {counts[i]}')\n",
    "        print(f'  Percentage: {percentages[i]:.2f}%')\n",
    "\n",
    "stats('Train', train_counts, train_percentages, bins)\n",
    "stats('Valid', valid_counts, valid_percentages, bins)\n",
    "stats('Test', test_counts, test_percentages, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1f2de-a768-4403-8264-76d678cd6e9d",
   "metadata": {},
   "source": [
    "Working with U-Net regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e5129-2700-449b-9a63-506feeb91740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This marks the beginning of our novel approach that is using U-Net for regression task. \"\"\"\n",
    "\"\"\"\n",
    "Importing different packages that we will use.\n",
    "\n",
    "\"\"\"\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset as NetCDFDataset\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dropout, BatchNormalization, MaxPool2D, Concatenate, Conv2DTranspose, Flatten, Dense, UpSampling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "We define our Custom Dataset which will read the .nc files (train, valid, test) and extract the XCO2, u, v, NO2 data from the .nc files.\n",
    "Note to the reader : For our comparision, we are going to consider Lippendorf location and as additional input we will consider NO2 images.\n",
    "\n",
    "\"\"\"\n",
    "class CustomDataset:\n",
    "    \n",
    "    def __init__(self, file_path, variable = None):\n",
    "        self.ncfile = NetCDFDataset(file_path, 'r') # Reading the .nc file\n",
    "        self.variable = variable # Variable in our comparison is no2_noisy\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ncfile.variables['xco2_noisy'].shape[0] # The total number of images or data we will have which is different for train (25152), valid (4608), test (6289)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Reading the necessary variables (xco2_noisy, u, v)\n",
    "        xco2_read = self.ncfile.variables['xco2_noisy'][idx] # Unit : ppmv (parts per million by volume), Shape : 64, 64\n",
    "        u_read = self.ncfile.variables['u'][idx] # Unit : m/s, Shape : 64, 64\n",
    "        v_read = self.ncfile.variables['v'][idx] # Unit : m/s, Shape : 64, 64\n",
    "        emiss_read = self.ncfile.variables['emiss'][idx] # emiss is the true output data that we need to train our model. Unit : Mt/yr (Million tonnes/ year)\n",
    "                                                         # Shape : 3,\n",
    "        # Converting into numpy arrays with data type float32\n",
    "        xco2_arr = np.array(xco2_read.data).astype('float32') \n",
    "        u_arr = np.array(u_read.data).astype('float32')\n",
    "        v_arr = np.array(v_read.data).astype('float32')\n",
    "        emiss_arr = np.array(emiss_read.data).astype('float32')\n",
    "\n",
    "        #noise = GaussianNoise(stddev = 0.7) # Addition of Gaussian Noise with std of 0.7\n",
    "        #xco2 = noise(xco2_arr).numpy() \n",
    "        if self.variable:\n",
    "            var_read = self.ncfile.variables[self.variable][idx] # Unit : molec/cm^2, Shape : 64, 64\n",
    "            var_arr = np.array(var_read.data).astype('float32') # Converting into numpy array after reading the variable\n",
    "            #var_noise = noise(var_tensor).numpy()\n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr, var_arr], axis = -1) # Stacking the inputs to get the desired input shape = (64, 64, 4) where 64, 64\n",
    "                                                                            # represents height, width and 4 represents the total number of features. Each pixel\n",
    "                                                                            # has a spatial resolution of 2km, i.e. 1*1 in pixel refers to an area of 2km*2km.\n",
    "        else: \n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr], axis = -1) # Stacking the inputs to get the shape = (64, 64, 3) in case when variable is None.\n",
    "        \n",
    "        mean = inputs.mean(axis = (0, 1), keepdims = True) # Calculating the mean of the 4 features separately and keeping the same shape as that of inputs.\n",
    "        \n",
    "        std = inputs.std(axis = (0, 1), keepdims = True) # Calculating the std of the 4 features separately and keeping the same shape as that of inputs.\n",
    "        \n",
    "        std[std==0] = 1 # If std becomes 0, we change it 1 to avoid division by 0. Multiple other methods can also be employed such as choosing an \n",
    "                        # appropriate epsilon such as 1e-5, 1e-6 or 1e-7\n",
    "        \n",
    "        inputs = (inputs - mean)/std # Normalization of the inputs separately for each feature.\n",
    "        \n",
    "        # emiss data is of shape (3,), from which we will only consider the middle value for our comparison. Hence, we multiply weights with emiss to\n",
    "        # get the middle value and round it off to 3 decimal places.\n",
    "        weights = np.array([0.0, 1.0, 0.0]) \n",
    "        weighted = np.round(np.sum(weights * emiss_arr), 3)\n",
    "        outputs = np.array([weighted], dtype = np.float32)\n",
    "        \n",
    "        return inputs, outputs # returns our inputs (64, 64, 4), outputs (1,)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.ncfile.close() # Closing the ncfile to avoid corruption of the file.\n",
    "\n",
    "\"\"\" The U-Net regression Model \"\"\"\n",
    "\n",
    "def unetreg(input_shape, dropout_rate = 0.2):\n",
    "    inputs = Input(input_shape) # If we use no2 then shape : (64, 64, 4), if we don't use no2 then shape : (64, 64, 3)\n",
    "    \n",
    "    # Encoder (downsampling path)\n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(inputs) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c1) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    d1 = Dropout(dropout_rate)(c1) \n",
    "    p1 = MaxPool2D(pool_size = (2, 2))(d1) # Shape : (32, 32, 64) \n",
    "    \n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p1) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c2) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    d2 = Dropout(dropout_rate)(c2)\n",
    "    p2 = MaxPool2D(pool_size = (2, 2))(d2) # Shape : (16, 16, 128)\n",
    "    \n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p2) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c3) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    d3 = Dropout(dropout_rate)(c3)\n",
    "    p3 = MaxPool2D(pool_size = (2, 2))(d3) # Shape : (8, 8, 256)\n",
    "\n",
    "    # Bottleneck\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p3) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c4) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    d4 = Dropout(dropout_rate)(c4)\n",
    "    \n",
    "    # Decoder (upsampling path)\n",
    "    u5 = Conv2DTranspose(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d4) # Shape : (8, 8, 256)\n",
    "    u5 = UpSampling2D((2, 2))(u5) # Shape : (16, 16, 256)\n",
    "    concat5 = Concatenate()([u5, c3]) # Shape : (16, 16, 256)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    d5 = Dropout(dropout_rate)(c5)\n",
    "    \n",
    "    u6 = Conv2DTranspose(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d5) # Shape : (16, 16, 128)\n",
    "    u6 = UpSampling2D((2, 2))(u6) # Shape : (32, 32, 128)\n",
    "    concat6 = Concatenate()([u6, c2]) # Shape : (32, 32, 128)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    d6 = Dropout(dropout_rate)(c6)\n",
    "    \n",
    "    u7 = Conv2DTranspose(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d6) # Shape : (32, 32, 64)\n",
    "    u7 = UpSampling2D((2, 2))(u7) # Shape : (64, 64, 64)\n",
    "    concat7 = Concatenate()([u7, c1])  # Shape : (64, 64, 64)\n",
    "    c7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat7) # Shape : (64, 64, 64)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Conv2D(1, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_uniform')(c7) # Shape : (64, 64, 1)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    \n",
    "    # Output layer\n",
    "    flat = Flatten()(c7) # Shape : 64*64*1\n",
    "    output = Dense(1, activation = 'linear')(flat) # Shape : 1\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creating and checking if saved_models folder exists or not. If not, then create a folder where we will store our weights.h5 file.\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "best_model_filepath = os.path.join(checkpoint_dir, 'modelweights_new.h5')\n",
    "# Callbacks can be of best weights or last weights. We can do either one. We went with saving the weights after every epoch. \n",
    "best_model_checkpoint_callback = ModelCheckpoint(filepath = best_model_filepath, save_weights_only = True, verbose = 1)\n",
    "\n",
    "# Applying data augmentation to Training data.\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "# Filepaths for Train, Valid, Test. From the paper we know that for Train, Valid we will choose data points from everywhere except the location in which\n",
    "# we will test our model's performance. Location = Lippendorf\n",
    "test_set = 'curated/lip_test_dataset.nc'\n",
    "valid_set = 'curated/lip_valid_dataset.nc'\n",
    "train_set = 'curated/lip_train_dataset.nc'\n",
    "\n",
    "# Here, we have generated our custom dataset.\n",
    "train_dataset = CustomDataset(train_set, variable = 'no2_noisy')\n",
    "valid_dataset = CustomDataset(valid_set, variable = 'no2_noisy')\n",
    "test_dataset = CustomDataset(test_set, variable = 'no2_noisy')\n",
    "\n",
    "# To store the inputs and outputs from our dataset.\n",
    "combine_inputs = []\n",
    "combine_outputs = []\n",
    "\n",
    "for dataset in [train_dataset, valid_dataset, test_dataset]:\n",
    "    for inputs, outputs in dataset:\n",
    "        combine_inputs.append(inputs)\n",
    "        combine_outputs.append(outputs)\n",
    "\n",
    "# Converting into numpy arrays\n",
    "combine_inputs = np.array(combine_inputs) # Shape : (36049, 64, 64, 4)\n",
    "combine_outputs = np.array(combine_outputs) # Shape : (36049, 1)\n",
    "\n",
    "indices = np.arange(combine_inputs.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "combine_inputs = combine_inputs[indices]\n",
    "combine_outputs = combine_outputs[indices]\n",
    "\n",
    "# Train, Val, Test lists\n",
    "train_size = int(0.65 * combine_inputs.shape[0])\n",
    "valid_size = int(0.15 * combine_inputs.shape[0])\n",
    "test_size = combine_inputs.shape[0] - train_size - valid_size\n",
    "\n",
    "train_inputs, val_inputs, test_inputs = np.split(combine_inputs, [train_size, train_size + valid_size])\n",
    "train_outputs, val_outputs, test_outputs = np.split(combine_outputs, [train_size, train_size + valid_size])\n",
    "\n",
    "# Delete unnecessary variables to free up memory\n",
    "del combine_inputs\n",
    "del combine_outputs\n",
    "del train_dataset\n",
    "del valid_dataset\n",
    "del test_dataset\n",
    "del indices\n",
    "del train_size\n",
    "del valid_size\n",
    "del test_size\n",
    "\n",
    "# Shapes for our data : \n",
    "# Train input Shape : (23431, 64, 64, 4), Train output Shape : (23431, 1)\n",
    "# Valid input Shape : (5407, 64, 64, 4), Valid output Shape :  (5407, 1)\n",
    "# Test input Shape : (7211, 64, 64, 4), Test output Shape : (7211, 1)\n",
    "\n",
    "model = unetreg(input_shape = (64, 64, 4), dropout_rate = 0.2) # Declaring the model\n",
    "#model.load_weights('saved_models/modelweights_new.h5') # If we have already stored the weights, then uncomment this line to load.\n",
    "\n",
    "optimizer = Adam(learning_rate = 1e-3) # Taking Adam optimizer with lr = 1e-3\n",
    "\n",
    "# We will update our lr if we reach a plateau. For this we use ReduceLROnPlateau which will monitor val_loss, wait for 20 epochs before changing lr and measures \n",
    "# min_delta threshold, reduces lr by 0.5 and lower bound on lr is 5e-5.\n",
    "learning_rate_monitor_callback = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 20, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-5)\n",
    "\n",
    "# Loss function for this task is MeanAbsoluteError and metrics are MeanAbsolutePercentageError, MeanSquaredError.\n",
    "model.compile(optimizer = optimizer, loss = 'mae', metrics = ['mape'])\n",
    "\n",
    "# Generate the summary of the model. Around 8 Million trainable paramaters are there.\n",
    "model.summary()\n",
    "\n",
    "# Total epochs = 200, batch size for training set = 32, steps will be 25152/32 = 786, validation will not have any augmentation with batch size of 32 and validating\n",
    "# on the entire validation set.\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 200, steps_per_epoch = len(train_inputs)//32,\n",
    "validation_data = (val_inputs, val_outputs), validation_batch_size = 32, validation_steps = None, callbacks = [learning_rate_monitor_callback, best_model_checkpoint_callback])\n",
    "\n",
    "# To test the model's performance on test set.\n",
    "test_loss, test_metrics = model.evaluate(test_inputs, test_outputs, batch_size = None) # Uncomment these lines to infer model's performance.\n",
    "print(f'Test Loss: {test_loss}, Test MAPE: {test_metrics}')\n",
    "\n",
    "# To store the true_emission and predicted emissions for the .csv file.\n",
    "true_emissions = []\n",
    "predicted_emissions = []\n",
    "for i in range(len(test_inputs)):\n",
    "    inputs = np.expand_dims(test_inputs[i], axis = 0)\n",
    "    outputs = model.predict(inputs)  # Predicting the output based on the model's learning.\n",
    "    print(\"True value : \", test_outputs[i])\n",
    "    print(\"Predicted value : \", outputs)\n",
    "    true_emissions.append(test_outputs[i])  # Append the true emissions into true_emissions list\n",
    "    predicted_emissions.append(outputs)  # Append the predicted emissions into predicted_emissions list\n",
    "\n",
    "# Converting into numpy arrays\n",
    "true_emissions = np.array(true_emissions)  # Shape : 7211, 3\n",
    "predicted_emissions = np.array(predicted_emissions)  # Shape : 7211, 1, 3\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape)  # After reshaping : 7211, 1, 3\n",
    "\n",
    "# Storing the true emissions and predicted emissions in a csv file.\n",
    "df = pd.DataFrame({'True Emissions': true_emissions.flatten(), 'Predicted Emissions': predicted_emissions.flatten()}, index = np.arange(1, len(true_emissions) + 1))\n",
    "df.to_csv('emiss_results.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62ada3-c5e6-40db-a1c4-7c9dcb45f061",
   "metadata": {},
   "source": [
    "Error Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269eb10-caf2-426f-a915-cc120f0aa83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" After doing it twice, we now look at error, absolute error, relative errors. \"\"\" \n",
    "results_df = pd.read_csv('emiss_data.csv') # Reading emiss_data\n",
    "\n",
    "# Creating new column error which is true-pred.\n",
    "results_df['error'] = (results_df['True Emissions'] - results_df['Predicted Emissions']).round(3)\n",
    "\n",
    "# Creating new column absolute_error which is the absolute value of error rounded off to 3 decimal places.\n",
    "results_df['absolute_error'] = (abs(results_df['error'])).round(3)\n",
    "\n",
    "# Creating new column relative_error which is the relative value of error rounded off to 3 decimal places with epsilon = 1e-15.\n",
    "results_df['relative_error'] = ((results_df['error'] / (results_df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "# Updates the csv file\n",
    "results_df.to_csv('emiss_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d5632-87ca-4ba8-ba1e-d144ff4fcc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" After doing it twice, we now look at error, absolute error, relative errors. \"\"\" \n",
    "results_df = pd.read_csv('emiss_results.csv') # Reading emiss_results\n",
    "\n",
    "# Creating new column error which is true-pred.\n",
    "results_df['error'] = (results_df['True Emissions'] - results_df['Predicted Emissions']).round(3)\n",
    "\n",
    "# Creating new column absolute_error which is the absolute value of error rounded off to 3 decimal places.\n",
    "results_df['absolute_error'] = (abs(results_df['error'])).round(3)\n",
    "\n",
    "# Creating new column relative_error which is the relative value of error rounded off to 3 decimal places with epsilon = 1e-15.\n",
    "results_df['relative_error'] = ((results_df['error'] / (results_df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "# Updates the csv file\n",
    "results_df.to_csv('emiss_results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70adf2-ebd7-4b27-9f79-4414bea071ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_data.csv') # Reading emiss_data\n",
    "\n",
    "# We want to know the Q1, Q2, Q3 values or (25%, 50%, 75%) values from our absolute_error distribution.\n",
    "percentiles = df['absolute_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "\n",
    "percentiles['25%'] = abs(percentiles['25%'])\n",
    "percentiles['50%'] = abs(percentiles['50%'])\n",
    "percentiles['75%'] = abs(percentiles['75%'])\n",
    "\n",
    "# Only the percentiles are of impportant to us.\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 1.062, 50% : 2.40, 75% : 4.99 (All are in Mt/yr)\n",
    "\n",
    "# Stated in the paper, the median absolute error is 3 Mt/yr. We got 2.4 Mt/ yr which means we are at a -0.6 Mt/yr deviation which is better than the paper.\n",
    "\n",
    "# The true percentile values as mentioned in the paper.\n",
    "true_percentiles = [1.3, 2.7, 4.5] # (All are in Mt/yr)\n",
    "\n",
    "# Absolute error between our percentile values and the true percentile values rounded off to 3 decimal places.\n",
    "error = [round(abs(true - pred), 3) for true, pred in zip(true_percentiles, percentiles)] # -0.238, -0.296, 0.49.\n",
    "\n",
    "# This means that our percentile values are on an average -0.35 Mt/yr range of the true percentile values.\n",
    "# This is better than the original results as our model is giving us less error.\n",
    "\n",
    "print(percentiles)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6faa72e-d41c-4d2c-b67d-80b7cd8913f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_results.csv') # Reading emiss_results\n",
    "\n",
    "# We want to know the Q1, Q2, Q3 values or (25%, 50%, 75%) values from our absolute_error distribution.\n",
    "percentiles = df['absolute_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "\n",
    "percentiles['25%'] = abs(percentiles['25%'])\n",
    "percentiles['50%'] = abs(percentiles['50%'])\n",
    "percentiles['75%'] = abs(percentiles['75%'])\n",
    "\n",
    "# Only the percentiles are of impportant to us.\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 1.04, 50% : 2.28, 75% : 4.29 (All are in Mt/yr)\n",
    "\n",
    "# Stated in the paper, the median absolute error is 3 Mt/yr. We got 2.28 Mt/ yr which means we are at a -0.72 Mt/yr deviation which is better than the paper.\n",
    "\n",
    "# The true percentile values as mentioned in the paper.\n",
    "true_percentiles = [1.3, 2.7, 4.5] # (All are in Mt/yr)\n",
    "\n",
    "# Absolute error between our percentile values and the true percentile values rounded off to 3 decimal places.\n",
    "error = [round(abs(true - pred), 3) for true, pred in zip(true_percentiles, percentiles)] # -0.257, -0.416, -0.204 (All these are negative i.e. our model produced\n",
    "                                                                                          # better results than the original model.\n",
    "\n",
    "# This means that our percentile values are on an average -0.35 Mt/yr range of the true percentile values.\n",
    "# This is better than the original results as our model is giving us less error.\n",
    "\n",
    "print(percentiles)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b911b52-ead4-40b9-a157-bf95131d541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_data.csv') # Reading emissions_results\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "# We want to know the Q1, Q2, Q3 values or (25%, 50%, 75%) values from our relative_error distribution.\n",
    "percentiles = df['abs_rel_error'].describe()\n",
    "#percentiles = [0.25, 0.5, 0.75]\n",
    "# Only the percentiles are of impportant to us.\n",
    "#percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 10.16, 50% : 21, 75% : 38.7 (Unitless)\n",
    "\n",
    "# Stated in the paper, the median absolute relative paper is around 20%. We have got 21.75% which can be approximated to 22%.\n",
    "\n",
    "# The true percentile values as mentioned in the paper.\n",
    "true_percentiles = [8.6, 18.1, 30.3] # Note : These values are percentage values as the relative errors are calculated in terms of percentage.\n",
    "\n",
    "# Absolute error between our percentile values and the true percentile values rounded off to 3 decimal places.\n",
    "error = [round(abs(true - pred), 3) for true, pred in zip(true_percentiles, percentiles)] # 1.56, 2.9, 8.4 \n",
    "\n",
    "#df.to_csv('emiss_data.csv', index = False)\n",
    "print(percentiles)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bd126-783c-4a04-b1c5-523f81ab45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_results.csv') # Reading emiss_results\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "# We want to know the Q1, Q2, Q3 values or (25%, 50%, 75%) values from our relative_error distribution.\n",
    "percentiles = df['abs_rel_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "\n",
    "# Only the percentiles are of impportant to us.\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 10.6, 50% : 21, 75% : 33.3 (Unitless)\n",
    "\n",
    "# Stated in the paper, the median absolute relative paper is around 20%. We have got 21%.\n",
    "# This means our model performed better.\n",
    "\n",
    "# The true percentile values as mentioned in the paper.\n",
    "true_percentiles = [8.6, 18.1, 30.3] # Note : These values are percentage values as the relative errors are calculated in terms of percentage.\n",
    "\n",
    "# Absolute error between our percentile values and the true percentile values rounded off to 3 decimal places.\n",
    "error = [round(abs(true - pred), 3) for true, pred in zip(true_percentiles, percentiles)] # 2, 2.9, 3 \n",
    "\n",
    "df.to_csv('emiss_results.csv', index = False)\n",
    "print(percentiles)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c383334-9d14-49c9-8724-150a8874a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.read_csv('emiss_results.csv') # Reading emiss_results\n",
    "df2 = pd.read_csv('emiss_data.csv') # Reading emiss_data\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df1['absolute_error'], fill = True, color = 'b', label = 'Using U-Net regression') # Using seaborn's kdeplot.\n",
    "sns.kdeplot(df2['absolute_error'], fill = True, color = 'r', label = 'Using CNN regression')\n",
    "plt.title('Kernel Density Estimate of Absolute Error for Lippendorf')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.xlim(0, 25)\n",
    "#plt.savefig('absolute_error_new.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df1['relative_error'], fill = True, color = 'b', label = 'Using U-Net regression') # Using seaborn's kdeplot.\n",
    "sns.kdeplot(df2['relative_error'], fill = True, color = 'r', label = 'Using CNN regression')\n",
    "plt.title('Kernel Density Estimate of Relative Error (%) for Lippendorf')\n",
    "plt.xlabel('Relative Error (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.xlim(-180, 180)\n",
    "#plt.savefig('relative_error_new.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3505c3-bfd0-4fb7-88fa-834a45aa0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our absolute errors lie in (0-2) Mt/yr range, in (2-5) Mt/yr range, in (5-10) Mt/yr range\n",
    "and in 10 Mt/yr above. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emiss_results.csv')\n",
    "\n",
    "# useful column\n",
    "abs = df['absolute_error']\n",
    "\n",
    "# get the required info\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 3219\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 2509\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 1099\n",
    "print(\"above 10 : \", ab10.sum()) # 384\n",
    "print(\"mean : \", mean) # 2.9\n",
    "print(\"median : \", median) # 2.28\n",
    "print(\"std : \", std) # 3.3\n",
    "\n",
    "# According to the paper, \" Majority of errors are concentrated below 10 Mt/yr.\" which we can see is true. In our case, we can say that majority of errors are\n",
    "# below 5 Mt/yr which is a significant increase in performance from 10 Mt/yr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6f320-fbf3-4c06-8cb5-60703f508296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our absolute errors lie in (0-2) Mt/yr range, in (2-5) Mt/yr range, in (5-10) Mt/yr range\n",
    "and in 10 Mt/yr above. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emiss_data.csv')\n",
    "\n",
    "# useful column\n",
    "abs = df['absolute_error']\n",
    "\n",
    "# get the required info\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 3143\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 2267\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 1305\n",
    "print(\"above 10 : \", ab10.sum()) # 496\n",
    "print(\"mean : \", mean) # 3.6\n",
    "print(\"median : \", median) # 2.4\n",
    "print(\"std : \", std) # 3.75\n",
    "\n",
    "# According to the paper, \" Majority of errors are concentrated below 10 Mt/yr.\" which we can see is true. In our case, we can say that majority of errors are\n",
    "# below 5 Mt/yr which is a significant increase in performance from 10 Mt/yr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcea71a-1ed0-4301-862d-8bf95debc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our relative errors lie less than -150%, in (-150 to -100)% range, in (-100 to -50)% range,\n",
    "in (-50 to 0) %, in (0 to 50) % range, in (50 to 100) % and above 100%. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emiss_data.csv')\n",
    "\n",
    "# useful column\n",
    "rel = df['relative_error']\n",
    "\n",
    "# get the required info\n",
    "lessneg150 = (rel <= -150)\n",
    "neg150_100 = (rel > -150) & (rel <= -100)\n",
    "neg100_50 = (rel > -100) & (rel <= -50)\n",
    "neg50_0 = (rel > -50) & (rel <= 0)\n",
    "in0_50 = (rel > 0) & (rel <= 50)\n",
    "in50_100 = (rel > 50) & (rel <= 100)\n",
    "ab100 = rel > 100\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"less than -150% : \", lessneg150.sum()) # 104\n",
    "print(\"in between -150% and -100% : \", neg150_100.sum()) # 202\n",
    "print(\"in between -100% and -50% : \", neg100_50.sum()) # 789\n",
    "print(\"in between -50% and 0% : \", neg50_0.sum()) # 2782\n",
    "print(\"in between 0% and 50% : \", in0_50.sum()) # 3249\n",
    "print(\"in between 50% and 100% : \", in50_100.sum()) # 145\n",
    "print(\"above 100% : \", ab100.sum()) # 0\n",
    "print(\"mean : \", mean) # -11.081\n",
    "print(\"median : \", median) # -2.5\n",
    "print(\"std : \", std) # 43.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2e016-8c60-42ce-bde7-b9c3cd333457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our relative errors lie less than -150%, in (-150 to -100)% range, in (-100 to -50)% range,\n",
    "in (-50 to 0) %, in (0 to 50) % range, in (50 to 100) % and above 100%. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emiss_results.csv')\n",
    "\n",
    "# useful column\n",
    "rel = df['relative_error']\n",
    "\n",
    "# get the required info\n",
    "lessneg150 = (rel <= -150)\n",
    "neg150_100 = (rel > -150) & (rel <= -100)\n",
    "neg100_50 = (rel > -100) & (rel <= -50)\n",
    "neg50_0 = (rel > -50) & (rel <= 0)\n",
    "in0_50 = (rel > 0) & (rel <= 50)\n",
    "in50_100 = (rel > 50) & (rel <= 100)\n",
    "ab100 = rel > 100\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"less than -150% : \", lessneg150.sum()) # 56\n",
    "print(\"in between -150% and -100% : \", neg150_100.sum()) # 96\n",
    "print(\"in between -100% and -50% : \", neg100_50.sum()) # 319\n",
    "print(\"in between -50% and 0% : \", neg50_0.sum()) # 1938\n",
    "print(\"in between 0% and 50% : \", in0_50.sum()) # 4633\n",
    "print(\"in between 50% and 100% : \", in50_100.sum()) # 169\n",
    "print(\"above 100% : \", ab100.sum()) # 0\n",
    "print(\"mean : \", mean) # 3.818\n",
    "print(\"median : \", median) # 11.6\n",
    "print(\"std : \", std) # 36.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f47bb4a-1f98-4619-95c8-d5cfa40b463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our absolute relative errors lie in (0 to 20) % range, in (20 to 50)% range, in (50 to 100) %, in (100 to 150)% \n",
    "and above 150%. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emiss_data.csv')\n",
    "\n",
    "# useful column\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "# get the required info\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 104\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 202\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 874\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 2676\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 3355\n",
    "print(\"mean : \", mean) # 30.727\n",
    "print(\"median : \", median) # 22\n",
    "print(\"std : \", std) # 32.7\n",
    "\n",
    "# According to the paper, \" Majority of errors are concentrated below 10 Mt/yr or 50% with very few exceeding 100%.\" which we can again see is true\n",
    "# In our model also, Majority of the errors are below 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe733f-2db0-419f-b510-0ea918d633be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code is to know how many of our absolute relative errors lie in (0 to 20) % range, in (20 to 50)% range, in (50 to 100) %, in (100 to 150)% \n",
    "and above 150%. Along with that, let us also get some statistics. \"\"\"\n",
    "\n",
    "# importing necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('emiss_results.csv')\n",
    "\n",
    "# useful column\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "# get the required info\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 56\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 96\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 487\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 3127\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 3444\n",
    "print(\"mean : \", mean) # 26\n",
    "print(\"median : \", median) # 21.02\n",
    "print(\"std : \", std) # 26.2\n",
    "\n",
    "# According to the paper, \" Majority of errors are concentrated below 10 Mt/yr or 50% with very few exceeding 100%.\" which we can again see is true\n",
    "# In our model also, Majority of the errors are below 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c0d01-5e17-4589-867b-ce04ba184601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code gets us a few statistics about true emissions and predicted emissions. \"\"\"\n",
    "\n",
    "# necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# read\n",
    "df = pd.read_csv('emiss_data.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Predicted Emissions']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 2.861\n",
    "print(\"max : \", max_true) # 52.659\n",
    "print(\"mean : \", mean_true) # 13.534\n",
    "print(\"std : \", std_true) # 8.772\n",
    "print(\"median : \", median_true) # 10.27\n",
    "print(\"range : \", range_true) # 49.798\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 4.347\n",
    "print(\"max : \", max_pred) # 46.892\n",
    "print(\"mean : \", mean_pred) # 12.479\n",
    "print(\"std : \", std_pred) # 7.692\n",
    "print(\"median : \", median_pred) # 11.123\n",
    "print(\"range : \", range_pred) # 42.545\n",
    "\n",
    "# The mean true emission = 13.5 Mt/yr and mean pred emission = 12.48 Mt/yr which tantamounts to the fact that our model has come closer to the real true emission values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d87700a-f796-4d33-ae30-15e4c71af64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This code gets us a few statistics about true emissions and predicted emissions. \"\"\"\n",
    "\n",
    "# necessary package\n",
    "import pandas as pd\n",
    "\n",
    "# read\n",
    "df = pd.read_csv('emiss_results.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Predicted Emissions']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 2.875\n",
    "print(\"max : \", max_true) # 52.659\n",
    "print(\"mean : \", mean_true) # 13.672\n",
    "print(\"std : \", std_true) # 8.982\n",
    "print(\"median : \", median_true) # 10.294\n",
    "print(\"range : \", range_true) # 49.784\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 2.748\n",
    "print(\"max : \", max_pred) # 52.902\n",
    "print(\"mean : \", mean_pred) # 13.459\n",
    "print(\"std : \", std_pred) # 8.02\n",
    "print(\"median : \", median_pred) # 9.264\n",
    "print(\"range : \", range_pred) # 50.154\n",
    "\n",
    "# The mean true emission = 13.67 Mt/yr and mean pred emission = 13.5 Mt/yr which tantamounts to the fact that our model has come closer to the real true emission values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d93d85-d199-41c1-a495-7183cbced75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.read_csv('emiss_results_lip.csv') # Reading emiss_results\n",
    "df2 = pd.read_csv('emiss_data_lip.csv') # Reading emiss_data\n",
    "df3 = pd.read_csv('emissions_data_lip.csv')\n",
    "df4 = pd.read_csv('emissions_results_lip.csv')\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df1['absolute_error'], fill = True, color = 'b', label = 'Using U-Net regression on curated dataset') # Using seaborn's kdeplot.\n",
    "sns.kdeplot(df2['absolute_error'], fill = True, color = 'r', label = 'Using CNN regression on curated dataset')\n",
    "sns.kdeplot(df3['absolute_error'], fill = True, color = 'g', label = 'Using CNN regression on original dataset')\n",
    "sns.kdeplot(df4['absolute_error'], fill = True, color = 'yellow', label = 'Using U-Net regression on original dataset')\n",
    "plt.title('Kernel Density Estimate of Absolute Error for Lippendorf')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.xlim(0, 25)\n",
    "plt.savefig('absolute_error_comb.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df1['relative_error'], fill = True, color = 'b', label = 'Using U-Net regression on curated dataset') # Using seaborn's kdeplot.\n",
    "sns.kdeplot(df2['relative_error'], fill = True, color = 'r', label = 'Using CNN regression on curated dataset')\n",
    "sns.kdeplot(df3['relative_error'], fill = True, color = 'g', label = 'Using CNN regression on original dataset')\n",
    "sns.kdeplot(df4['relative_error'], fill = True, color = 'yellow', label = 'Using U-Net regression on original dataset')\n",
    "plt.title('Kernel Density Estimate of Relative Error (%) for Lippendorf')\n",
    "plt.xlabel('Relative Error (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.xlim(-180, 180)\n",
    "plt.savefig('relative_error_comb.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b5a2e-b104-4aa7-890f-1ac72e7e28c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
