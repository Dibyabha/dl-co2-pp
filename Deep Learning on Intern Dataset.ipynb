{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f9f63c-1b7d-4c61-be94-9cc23aea3a11",
   "metadata": {},
   "source": [
    "Start with EDA of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4504a-34aa-4961-855f-8be29184715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "\n",
    "file_path = 'Intern Dataset/Data/1-Abha-Aseer-Crude.nc'\n",
    "\n",
    "with netCDF4.Dataset(file_path, 'r') as dataset:\n",
    "    variables = dataset.variables.keys()\n",
    "    print(\"Variables in the NetCDF file:\")\n",
    "    for var in variables:\n",
    "        print(var)\n",
    "    if '1-Abha-Aseer-Crude' in dataset.variables:\n",
    "        var = dataset.variables['1-Abha-Aseer-Crude']\n",
    "        print(\"\\nDetails of '1-Abha-Aseer-Crude':\")\n",
    "        print(f\"  Dimensions: {var.dimensions}\")\n",
    "        print(f\"  Shape: {var.shape}\")\n",
    "        print(f\"  Data type: {var.dtype}\")\n",
    "        print(f\"  Sample data (first few elements): {var[366]}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03250038-e75c-4a03-b5ed-ce1014801ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "\n",
    "file_path = 'Intern Dataset/Data/1-Abha-Aseer-Crude.nc'\n",
    "\n",
    "with netCDF4.Dataset(file_path, 'r') as dataset:\n",
    "    variables = dataset.variables\n",
    "    time_var = variables['time']\n",
    "    print(\"Time variable:\")\n",
    "    print(time_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066e821-e669-4499-a46e-dc154ea375d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = 'Intern Dataset/Data'\n",
    "files = os.listdir(directory)\n",
    "netcdf_files = [f for f in files if f.endswith('.nc')]\n",
    "total_files = len(netcdf_files)\n",
    "\n",
    "print(f\"Total number of NetCDF files: {total_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea0ada-bedb-416c-8c05-8f6b388ff274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "directory = 'Intern Dataset/Data/Data'\n",
    "train_dir = 'Intern Dataset/train'\n",
    "valid_dir = 'Intern Dataset/valid'\n",
    "test_dir = 'Intern Dataset/test'\n",
    "\n",
    "os.makedirs(train_dir, exist_ok = True)\n",
    "os.makedirs(valid_dir, exist_ok = True)\n",
    "os.makedirs(test_dir, exist_ok = True)\n",
    "\n",
    "files = os.listdir(directory)\n",
    "netcdf_files = [f for f in files if f.endswith('.nc')]\n",
    "\n",
    "random.shuffle(netcdf_files)\n",
    "\n",
    "total_files = len(netcdf_files)\n",
    "train_size = int(total_files * 0.65)\n",
    "valid_size = int(total_files * 0.15)\n",
    "\n",
    "train_files = netcdf_files[:train_size]\n",
    "valid_files = netcdf_files[train_size:train_size + valid_size]\n",
    "test_files = netcdf_files[train_size + valid_size:]\n",
    "\n",
    "def move_files(file_list, target_directory):\n",
    "    for file in file_list:\n",
    "        shutil.move(os.path.join(directory, file), os.path.join(target_directory, file))\n",
    "\n",
    "move_files(train_files, train_dir)\n",
    "move_files(valid_files, valid_dir)\n",
    "move_files(test_files, test_dir)\n",
    "\n",
    "print(f\"Moved {len(train_files)} files to train.\")\n",
    "print(f\"Moved {len(valid_files)} files to valid.\")\n",
    "print(f\"Moved {len(test_files)} files to test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da98937-4e75-456f-b9c6-6b0006cb202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset as NetCDFDataset\n",
    "import os\n",
    "\n",
    "class CustomInput:\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.ncfile = NetCDFDataset(file_path, 'r')\n",
    "        self.variable_name = os.path.basename(file_path).replace('.nc', '')\n",
    "        self.data = self.ncfile.variables[self.variable_name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 366\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if 0 <= idx < 366:\n",
    "            \n",
    "            xco2_arr = self.data[idx]\n",
    "            no2_arr = self.data[idx + 366]\n",
    "            u_arr = self.data[idx + 732]\n",
    "            v_arr = self.data[idx + 1098]\n",
    "            emiss_arr = self.data[1464]\n",
    "            mean_emiss = np.mean(emiss_arr)\n",
    "            outputs = np.full((366,), mean_emiss, dtype = np.float32)\n",
    "            inputs = np.stack([xco2_arr, no2_arr, u_arr, v_arr], axis = -1)\n",
    "            min_val = inputs.min(axis = (0, 1), keepdims = True)\n",
    "            max_val = inputs.max(axis = (0, 1), keepdims = True)\n",
    "            range_val = max_val - min_val\n",
    "            range_val[range_val == 0] = 1  \n",
    "            inputs = (inputs - min_val) / range_val\n",
    "            \n",
    "            return inputs, outputs[idx]\n",
    "\n",
    "    def __del__(self):\n",
    "        self.ncfile.close()\n",
    "        del self.data\n",
    "        del self.ncfile\n",
    "    \n",
    "def load_datasets(file_paths):\n",
    "    all_inputs = []\n",
    "    for file_path in file_paths:\n",
    "        dataset = CustomInput(file_path)\n",
    "        for idx in range(len(dataset)):\n",
    "            inputs, outputs = dataset[idx]\n",
    "            if inputs is not None and inputs.shape == (64, 64, 4) and outputs != 0:\n",
    "                all_inputs.append(inputs)\n",
    "        del dataset\n",
    "    return np.array(all_inputs)\n",
    "\n",
    "train_set = 'Intern Dataset/train'\n",
    "valid_set = 'Intern Dataset/valid'\n",
    "test_set = 'Intern Dataset/test'\n",
    "\n",
    "#train_files = [os.path.join(train_set, f) for f in os.listdir(train_set) if f.endswith('.nc')]\n",
    "valid_files = [os.path.join(valid_set, f) for f in os.listdir(valid_set) if f.endswith('.nc')]\n",
    "#test_files = [os.path.join(test_set, f) for f in os.listdir(test_set) if f.endswith('.nc')]\n",
    "\n",
    "#train_inputs = load_datasets(train_files)\n",
    "valid_inputs = load_datasets(valid_files)\n",
    "#test_inputs = load_datasets(test_files)\n",
    "\n",
    "#print(f\"Shape of train_inputs: {train_inputs.shape}\") # (15738, 64, 64, 4)\n",
    "print(f\"Shape of valid_inputs: {valid_inputs.shape}\") # (3660, 64, 64, 4)\n",
    "#print(f\"Shape of test_inputs: {test_inputs.shape}\") # (4392, 64, 64, 4)\n",
    "\n",
    "#np.save('train_inputs.npy', train_inputs)\n",
    "np.save('valid_inputs.npy', valid_inputs)\n",
    "#np.save('test_inputs.npy', test_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd611e-da63-481c-bbab-4c3bbd3b8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset as NetCDFDataset\n",
    "import os\n",
    "\n",
    "class CustomOutput:\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.ncfile = NetCDFDataset(file_path, 'r')\n",
    "        self.variable_name = os.path.basename(file_path).replace('.nc', '')\n",
    "        self.data = self.ncfile.variables[self.variable_name]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 366\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if 0 <= idx < 366:\n",
    "            emiss_arr = self.data[1464]\n",
    "            mean_emiss = np.mean(emiss_arr)\n",
    "            print(mean_emiss)\n",
    "            #outputs = np.full((366,), mean_emiss, dtype = np.float32)\n",
    "            \n",
    "            #return outputs[idx]\n",
    "            return mean_emiss\n",
    "    def __del__(self):\n",
    "        self.ncfile.close()\n",
    "        del self.data\n",
    "\n",
    "\n",
    "def load_outputs(file_paths):\n",
    "    all_outputs = []\n",
    "    for file_path in file_paths:\n",
    "        dataset = CustomOutput(file_path)\n",
    "        for idx in range(len(dataset)):\n",
    "            output = dataset[idx]\n",
    "            all_outputs.append(output)\n",
    "        #    if output != 0:\n",
    "        #        all_outputs.append(output)\n",
    "        del dataset\n",
    "    return np.array(all_outputs)\n",
    "\n",
    "#train_output_set = 'Intern Dataset/train'\n",
    "#valid_output_set = 'Intern Dataset/valid'\n",
    "test_output_set = 'Intern Dataset/test'\n",
    "\n",
    "#train_output_files = [os.path.join(train_output_set, f) for f in os.listdir(train_output_set) if f.endswith('.nc')]\n",
    "#valid_output_files = [os.path.join(valid_output_set, f) for f in os.listdir(valid_output_set) if f.endswith('.nc')]\n",
    "test_output_files = [os.path.join(test_output_set, f) for f in os.listdir(test_output_set) if f.endswith('.nc')]\n",
    "\n",
    "#train_outputs = load_outputs(train_output_files)\n",
    "#valid_outputs = load_outputs(valid_output_files)\n",
    "test_outputs = load_outputs(test_output_files)\n",
    "\n",
    "#del train_output_files, valid_output_files, test_output_files\n",
    "\n",
    "#print(f\"Shape of train_outputs: {train_outputs.shape}\")\n",
    "#print(f\"Shape of valid_outputs: {valid_outputs.shape}\")\n",
    "print(f\"Shape of test_outputs: {test_outputs.shape}\")\n",
    "\n",
    "#np.save('train_outputs.npy', train_outputs)\n",
    "#np.save('valid_outputs.npy', valid_outputs)\n",
    "#np.save('test_outputs.npy', test_outputs)\n",
    "\n",
    "#del train_outputs, valid_outputs, test_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ffa8af-0cad-46e8-b1a2-f0cd488622d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_outputs = np.load('train_comb_outputs.npy')\n",
    "valid_outputs = np.load('valid_comb_outputs.npy')\n",
    "test_outputs = np.load('test_comb_outputs.npy')\n",
    "\n",
    "def compute_statistics(data):\n",
    "    \n",
    "    \n",
    "    total_min = np.min(data)\n",
    "    total_max = np.max(data) \n",
    "    total_range = total_max - total_min\n",
    "    total_mean = np.mean(data)\n",
    "    total_median = np.median(data)\n",
    "    \n",
    "    return {\n",
    "        \n",
    "        'total_min': total_min,\n",
    "        'total_max': total_max,\n",
    "        'total_range': total_range,\n",
    "        'total_mean': total_mean,\n",
    "        'total_median': total_median\n",
    "    }\n",
    "\n",
    "train_stats = compute_statistics(train_outputs)\n",
    "valid_stats = compute_statistics(valid_outputs)\n",
    "test_stats = compute_statistics(test_outputs)\n",
    "\n",
    "print(\"Train Outputs Statistics:\")\n",
    "for key, value in train_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nValidation Outputs Statistics:\")\n",
    "for key, value in valid_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nTest Outputs Statistics:\")\n",
    "for key, value in test_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "'''\n",
    "Saudi:\n",
    "Train Outputs Statistics:\n",
    "total_min: 0.016926101\n",
    "total_max: 42.4713798\n",
    "total_range: 42.454453699\n",
    "total_mean: 2.8974418592431697\n",
    "total_median: 1.3505546285\n",
    "\n",
    "Validation Outputs Statistics:\n",
    "total_min: 0.017671046\n",
    "total_max: 28.06594092\n",
    "total_range: 28.048269874\n",
    "total_mean: 3.356000043450273\n",
    "total_median: 2.8229551075\n",
    "\n",
    "Test Outputs Statistics:\n",
    "total_min: 0.065269343\n",
    "total_max: 22.80542092\n",
    "total_range: 22.740151577\n",
    "total_mean: 2.0433333689961293\n",
    "total_median: 1.117680595\n",
    "\n",
    "Combined:\n",
    "Train Outputs Statistics:\n",
    "total_min: 0.016926101\n",
    "total_max: 42.4713798\n",
    "total_range: 42.454453699\n",
    "total_mean: 9.230586244294928\n",
    "total_median: 7.8671523105\n",
    "\n",
    "Validation Outputs Statistics:\n",
    "total_min: 0.017671046\n",
    "total_max: 30.11400032043457\n",
    "total_range: 30.09632927443457\n",
    "total_mean: 8.660432125355227\n",
    "total_median: 5.810635911\n",
    "\n",
    "Test Outputs Statistics:\n",
    "total_min: 0.065269343\n",
    "total_max: 30.11400032043457\n",
    "total_range: 30.04873097743457\n",
    "total_mean: 7.220526309503236\n",
    "total_median: 2.0419865755\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f4ac0-42d8-44b0-8a3a-367e784f0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "train_outputs = np.load('train_comb_outputs.npy')\n",
    "valid_outputs = np.load('valid_comb_outputs.npy')\n",
    "test_outputs = np.load('test_comb_outputs.npy')\n",
    "\n",
    "bins = np.concatenate([np.arange(0, 5.5, 0.5), np.arange(5, 25, 2), [np.inf]])\n",
    "output_dir = 'Figures/Intern'\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "def plot_histogram(data, filename, title, bins):\n",
    "    plt.figure(figsize = (12, 6))\n",
    "    plt.hist(data, bins = bins, edgecolor = 'black', alpha = 0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "train_filename = os.path.join(output_dir, 'train_emiss_dist_comb.png')\n",
    "valid_filename = os.path.join(output_dir, 'valid_emiss_dist_comb.png')\n",
    "test_filename = os.path.join(output_dir, 'test_emiss_dist_comb.png')\n",
    "\n",
    "plot_histogram(train_outputs.flatten(), train_filename, 'Histogram of Train Outputs', bins)\n",
    "plot_histogram(valid_outputs.flatten(), valid_filename, 'Histogram of Valid Outputs', bins)\n",
    "plot_histogram(test_outputs.flatten(), test_filename, 'Histogram of Test Outputs', bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa11ff0-0aa3-4b82-8904-7bfd70c267bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_outputs = np.load('train_comb_outputs.npy')\n",
    "valid_outputs = np.load('valid_comb_outputs.npy')\n",
    "test_outputs = np.load('test_comb_outputs.npy')\n",
    "\n",
    "bins = np.concatenate([np.arange(0, 5.5, 0.5), np.arange(5, 25, 2), [np.inf]])\n",
    "\n",
    "def calculate_percentage(data, bins):\n",
    "    counts, _ = np.histogram(data, bins = bins)\n",
    "    total_count = np.sum(counts)\n",
    "    print(total_count)\n",
    "    percentages = (counts / total_count) * 100\n",
    "    return counts, percentages\n",
    "\n",
    "train_counts, train_percentages = calculate_percentage(train_outputs.flatten(), bins)\n",
    "valid_counts, valid_percentages = calculate_percentage(valid_outputs.flatten(), bins)\n",
    "test_counts, test_percentages = calculate_percentage(test_outputs.flatten(), bins)\n",
    "\n",
    "def stats(name, counts, percentages, bins):\n",
    "    print(f'\\n{name} Data Histogram:')\n",
    "    for i in range(len(counts)):\n",
    "        print(f'Bin range {bins[i]:.1f} to {bins[i+1]:.1f} (or {bins[i]} to {bins[i+1]}):')\n",
    "        print(f'  Count: {counts[i]}')\n",
    "        print(f'  Percentage: {percentages[i]:.2f}%')\n",
    "\n",
    "stats('Train', train_counts, train_percentages, bins)\n",
    "stats('Valid', valid_counts, valid_percentages, bins)\n",
    "stats('Test', test_counts, test_percentages, bins)\n",
    "\n",
    "'''\n",
    "KSA:\n",
    "Train Data Histogram:\n",
    "Bin range 0.0 to 0.5 (or 0.0 to 0.5):\n",
    "  Count: 4419\n",
    "  Percentage: 28.08%\n",
    "Bin range 0.5 to 1.0 (or 0.5 to 1.0):\n",
    "  Count: 2082\n",
    "  Percentage: 13.23%\n",
    "Bin range 1.0 to 1.5 (or 1.0 to 1.5):\n",
    "  Count: 1845\n",
    "  Percentage: 11.72%\n",
    "Bin range 1.5 to 2.0 (or 1.5 to 2.0):\n",
    "  Count: 1146\n",
    "  Percentage: 7.28%\n",
    "Bin range 2.0 to 2.5 (or 2.0 to 2.5):\n",
    "  Count: 784\n",
    "  Percentage: 4.98%\n",
    "Bin range 2.5 to 3.0 (or 2.5 to 3.0):\n",
    "  Count: 634\n",
    "  Percentage: 4.03%\n",
    "Bin range 3.0 to 3.5 (or 3.0 to 3.5):\n",
    "  Count: 583\n",
    "  Percentage: 3.70%\n",
    "Bin range 3.5 to 4.0 (or 3.5 to 4.0):\n",
    "  Count: 549\n",
    "  Percentage: 3.49%\n",
    "Bin range 4.0 to 4.5 (or 4.0 to 4.5):\n",
    "  Count: 431\n",
    "  Percentage: 2.74%\n",
    "Bin range 4.5 to 5.0 (or 4.5 to 5.0):\n",
    "  Count: 391\n",
    "  Percentage: 2.48%\n",
    "Bin range 5.0 to 5.0 (or 5.0 to 5.0):\n",
    "  Count: 0\n",
    "  Percentage: 0.00%\n",
    "Bin range 5.0 to 7.0 (or 5.0 to 7.0):\n",
    "  Count: 920\n",
    "  Percentage: 5.85%\n",
    "Bin range 7.0 to 9.0 (or 7.0 to 9.0):\n",
    "  Count: 707\n",
    "  Percentage: 4.49%\n",
    "Bin range 9.0 to 11.0 (or 9.0 to 11.0):\n",
    "  Count: 507\n",
    "  Percentage: 3.22%\n",
    "Bin range 11.0 to 13.0 (or 11.0 to 13.0):\n",
    "  Count: 295\n",
    "  Percentage: 1.87%\n",
    "Bin range 13.0 to 15.0 (or 13.0 to 15.0):\n",
    "  Count: 172\n",
    "  Percentage: 1.09%\n",
    "Bin range 15.0 to 17.0 (or 15.0 to 17.0):\n",
    "  Count: 78\n",
    "  Percentage: 0.50%\n",
    "Bin range 17.0 to 19.0 (or 17.0 to 19.0):\n",
    "  Count: 54\n",
    "  Percentage: 0.34%\n",
    "Bin range 19.0 to 21.0 (or 19.0 to 21.0):\n",
    "  Count: 36\n",
    "  Percentage: 0.23%\n",
    "Bin range 21.0 to 23.0 (or 21.0 to 23.0):\n",
    "  Count: 25\n",
    "  Percentage: 0.16%\n",
    "Bin range 23.0 to inf (or 23.0 to inf):\n",
    "  Count: 80\n",
    "  Percentage: 0.51%\n",
    "\n",
    "Valid Data Histogram:\n",
    "Bin range 0.0 to 0.5 (or 0.0 to 0.5):\n",
    "  Count: 626\n",
    "  Percentage: 17.10%\n",
    "Bin range 0.5 to 1.0 (or 0.5 to 1.0):\n",
    "  Count: 311\n",
    "  Percentage: 8.50%\n",
    "Bin range 1.0 to 1.5 (or 1.0 to 1.5):\n",
    "  Count: 278\n",
    "  Percentage: 7.60%\n",
    "Bin range 1.5 to 2.0 (or 1.5 to 2.0):\n",
    "  Count: 308\n",
    "  Percentage: 8.42%\n",
    "Bin range 2.0 to 2.5 (or 2.0 to 2.5):\n",
    "  Count: 195\n",
    "  Percentage: 5.33%\n",
    "Bin range 2.5 to 3.0 (or 2.5 to 3.0):\n",
    "  Count: 167\n",
    "  Percentage: 4.56%\n",
    "Bin range 3.0 to 3.5 (or 3.0 to 3.5):\n",
    "  Count: 209\n",
    "  Percentage: 5.71%\n",
    "Bin range 3.5 to 4.0 (or 3.5 to 4.0):\n",
    "  Count: 185\n",
    "  Percentage: 5.05%\n",
    "Bin range 4.0 to 4.5 (or 4.0 to 4.5):\n",
    "  Count: 213\n",
    "  Percentage: 5.82%\n",
    "Bin range 4.5 to 5.0 (or 4.5 to 5.0):\n",
    "  Count: 193\n",
    "  Percentage: 5.27%\n",
    "Bin range 5.0 to 5.0 (or 5.0 to 5.0):\n",
    "  Count: 0\n",
    "  Percentage: 0.00%\n",
    "Bin range 5.0 to 7.0 (or 5.0 to 7.0):\n",
    "  Count: 567\n",
    "  Percentage: 15.49%\n",
    "Bin range 7.0 to 9.0 (or 7.0 to 9.0):\n",
    "  Count: 267\n",
    "  Percentage: 7.30%\n",
    "Bin range 9.0 to 11.0 (or 9.0 to 11.0):\n",
    "  Count: 91\n",
    "  Percentage: 2.49%\n",
    "Bin range 11.0 to 13.0 (or 11.0 to 13.0):\n",
    "  Count: 32\n",
    "  Percentage: 0.87%\n",
    "Bin range 13.0 to 15.0 (or 13.0 to 15.0):\n",
    "  Count: 13\n",
    "  Percentage: 0.36%\n",
    "Bin range 15.0 to 17.0 (or 15.0 to 17.0):\n",
    "  Count: 3\n",
    "  Percentage: 0.08%\n",
    "Bin range 17.0 to 19.0 (or 17.0 to 19.0):\n",
    "  Count: 1\n",
    "  Percentage: 0.03%\n",
    "Bin range 19.0 to 21.0 (or 19.0 to 21.0):\n",
    "  Count: 0\n",
    "  Percentage: 0.00%\n",
    "Bin range 21.0 to 23.0 (or 21.0 to 23.0):\n",
    "  Count: 0\n",
    "  Percentage: 0.00%\n",
    "Bin range 23.0 to inf (or 23.0 to inf):\n",
    "  Count: 1\n",
    "  Percentage: 0.03%\n",
    "\n",
    "Test Data Histogram:\n",
    "Bin range 0.0 to 0.5 (or 0.0 to 0.5):\n",
    "  Count: 562\n",
    "  Percentage: 12.80%\n",
    "Bin range 0.5 to 1.0 (or 0.5 to 1.0):\n",
    "  Count: 1387\n",
    "  Percentage: 31.58%\n",
    "Bin range 1.0 to 1.5 (or 1.0 to 1.5):\n",
    "  Count: 828\n",
    "  Percentage: 18.85%\n",
    "Bin range 1.5 to 2.0 (or 1.5 to 2.0):\n",
    "  Count: 541\n",
    "  Percentage: 12.32%\n",
    "Bin range 2.0 to 2.5 (or 2.0 to 2.5):\n",
    "  Count: 291\n",
    "  Percentage: 6.63%\n",
    "Bin range 2.5 to 3.0 (or 2.5 to 3.0):\n",
    "  Count: 122\n",
    "  Percentage: 2.78%\n",
    "Bin range 3.0 to 3.5 (or 3.0 to 3.5):\n",
    "  Count: 103\n",
    "  Percentage: 2.35%\n",
    "Bin range 3.5 to 4.0 (or 3.5 to 4.0):\n",
    "  Count: 62\n",
    "  Percentage: 1.41%\n",
    "Bin range 4.0 to 4.5 (or 4.0 to 4.5):\n",
    "  Count: 51\n",
    "  Percentage: 1.16%\n",
    "Bin range 4.5 to 5.0 (or 4.5 to 5.0):\n",
    "  Count: 39\n",
    "  Percentage: 0.89%\n",
    "Bin range 5.0 to 5.0 (or 5.0 to 5.0):\n",
    "  Count: 0\n",
    "  Percentage: 0.00%\n",
    "Bin range 5.0 to 7.0 (or 5.0 to 7.0):\n",
    "  Count: 96\n",
    "  Percentage: 2.19%\n",
    "Bin range 7.0 to 9.0 (or 7.0 to 9.0):\n",
    "  Count: 109\n",
    "  Percentage: 2.48%\n",
    "Bin range 9.0 to 11.0 (or 9.0 to 11.0):\n",
    "  Count: 84\n",
    "  Percentage: 1.91%\n",
    "Bin range 11.0 to 13.0 (or 11.0 to 13.0):\n",
    "  Count: 62\n",
    "  Percentage: 1.41%\n",
    "Bin range 13.0 to 15.0 (or 13.0 to 15.0):\n",
    "  Count: 34\n",
    "  Percentage: 0.77%\n",
    "Bin range 15.0 to 17.0 (or 15.0 to 17.0):\n",
    "  Count: 14\n",
    "  Percentage: 0.32%\n",
    "Bin range 17.0 to 19.0 (or 17.0 to 19.0):\n",
    "  Count: 4\n",
    "  Percentage: 0.09%\n",
    "Bin range 19.0 to 21.0 (or 19.0 to 21.0):\n",
    "  Count: 2\n",
    "  Percentage: 0.05%\n",
    "Bin range 21.0 to 23.0 (or 21.0 to 23.0):\n",
    "  Count: 1\n",
    "  Percentage: 0.02%\n",
    "Bin range 23.0 to inf (or 23.0 to inf):\n",
    "  Count: 0\n",
    "  Percentage: 0.00%\n",
    "\n",
    "Combined:\n",
    "Train Data Histogram:\n",
    "Bin range 0.0 to 0.5 (or 0.0 to 0.5):\n",
    "  Count: 4419\n",
    "  Percentage: 15.61%\n",
    "Bin range 0.5 to 1.0 (or 0.5 to 1.0):\n",
    "  Count: 2082\n",
    "  Percentage: 7.35%\n",
    "Bin range 1.0 to 1.5 (or 1.0 to 1.5):\n",
    "  Count: 1845\n",
    "  Percentage: 6.52%\n",
    "Bin range 1.5 to 2.0 (or 1.5 to 2.0):\n",
    "  Count: 1146\n",
    "  Percentage: 4.05%\n",
    "Bin range 2.0 to 2.5 (or 2.0 to 2.5):\n",
    "  Count: 784\n",
    "  Percentage: 2.77%\n",
    "Bin range 2.5 to 3.0 (or 2.5 to 3.0):\n",
    "  Count: 634\n",
    "  Percentage: 2.24%\n",
    "Bin range 3.0 to 3.5 (or 3.0 to 3.5):\n",
    "  Count: 583\n",
    "  Percentage: 2.06%\n",
    "Bin range 3.5 to 4.0 (or 3.5 to 4.0):\n",
    "  Count: 549\n",
    "  Percentage: 1.94%\n",
    "Bin range 4.0 to 4.5 (or 4.0 to 4.5):\n",
    "  Count: 431\n",
    "  Percentage: 1.52%\n",
    "Bin range 4.5 to 5.0 (or 4.5 to 5.0):\n",
    "  Count: 391\n",
    "  Percentage: 1.38%\n",
    "Bin range 5.0 to 5.0 (or 5.0 to 5.0):\n",
    "  Count: 0\n",
    "  Percentage: 0.00%\n",
    "Bin range 5.0 to 7.0 (or 5.0 to 7.0):\n",
    "  Count: 920\n",
    "  Percentage: 3.25%\n",
    "Bin range 7.0 to 9.0 (or 7.0 to 9.0):\n",
    "  Count: 870\n",
    "  Percentage: 3.07%\n",
    "Bin range 9.0 to 11.0 (or 9.0 to 11.0):\n",
    "  Count: 1248\n",
    "  Percentage: 4.41%\n",
    "Bin range 11.0 to 13.0 (or 11.0 to 13.0):\n",
    "  Count: 1820\n",
    "  Percentage: 6.43%\n",
    "Bin range 13.0 to 15.0 (or 13.0 to 15.0):\n",
    "  Count: 2068\n",
    "  Percentage: 7.30%\n",
    "Bin range 15.0 to 17.0 (or 15.0 to 17.0):\n",
    "  Count: 2230\n",
    "  Percentage: 7.88%\n",
    "Bin range 17.0 to 19.0 (or 17.0 to 19.0):\n",
    "  Count: 1991\n",
    "  Percentage: 7.03%\n",
    "Bin range 19.0 to 21.0 (or 19.0 to 21.0):\n",
    "  Count: 1732\n",
    "  Percentage: 6.12%\n",
    "Bin range 21.0 to 23.0 (or 21.0 to 23.0):\n",
    "  Count: 1126\n",
    "  Percentage: 3.98%\n",
    "Bin range 23.0 to inf (or 23.0 to inf):\n",
    "  Count: 1447\n",
    "  Percentage: 5.11%\n",
    "\n",
    "Valid Data Histogram:\n",
    "Bin range 0.0 to 0.5 (or 0.0 to 0.5):\n",
    "  Count: 626\n",
    "  Percentage: 10.50%\n",
    "Bin range 0.5 to 1.0 (or 0.5 to 1.0):\n",
    "  Count: 311\n",
    "  Percentage: 5.21%\n",
    "Bin range 1.0 to 1.5 (or 1.0 to 1.5):\n",
    "  Count: 278\n",
    "  Percentage: 4.66%\n",
    "Bin range 1.5 to 2.0 (or 1.5 to 2.0):\n",
    "  Count: 308\n",
    "  Percentage: 5.16%\n",
    "Bin range 2.0 to 2.5 (or 2.0 to 2.5):\n",
    "  Count: 195\n",
    "  Percentage: 3.27%\n",
    "Bin range 2.5 to 3.0 (or 2.5 to 3.0):\n",
    "  Count: 167\n",
    "  Percentage: 2.80%\n",
    "Bin range 3.0 to 3.5 (or 3.0 to 3.5):\n",
    "  Count: 209\n",
    "  Percentage: 3.50%\n",
    "Bin range 3.5 to 4.0 (or 3.5 to 4.0):\n",
    "  Count: 185\n",
    "  Percentage: 3.10%\n",
    "Bin range 4.0 to 4.5 (or 4.0 to 4.5):\n",
    "  Count: 213\n",
    "  Percentage: 3.57%\n",
    "Bin range 4.5 to 5.0 (or 4.5 to 5.0):\n",
    "  Count: 193\n",
    "  Percentage: 3.24%\n",
    "Bin range 5.0 to 5.0 (or 5.0 to 5.0):\n",
    "  Count: 0\n",
    "  Percentage: 0.00%\n",
    "Bin range 5.0 to 7.0 (or 5.0 to 7.0):\n",
    "  Count: 567\n",
    "  Percentage: 9.51%\n",
    "Bin range 7.0 to 9.0 (or 7.0 to 9.0):\n",
    "  Count: 291\n",
    "  Percentage: 4.88%\n",
    "Bin range 9.0 to 11.0 (or 9.0 to 11.0):\n",
    "  Count: 232\n",
    "  Percentage: 3.89%\n",
    "Bin range 11.0 to 13.0 (or 11.0 to 13.0):\n",
    "  Count: 323\n",
    "  Percentage: 5.42%\n",
    "Bin range 13.0 to 15.0 (or 13.0 to 15.0):\n",
    "  Count: 357\n",
    "  Percentage: 5.99%\n",
    "Bin range 15.0 to 17.0 (or 15.0 to 17.0):\n",
    "  Count: 394\n",
    "  Percentage: 6.61%\n",
    "Bin range 17.0 to 19.0 (or 17.0 to 19.0):\n",
    "  Count: 373\n",
    "  Percentage: 6.25%\n",
    "Bin range 19.0 to 21.0 (or 19.0 to 21.0):\n",
    "  Count: 310\n",
    "  Percentage: 5.20%\n",
    "Bin range 21.0 to 23.0 (or 21.0 to 23.0):\n",
    "  Count: 198\n",
    "  Percentage: 3.32%\n",
    "Bin range 23.0 to inf (or 23.0 to inf):\n",
    "  Count: 234\n",
    "  Percentage: 3.92%\n",
    "\n",
    "Test Data Histogram:\n",
    "Bin range 0.0 to 0.5 (or 0.0 to 0.5):\n",
    "  Count: 562\n",
    "  Percentage: 8.39%\n",
    "Bin range 0.5 to 1.0 (or 0.5 to 1.0):\n",
    "  Count: 1387\n",
    "  Percentage: 20.71%\n",
    "Bin range 1.0 to 1.5 (or 1.0 to 1.5):\n",
    "  Count: 828\n",
    "  Percentage: 12.37%\n",
    "Bin range 1.5 to 2.0 (or 1.5 to 2.0):\n",
    "  Count: 541\n",
    "  Percentage: 8.08%\n",
    "Bin range 2.0 to 2.5 (or 2.0 to 2.5):\n",
    "  Count: 291\n",
    "  Percentage: 4.35%\n",
    "Bin range 2.5 to 3.0 (or 2.5 to 3.0):\n",
    "  Count: 122\n",
    "  Percentage: 1.82%\n",
    "Bin range 3.0 to 3.5 (or 3.0 to 3.5):\n",
    "  Count: 103\n",
    "  Percentage: 1.54%\n",
    "Bin range 3.5 to 4.0 (or 3.5 to 4.0):\n",
    "  Count: 62\n",
    "  Percentage: 0.93%\n",
    "Bin range 4.0 to 4.5 (or 4.0 to 4.5):\n",
    "  Count: 51\n",
    "  Percentage: 0.76%\n",
    "Bin range 4.5 to 5.0 (or 4.5 to 5.0):\n",
    "  Count: 39\n",
    "  Percentage: 0.58%\n",
    "Bin range 5.0 to 5.0 (or 5.0 to 5.0):\n",
    "  Count: 0\n",
    "  Percentage: 0.00%\n",
    "Bin range 5.0 to 7.0 (or 5.0 to 7.0):\n",
    "  Count: 96\n",
    "  Percentage: 1.43%\n",
    "Bin range 7.0 to 9.0 (or 7.0 to 9.0):\n",
    "  Count: 142\n",
    "  Percentage: 2.12%\n",
    "Bin range 9.0 to 11.0 (or 9.0 to 11.0):\n",
    "  Count: 240\n",
    "  Percentage: 3.58%\n",
    "Bin range 11.0 to 13.0 (or 11.0 to 13.0):\n",
    "  Count: 329\n",
    "  Percentage: 4.91%\n",
    "Bin range 13.0 to 15.0 (or 13.0 to 15.0):\n",
    "  Count: 403\n",
    "  Percentage: 6.02%\n",
    "Bin range 15.0 to 17.0 (or 15.0 to 17.0):\n",
    "  Count: 397\n",
    "  Percentage: 5.93%\n",
    "Bin range 17.0 to 19.0 (or 17.0 to 19.0):\n",
    "  Count: 353\n",
    "  Percentage: 5.27%\n",
    "Bin range 19.0 to 21.0 (or 19.0 to 21.0):\n",
    "  Count: 297\n",
    "  Percentage: 4.44%\n",
    "Bin range 21.0 to 23.0 (or 21.0 to 23.0):\n",
    "  Count: 203\n",
    "  Percentage: 3.03%\n",
    "Bin range 23.0 to inf (or 23.0 to inf):\n",
    "  Count: 250\n",
    "  Percentage: 3.73%\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152974a-f6d5-4586-86b3-7472a6765013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#train_inputs = np.load('train_inputs.npy')\n",
    "val_inputs = np.load('valid_inputs.npy')\n",
    "#test_inputs = np.load('test_inputs.npy')\n",
    "\n",
    "#print('Original train_inputs shape:', train_inputs.shape)\n",
    "print('Original val_inputs shape:', val_inputs.shape)\n",
    "#print('Original test_inputs shape:', test_inputs.shape)\n",
    "\n",
    "num_days = 366\n",
    "#site_count = train_inputs.shape[0] // num_days\n",
    "\n",
    "#print('Number of sites:', site_count)\n",
    "\n",
    "def reshape_data(data, num_days):\n",
    "    num_sites = data.shape[0] // num_days\n",
    "    reshaped_data = data.reshape(num_sites, num_days, 64, 64, 4)\n",
    "    return reshaped_data\n",
    "\n",
    "#train_inputs_reshaped = reshape_data(train_inputs, num_days)\n",
    "val_inputs_reshaped = reshape_data(val_inputs, num_days)\n",
    "#test_inputs_reshaped = reshape_data(test_inputs, num_days)\n",
    "\n",
    "#print('Reshaped train_inputs shape:', train_inputs_reshaped.shape)\n",
    "print('Reshaped val_inputs shape:', val_inputs_reshaped.shape)\n",
    "#print('Reshaped test_inputs shape:', test_inputs_reshaped.shape)\n",
    "\n",
    "#np.save('train_inputs.npy', train_inputs_reshaped)\n",
    "np.save('valid_inputs.npy', val_inputs_reshaped)\n",
    "#np.save('test_inputs.npy', test_inputs_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5535f-ab9b-4fdd-b92f-9dacda151c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_inputs = np.load('train_inputs.npy')\n",
    "val_inputs = np.load('valid_inputs.npy')\n",
    "test_inputs = np.load('test_inputs.npy')\n",
    "\n",
    "print('Train inputs shape:', train_inputs.shape)\n",
    "print('Validation inputs shape:', val_inputs.shape)\n",
    "print('Test inputs shape:', test_inputs.shape)\n",
    "\n",
    "def compute_mean_per_site(data):\n",
    "    mean_per_site = np.mean(data, axis = 1)\n",
    "    return mean_per_site\n",
    "\n",
    "train_mean = compute_mean_per_site(train_inputs)\n",
    "val_mean = compute_mean_per_site(val_inputs)\n",
    "test_mean = compute_mean_per_site(test_inputs)\n",
    "\n",
    "print('Mean train inputs shape:', train_mean.shape)\n",
    "print('Mean validation inputs shape:', val_mean.shape)\n",
    "print('Mean test inputs shape:', test_mean.shape)\n",
    "\n",
    "np.save('train_inputs.npy', train_mean)\n",
    "np.save('valid_inputs.npy', val_mean)\n",
    "np.save('test_inputs.npy', test_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e07d7-7016-42a6-91d5-6e3052844c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#train_outputs = np.load('train_outputs.npy')\n",
    "valid_outputs = np.load('valid_outputs.npy')\n",
    "#test_outputs = np.load('test_outputs.npy')\n",
    "\n",
    "#print('Train outputs shape:', train_outputs.shape)\n",
    "print('Validation outputs shape:', valid_outputs.shape)\n",
    "#print('Test outputs shape:', test_outputs.shape)\n",
    "\n",
    "def reshape_to_sites(data, num_days):\n",
    "    num_samples = data.shape[0]\n",
    "    num_sites = num_samples // num_days\n",
    "    reshaped_data = data.reshape(num_sites, num_days)\n",
    "    return reshaped_data\n",
    "\n",
    "def aggregate_to_sites(data):\n",
    "    aggregated_data = np.mean(data, axis = 1)\n",
    "    return aggregated_data\n",
    "\n",
    "num_days = 366\n",
    "\n",
    "#train_outputs_sites = reshape_to_sites(train_outputs, num_days)\n",
    "valid_outputs_sites = reshape_to_sites(valid_outputs, num_days)\n",
    "#test_outputs_sites = reshape_to_sites(test_outputs, num_days)\n",
    "\n",
    "#print('Reshaped train outputs shape:', train_outputs_sites.shape)\n",
    "print('Reshaped validation outputs shape:', valid_outputs_sites.shape)\n",
    "#print('Reshaped test outputs shape:', test_outputs_sites.shape)\n",
    "\n",
    "#train_outputs_aggregated = aggregate_to_sites(train_outputs_sites)\n",
    "valid_outputs_aggregated = aggregate_to_sites(valid_outputs_sites)\n",
    "#test_outputs_aggregated = aggregate_to_sites(test_outputs_sites)\n",
    "\n",
    "#print('Aggregated train outputs shape:', train_outputs_aggregated.shape)\n",
    "print('Aggregated validation outputs shape:', valid_outputs_aggregated.shape)\n",
    "#print('Aggregated test outputs shape:', test_outputs_aggregated.shape)\n",
    "\n",
    "#np.save('train_outputs.npy', train_outputs_aggregated)\n",
    "np.save('valid_outputs.npy', valid_outputs_aggregated)\n",
    "#np.save('test_outputs.npy', test_outputs_aggregated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc80a5-1417-4dfb-8b12-19b1776d2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#train_inputs = np.load('train_inputs.npy')\n",
    "valid_inputs = np.load('valid_inputs.npy')\n",
    "#test_inputs = np.load('test_inputs.npy')\n",
    "\n",
    "#print('Train inputs shape:', train_inputs.shape)\n",
    "print('Validation inputs shape:', valid_inputs.shape)\n",
    "#print('Test inputs shape:', test_inputs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c0a2c0-589a-4f28-b434-e7901b88a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPool2D, UpSampling2D, Concatenate, Dropout, BatchNormalization, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import gc\n",
    "\n",
    "def unetreg(input_shape, dropout_rate = 0.2):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    c1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(inputs)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    c1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(c1)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    d1 = Dropout(dropout_rate)(c1)\n",
    "    p1 = MaxPool2D(pool_size=(2, 2))(d1)\n",
    "    \n",
    "    c2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(p1)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(c2)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    d2 = Dropout(dropout_rate)(c2)\n",
    "    p2 = MaxPool2D(pool_size=(2, 2))(d2)\n",
    "    \n",
    "    c3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(p2)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    c3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(c3)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    d3 = Dropout(dropout_rate)(c3)\n",
    "    p3 = MaxPool2D(pool_size=(2, 2))(d3)\n",
    "\n",
    "    c4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(p3)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(c4)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    d4 = Dropout(dropout_rate)(c4)\n",
    "    \n",
    "    u5 = Conv2DTranspose(256, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(d4)\n",
    "    u5 = UpSampling2D((2, 2))(u5)\n",
    "    concat5 = Concatenate()([u5, c3])\n",
    "    c5 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(concat5)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    c5 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(c5)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    d5 = Dropout(dropout_rate)(c5)\n",
    "    \n",
    "    u6 = Conv2DTranspose(128, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(d5)\n",
    "    u6 = UpSampling2D((2, 2))(u6)\n",
    "    concat6 = Concatenate()([u6, c2])\n",
    "    c6 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(concat6)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(c6)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    d6 = Dropout(dropout_rate)(c6)\n",
    "    \n",
    "    u7 = Conv2DTranspose(64, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(d6)\n",
    "    u7 = UpSampling2D((2, 2))(u7)\n",
    "    concat7 = Concatenate()([u7, c1])\n",
    "    c7 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(concat7)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Conv2D(1, 3, activation='relu', padding='same', kernel_initializer='glorot_uniform')(c7)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    \n",
    "    flat = Flatten()(c7)\n",
    "    output = Dense(1, activation='linear')(flat)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = unetreg((64, 64, 4))\n",
    "model.load_weights('saved_models/modelweights_new_lip.h5')\n",
    "\n",
    "\n",
    "# average +- 5\n",
    "train_inputs = np.load('test_inputs.npy')\n",
    "train_outputs = np.load('test_outputs.npy')\n",
    "\n",
    "def predict_emissions(inputs, model, batch_size = 8):\n",
    "    num_sites = inputs.shape[0] # (num_sites, 366, 64, 64, 4) input shape\n",
    "    days = inputs.shape[1]\n",
    "    predictions = np.zeros((num_sites, days)) # num_sites, 366\n",
    "    \n",
    "    for site_idx in range(num_sites):\n",
    "        for start_day in range(0, days, batch_size):\n",
    "            end_day = min(start_day + batch_size, days)\n",
    "            input_batch = inputs[site_idx, start_day:end_day, :, :, :]\n",
    "            preds_batch = model.predict(input_batch)\n",
    "            predictions[site_idx, start_day:end_day] = preds_batch.flatten()\n",
    "        gc.collect()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def adjust_predictions(predictions, annual_emissions):\n",
    "    adjusted_predictions = np.copy(predictions)\n",
    "    for i in range(predictions.shape[0]): \n",
    "        mean_prediction = np.mean(predictions[i])\n",
    "        if mean_prediction != 0:\n",
    "            scale_factor = annual_emissions[i] / mean_prediction\n",
    "            adjusted_predictions[i] *= scale_factor\n",
    "    return adjusted_predictions\n",
    "\n",
    "train_predictions = predict_emissions(train_inputs, model)\n",
    "train_adjusted_predictions = adjust_predictions(train_predictions, train_outputs)\n",
    "np.save('valid_outputs.npy', train_adjusted_predictions)\n",
    "\n",
    "print(\"Done\") # (total_test,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75dd927-dbe1-4ceb-890b-e2844a7b18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_inputs = np.load('valid_inputs_box.npy')\n",
    "train_outputs = np.load('train_inputs_box.npy')\n",
    "#train = np.load('test_outputs_box.npy')\n",
    "\n",
    "#tr = train_inputs.reshape(-1)\n",
    "#va = train_outputs.reshape(-1)\n",
    "#te = train.reshape(-1)\n",
    "\n",
    "print(\"Shape : \", train_inputs.shape)\n",
    "print(\"Shape : \", train_outputs.shape)\n",
    "\n",
    "#train_inputs = train_inputs.reshape(-1, 64, 64, 4)\n",
    "#train_outputs = train_outputs.reshape(-1)\n",
    "\n",
    "#print(\"Shape : \", train_inputs.shape)\n",
    "#print(\"Shape : \", train_outputs.shape)\n",
    "\n",
    "#np.save('valid_outputs_box.npy', tr)\n",
    "#np.save('train_outputs_box.npy', va)\n",
    "#np.save('test_outputs_box.npy', te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9a3f4-4d91-49d7-975f-c3531f1d88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "class CustomDataset:\n",
    "    def __init__(self, file_path, variable = None):\n",
    "        self.ncfile = Dataset(file_path, 'r')\n",
    "        self.variable = variable\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ncfile.variables['xco2_noisy'].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xco2_read = self.ncfile.variables['xco2_noisy'][idx]\n",
    "        u_read = self.ncfile.variables['u'][idx]\n",
    "        v_read = self.ncfile.variables['v'][idx]\n",
    "        emiss_read = self.ncfile.variables['emiss'][idx]\n",
    "        \n",
    "        xco2_arr = np.array(xco2_read).astype('float32')\n",
    "        u_arr = np.array(u_read).astype('float32')\n",
    "        v_arr = np.array(v_read).astype('float32')\n",
    "        emiss_arr = np.array(emiss_read).astype('float32')\n",
    "        \n",
    "        if self.variable:\n",
    "            var_read = self.ncfile.variables[self.variable][idx]\n",
    "            var_arr = np.array(var_read).astype('float32')\n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr, var_arr], axis = -1)\n",
    "        else:\n",
    "            inputs = np.stack([xco2_arr, u_arr, v_arr], axis = -1)\n",
    "        \n",
    "        min_val = inputs.min(axis = (0, 1), keepdims = True)\n",
    "        max_val = inputs.max(axis = (0, 1), keepdims = True)\n",
    "        max_val[max_val == min_val] = min_val[max_val == min_val] + 1\n",
    "        inputs = (inputs - min_val) / (max_val - min_val)\n",
    "        \n",
    "        weights = np.array([0.0, 1.0, 0.0])\n",
    "        weighted = np.round(np.sum(weights * emiss_arr), 3)\n",
    "        outputs = np.array([weighted], dtype = np.float32)\n",
    "        \n",
    "        return inputs, outputs\n",
    "\n",
    "    def __del__(self):\n",
    "        self.ncfile.close()\n",
    "\n",
    "def save_dataset_to_npy(dataset, input_file, output_file):\n",
    "    num_samples = len(dataset)\n",
    "    inputs_all = np.zeros((num_samples, 64, 64, 4 if dataset.variable else 3), dtype = np.float32)\n",
    "    outputs_all = np.zeros((num_samples, 1), dtype = np.float32)\n",
    "    \n",
    "    for idx in range(num_samples):\n",
    "        inputs, outputs = dataset[idx]\n",
    "        inputs_all[idx] = inputs\n",
    "        outputs_all[idx] = outputs\n",
    "    \n",
    "    np.save(input_file, inputs_all)\n",
    "    np.save(output_file, outputs_all)\n",
    "\n",
    "train_set = 'Datasets/data_paper_inv_pp/lippendorf/train_dataset.nc'\n",
    "valid_set = 'Datasets/data_paper_inv_pp/lippendorf/valid_dataset.nc'\n",
    "test_set = 'Datasets/data_paper_inv_pp/lippendorf/test_dataset.nc'\n",
    "\n",
    "train_dataset = CustomDataset(train_set, variable = 'no2_noisy')\n",
    "valid_dataset = CustomDataset(valid_set, variable = 'no2_noisy')\n",
    "test_dataset = CustomDataset(test_set, variable = 'no2_noisy')\n",
    "\n",
    "save_dataset_to_npy(train_dataset, 'train_inputs_lip.npy', 'train_outputs_lip.npy')\n",
    "save_dataset_to_npy(valid_dataset, 'valid_inputs_lip.npy', 'valid_outputs_lip.npy')\n",
    "save_dataset_to_npy(test_dataset, 'test_inputs_lip.npy', 'test_outputs_lip.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f0b1c-e424-495a-8845-418691d96174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_inputs = np.load('test_inputs.npy')\n",
    "train_inputs_lip = np.load('test_inputs_lip.npy')\n",
    "train_inputs_box = np.load('test_inputs_box.npy')\n",
    "#train_inputs_lip = train_inputs_lip.flatten()\n",
    "#train_inputs_box = train_inputs_box.flatten()\n",
    "\n",
    "combined_inputs = np.concatenate([train_inputs, train_inputs_lip, train_inputs_box], axis = 0)\n",
    "\n",
    "print(\"Shape of the combined array:\", combined_inputs.shape)\n",
    "\n",
    "np.save('test_comb_inputs.npy', combined_inputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516b5f7-7968-41e3-9ab4-6a9eebe6a0d3",
   "metadata": {},
   "source": [
    "CNN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b668141-162b-4a82-b8b1-8b011fa00b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Dropout, BatchNormalization, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# (Number of samples, 64, 64, 4) xco2, no2, u, v\n",
    "# emission rate co2\n",
    "def model_arch(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1, input_shape = input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(LeakyReLU(alpha = 0.3))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return tf.sqrt(mse)\n",
    "    \n",
    "train_inputs = np.load('train_comb_inputs.npy')\n",
    "train_outputs = np.load('train_comb_outputs.npy')\n",
    "val_inputs = np.load('valid_comb_inputs.npy')\n",
    "val_outputs = np.load('valid_comb_outputs.npy')\n",
    "test_inputs = np.load('test_comb_inputs.npy')\n",
    "test_outputs = np.load('test_comb_outputs.npy')\n",
    "\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "check = os.path.join(checkpoint_dir, 'cnn_reg_comb.h5')\n",
    "check_call = ModelCheckpoint(filepath = check, save_weights_only = True, verbose = 1)\n",
    "\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "model = model_arch(input_shape = (64, 64, 4))\n",
    "# model.load_weights('saved_models/cnn_reg_comb.h5')\n",
    "optimizer = Adam(learning_rate = 1e-3)\n",
    "lr_check = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.75, patience = 30, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-6)\n",
    "model.compile(optimizer = optimizer, loss = tf.keras.losses.Huber())\n",
    "#tf.keras.losses.Huber()\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 200, steps_per_epoch = len(train_outputs) // 32,\n",
    "                    validation_data = (val_inputs, val_outputs), validation_batch_size = 32,\n",
    "                    callbacks = [lr_check, check_call])\n",
    "\n",
    "del train_inputs, train_outputs, val_inputs, val_outputs\n",
    "\n",
    "test_loss = model.evaluate(test_inputs, test_outputs, batch_size = None)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "predicted_emissions = []\n",
    "true_emissions = []\n",
    "\n",
    "for i in range(len(test_inputs)):\n",
    "    inputs = np.expand_dims(test_inputs[i], axis = 0)\n",
    "    prediction = model.predict(inputs)\n",
    "    true_emissions.append(test_outputs[i])\n",
    "    predicted_emissions.append(prediction)\n",
    "\n",
    "true_emissions = np.array(true_emissions)\n",
    "predicted_emissions = np.array(predicted_emissions)\n",
    "\n",
    "del test_inputs, test_outputs\n",
    "\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape)\n",
    "\n",
    "csv_file = 'emiss_comb_cnn.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "#df = pd.DataFrame({'True Emissions': true_emissions.flatten(), 'Predicted Emissions': predicted_emissions.flatten()})\n",
    "df['Pred Emiss 2'] = predicted_emissions.flatten()\n",
    "df.to_csv('emiss_comb_cnn.csv', index = False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eee598-7040-49ec-a92f-7eecb7a24704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_cnn.csv')\n",
    "# \n",
    "df['Average Emiss'] = ((df['Predicted Emissions'] + df['Pred Emiss 2'] + df['Pred Emiss 3'] + df['Pred Emiss 1'])/4).round(3) \n",
    "\n",
    "df['error'] = df['True Emissions'] - df['Average Emiss'] \n",
    "\n",
    "df['absolute_error'] = (abs(df['error'])).round(3)\n",
    "\n",
    "df['relative_error'] = ((df['error'] / (df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "df.to_csv('emiss_comb_cnn.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61100023-3d0b-41c4-bb2d-19ac683f0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_cnn.csv')\n",
    "\n",
    "percentiles = df['absolute_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 0.928, 50% : 1.502, 75% : 2.540 (All are in Mt/yr)\n",
    "print(percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016dfd4-d566-4857-aecd-5b8e1dc946ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_cnn.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "percentiles = df['abs_rel_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 18.208, 50% : 46.256, 75% : 148.138 (Unitless)\n",
    "print(percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49448c6a-ab8c-485b-9013-d58b0ae033ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('emiss_comb_cnn.csv')\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['absolute_error'], shade = True, color = 'b')\n",
    "plt.title('Kernel Density Estimate of Absolute Error')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 25)\n",
    "plt.savefig('Figures/Intern/absolute_error_plot_comb.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['relative_error'], shade = True, color = 'b')\n",
    "plt.title('Kernel Density Estimate of Relative Error')\n",
    "plt.xlabel('Relative Error')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(-180, 180)\n",
    "plt.savefig('Figures/Intern/relative_error_plot_comb.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b049e-1494-4a53-ac14-55f2c0264fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_cnn.csv')\n",
    "\n",
    "abs = df['absolute_error']\n",
    "\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 4652\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 1189\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 708\n",
    "print(\"above 10 : \", ab10.sum()) # 147\n",
    "print(\"mean : \", mean) # 2.332\n",
    "print(\"median : \", median) # 1.502\n",
    "print(\"std : \", std) # 2.47\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda08927-bf09-4159-9e89-0da5dd8f1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_cnn.csv')\n",
    "\n",
    "rel = df['relative_error']\n",
    "\n",
    "lessneg150 = (rel <= -150)\n",
    "neg150_100 = (rel > -150) & (rel <= -100)\n",
    "neg100_50 = (rel > -100) & (rel <= -50)\n",
    "neg50_0 = (rel > -50) & (rel <= 0)\n",
    "in0_50 = (rel > 0) & (rel <= 50)\n",
    "in50_100 = (rel > 50) & (rel <= 100)\n",
    "ab100 = rel > 100\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"less than - 150% : \", lessneg150.sum()) # 1655\n",
    "print(\"in between -150% and -100% : \", neg150_100.sum()) # 517 \n",
    "print(\"in between -100% and -50% : \", neg100_50.sum()) # 623\n",
    "print(\"in between -50% and 0% : \", neg50_0.sum()) # 1404\n",
    "print(\"in between 0% and 50% : \", in0_50.sum()) # 2045\n",
    "print(\"in between 50% and 100% : \", in50_100.sum()) # 452 \n",
    "print(\"above 100% : \", ab100.sum()) # 0\n",
    "print(\"mean : \", mean) # -89.679\n",
    "print(\"median : \", median) # -28.515\n",
    "print(\"std : \", std) # 182.786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee649e8a-6661-4db1-8e6a-7ed98aea2629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_cnn.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 1655\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 517\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 1075\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 1611\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 1838\n",
    "print(\"mean : \", mean) # 124.565\n",
    "print(\"median : \", median) # 46.256\n",
    "print(\"std : \", std) # 204.702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ba22d-02be-4fb6-8caf-32d0cc1709f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_cnn.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Average Emiss']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 0.065\n",
    "print(\"max : \", max_true) # 30.114\n",
    "print(\"mean : \", mean_true) # 7.221\n",
    "print(\"std : \", std_true) # 7.931\n",
    "print(\"median : \", median_true) # 2.042\n",
    "print(\"range : \", range_true) # 30.049\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 1.308\n",
    "print(\"max : \", max_pred) # 19.253\n",
    "print(\"mean : \", mean_pred) # 6.62\n",
    "print(\"std : \", std_pred) # 6.163\n",
    "print(\"median : \", median_pred) # 2.199\n",
    "print(\"range : \", range_pred) # 17.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5fd06-460a-490b-91a7-0d7643063a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Dropout, BatchNormalization, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def model_arch(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1, input_shape = input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(LeakyReLU(alpha = 0.3))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return tf.sqrt(mse)\n",
    "    \n",
    "train_inputs = np.load('train_inputs.npy')\n",
    "train_outputs = np.load('train_outputs.npy')\n",
    "val_inputs = np.load('valid_inputs.npy')\n",
    "val_outputs = np.load('valid_outputs.npy')\n",
    "test_inputs = np.load('test_inputs.npy')\n",
    "test_outputs = np.load('test_outputs.npy')\n",
    "\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "check = os.path.join(checkpoint_dir, 'cnn_reg_saudi.h5')\n",
    "check_call = ModelCheckpoint(filepath = check, save_weights_only = True, verbose = 1)\n",
    "\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "model = model_arch(input_shape = (64, 64, 4))\n",
    "#model.load_weights('saved_models/cnn_reg_saudi.h5')\n",
    "optimizer = Adam(learning_rate = 1e-3)\n",
    "lr_check = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.75, patience = 30, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-6)\n",
    "model.compile(optimizer = optimizer, loss = tf.keras.losses.Huber())\n",
    "# tf.keras.losses.Huber()\n",
    "model.summary() # Around 186000 params\n",
    "\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 150, steps_per_epoch = len(train_inputs) // 32,\n",
    "                    validation_data = (val_inputs, val_outputs), validation_batch_size = 32,\n",
    "                    callbacks = [lr_check, check_call])\n",
    "\n",
    "test_loss = model.evaluate(test_inputs, test_outputs, batch_size = None)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "del train_inputs, train_outputs, val_inputs, val_outputs\n",
    "\n",
    "predicted_emissions = []\n",
    "true_emissions = []\n",
    "for i in range(len(test_inputs)):\n",
    "    inputs = np.expand_dims(test_inputs[i], axis = 0)\n",
    "    prediction = model.predict(inputs)\n",
    "    true_emissions.append(test_outputs[i])\n",
    "    predicted_emissions.append(prediction)\n",
    "\n",
    "del test_inputs, test_outputs\n",
    "\n",
    "true_emissions = np.array(true_emissions)\n",
    "predicted_emissions = np.array(predicted_emissions)\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape)\n",
    "\n",
    "csv_file = 'emiss_saudi_cnn.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "#df = pd.DataFrame({'True Emissions': true_emissions.flatten(), 'Predicted Emissions': predicted_emissions.flatten()})\n",
    "df['Pred Emiss 2'] = predicted_emissions.flatten()\n",
    "df.to_csv('emiss_saudi_cnn.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2305b6-7dd1-4353-bb69-0e7c3905bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_cnn.csv')\n",
    "# \n",
    "df['Average Emiss'] = ((df['Predicted Emissions'] + df['Pred Emiss 2'] + df['Pred Emiss 1'] + df['Pred Emiss 3'])/4).round(3) \n",
    "\n",
    "df['error'] = df['True Emissions'] - df['Average Emiss'] \n",
    "\n",
    "df['absolute_error'] = (abs(df['error'])).round(3)\n",
    "\n",
    "df['relative_error'] = ((df['error'] / (df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "df.to_csv('emiss_saudi_cnn.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b05a41-399c-43f3-9797-e18d6a94e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_cnn.csv')\n",
    "\n",
    "percentiles = df['absolute_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 0.528, 50% : 0.951, 75% : 1.328 (All are in Mt/yr)\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1eb53-6e8e-4b91-a690-d0579e592e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_cnn.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "percentiles = df['abs_rel_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 33.114, 50% : 78.222, 75% : 154.050 (Unitless)\n",
    "print(percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988b0b2-11a9-4a58-8001-1a7995a1819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_cnn.csv')\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['absolute_error'], shade = True, color = 'b')\n",
    "plt.title('Kernel Density Estimate of Absolute Error')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 25)\n",
    "plt.savefig('Figures/Intern/absolute_error_plot_saudi.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['relative_error'], shade = True, color = 'b')\n",
    "plt.title('Kernel Density Estimate of Relative Error')\n",
    "plt.xlabel('Relative Error')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(-180, 180)\n",
    "plt.savefig('Figures/Intern/relative_error_plot_saudi.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721c363-c0b5-4758-a9a7-7d6498e932c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_cnn.csv')\n",
    "\n",
    "abs = df['absolute_error']\n",
    "\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 3861\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 212\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 232\n",
    "print(\"above 10 : \", ab10.sum()) # 87\n",
    "print(\"mean : \", mean) # 1.514\n",
    "print(\"median : \", median) # 0.951\n",
    "print(\"std : \", std) # 2.252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a5df8-9bfb-4565-af28-8ab7109a6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_cnn.csv')\n",
    "\n",
    "rel = df['relative_error']\n",
    "\n",
    "lessneg150 = (rel <= -150)\n",
    "neg150_100 = (rel > -150) & (rel <= -100)\n",
    "neg100_50 = (rel > -100) & (rel <= -50)\n",
    "neg50_0 = (rel > -50) & (rel <= 0)\n",
    "in0_50 = (rel > 0) & (rel <= 50)\n",
    "in50_100 = (rel > 50) & (rel <= 100)\n",
    "ab100 = rel > 100\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"less than - 150% : \", lessneg150.sum()) # 1137\n",
    "print(\"in between -150% and -100% : \", neg150_100.sum()) # 521\n",
    "print(\"in between -100% and -50% : \", neg100_50.sum()) # 656\n",
    "print(\"in between -50% and 0% : \", neg50_0.sum()) # 774\n",
    "print(\"in between 0% and 50% : \", in0_50.sum()) # 748\n",
    "print(\"in between 50% and 100% : \", in50_100.sum()) # 556\n",
    "print(\"above 100% : \", ab100.sum()) # 0\n",
    "print(\"mean : \", mean) # -108.167\n",
    "print(\"median : \", median) # -58.006\n",
    "print(\"std : \", std) # 199.331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bcb2f-2ec0-4a5c-8c6c-4ada29f317e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_cnn.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 1137\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 521\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 1212\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 798\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 724\n",
    "print(\"mean : \", mean) # 134.196\n",
    "print(\"median : \", median) # 78.222\n",
    "print(\"std : \", std) # 182.819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22f658-b51a-45f0-b9df-8a16cf43376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_cnn.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Average Emiss']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 0.065\n",
    "print(\"max : \", max_true) # 22.805\n",
    "print(\"mean : \", mean_true) # 2.043\n",
    "print(\"std : \", std_true) # 2.7\n",
    "print(\"median : \", median_true) # 1.118\n",
    "print(\"range : \", range_true) # 22.74\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 0.368\n",
    "print(\"max : \", max_pred) # 11.53\n",
    "print(\"mean : \", mean_pred) # 2.524\n",
    "print(\"std : \", std_pred) # 1.752\n",
    "print(\"median : \", median_pred) # 1.89\n",
    "print(\"range : \", range_pred) # 11.162"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ae2b4-6ba5-4912-927d-bb76b42da313",
   "metadata": {},
   "source": [
    "U-Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fe443-2a03-45eb-8c03-9e29d0cb20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Dropout, BatchNormalization, MaxPool2D, Flatten, Dense, Conv2DTranspose, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def unetreg(input_shape, dropout_rate = 0.2):\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "    \n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(inputs) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c1) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    d1 = Dropout(dropout_rate)(c1) \n",
    "    p1 = MaxPool2D(pool_size = (2, 2))(d1) # Shape : (32, 32, 64) \n",
    "    \n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p1) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c2) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    d2 = Dropout(dropout_rate)(c2)\n",
    "    p2 = MaxPool2D(pool_size = (2, 2))(d2) # Shape : (16, 16, 128)\n",
    "    \n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p2) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c3) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    d3 = Dropout(dropout_rate)(c3)\n",
    "    p3 = MaxPool2D(pool_size = (2, 2))(d3) # Shape : (8, 8, 256)\n",
    "\n",
    "    # Bottleneck\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p3) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c4) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    d4 = Dropout(dropout_rate)(c4)\n",
    "    \n",
    "    # Decoder (upsampling path)\n",
    "    u5 = Conv2DTranspose(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d4) # Shape : (8, 8, 256)\n",
    "    u5 = UpSampling2D((2, 2))(u5) # Shape : (16, 16, 256)\n",
    "    concat5 = Concatenate()([u5, c3]) # Shape : (16, 16, 256)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    d5 = Dropout(dropout_rate)(c5)\n",
    "    \n",
    "    u6 = Conv2DTranspose(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d5) # Shape : (16, 16, 128)\n",
    "    u6 = UpSampling2D((2, 2))(u6) # Shape : (32, 32, 128)\n",
    "    concat6 = Concatenate()([u6, c2]) # Shape : (32, 32, 128)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    d6 = Dropout(dropout_rate)(c6)\n",
    "    \n",
    "    u7 = Conv2DTranspose(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d6) # Shape : (32, 32, 64)\n",
    "    u7 = UpSampling2D((2, 2))(u7) # Shape : (64, 64, 64)\n",
    "    concat7 = Concatenate()([u7, c1])  # Shape : (64, 64, 64)\n",
    "    c7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat7) # Shape : (64, 64, 64)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Conv2D(1, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_uniform')(c7) # Shape : (64, 64, 1)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    \n",
    "    # Output layer\n",
    "    flat = Flatten()(c7) # Shape : 64*64*1\n",
    "    output = Dense(1, activation = 'linear')(flat) # Shape : 1\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    return model\n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return tf.sqrt(mse)\n",
    "    \n",
    "train_inputs = np.load('train_comb_inputs.npy')\n",
    "train_outputs = np.load('train_comb_outputs.npy')\n",
    "val_inputs = np.load('valid_comb_inputs.npy')\n",
    "val_outputs = np.load('valid_comb_outputs.npy')\n",
    "test_inputs = np.load('test_comb_inputs.npy')\n",
    "test_outputs = np.load('test_comb_outputs.npy')\n",
    "\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "check = os.path.join(checkpoint_dir, 'unet_reg_comb.h5')\n",
    "check_call = ModelCheckpoint(filepath = check, save_weights_only = True, verbose = 1)\n",
    "\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "model = unetreg(input_shape = (64, 64, 4))\n",
    "#model.load_weights('saved_models/unet_reg_comb.h5')\n",
    "optimizer = Adam(learning_rate = 1e-3)\n",
    "lr_check = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.75, patience = 20, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-6)\n",
    "model.compile(optimizer = optimizer, loss = tf.keras.losses.Huber())\n",
    "# tf.keras.losses.Huber()\n",
    "model.summary() # Around 8 million params\n",
    "\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 150, steps_per_epoch = len(train_inputs) // 32,\n",
    "                    validation_data = (val_inputs, val_outputs), validation_batch_size = 32,\n",
    "                    callbacks = [lr_check, check_call])\n",
    "\n",
    "test_loss = model.evaluate(test_inputs, test_outputs, batch_size = None)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "del train_inputs, train_outputs, val_inputs, val_outputs\n",
    "\n",
    "predicted_emissions = []\n",
    "true_emissions = []\n",
    "for i in range(len(test_inputs)):\n",
    "    inputs = np.expand_dims(test_inputs[i], axis = 0)\n",
    "    prediction = model.predict(inputs)\n",
    "    true_emissions.append(test_outputs[i])\n",
    "    predicted_emissions.append(prediction)\n",
    "\n",
    "del test_inputs, test_outputs\n",
    "\n",
    "true_emissions = np.array(true_emissions)\n",
    "predicted_emissions = np.array(predicted_emissions)\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape)\n",
    "\n",
    "csv_file = 'emiss_comb_unet.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "df = pd.DataFrame({'True Emissions': true_emissions.flatten(), 'Predicted Emissions': predicted_emissions.flatten()})\n",
    "#df['Pred Emiss 2'] = predicted_emissions.flatten()\n",
    "df.to_csv('emiss_comb_unet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80984d9-3123-49b9-9164-d01dd52442dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_unet.csv')\n",
    "# \n",
    "df['Average Emiss'] = ((df['Pred Emiss 2'] + df['Pred Emiss 1'] + df['Predicted Emissions'] + df['Pred Emiss 3'] )/4).round(3) \n",
    "\n",
    "df['error'] = df['True Emissions'] - df['Average Emiss'] \n",
    "\n",
    "df['absolute_error'] = (abs(df['error'])).round(3)\n",
    "\n",
    "df['relative_error'] = ((df['error'] / (df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "df.to_csv('emiss_comb_unet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991da3e-a7f6-4020-9076-1e1884569c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_unet.csv')\n",
    "\n",
    "percentiles = df['absolute_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 0.484, 50% : 1.069, 75% : 2.786 (All are in Mt/yr)\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25493500-2ad3-4930-9de6-a8fe6f1786e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_unet.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "percentiles = df['abs_rel_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 14.921, 50% : 37.68, 75% : 81.439 (Unitless)\n",
    "print(percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318226e-669b-40a8-a66b-389675cbb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('emiss_comb_unet.csv')\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['absolute_error'], shade = True, color = 'b')\n",
    "plt.title('Kernel Density Estimate of Absolute Error')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 25)\n",
    "plt.savefig('Figures/Intern/absolute_error_plot_unet_comb.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['relative_error'], shade = True, color = 'b')\n",
    "plt.title('Kernel Density Estimate of Relative Error')\n",
    "plt.xlabel('Relative Error')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(-180, 180)\n",
    "plt.savefig('Figures/Intern/relative_error_plot_unet_comb.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e820a6-4a0e-4672-8d36-1425734bd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_unet.csv')\n",
    "\n",
    "abs = df['absolute_error']\n",
    "\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 4546\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 1310\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 748\n",
    "print(\"above 10 : \", ab10.sum()) # 92\n",
    "print(\"mean : \", mean) # 2.076\n",
    "print(\"median : \", median) # 1.069\n",
    "print(\"std : \", std) # 2.426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a04d7-6e02-4864-8673-9d5671b41a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_unet.csv')\n",
    "\n",
    "rel = df['relative_error']\n",
    "\n",
    "lessneg150 = (rel <= -150)\n",
    "neg150_100 = (rel > -150) & (rel <= -100)\n",
    "neg100_50 = (rel > -100) & (rel <= -50)\n",
    "neg50_0 = (rel > -50) & (rel <= 0)\n",
    "in0_50 = (rel > 0) & (rel <= 50)\n",
    "in50_100 = (rel > 50) & (rel <= 100)\n",
    "ab100 = rel > 100\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"less than - 150% : \", lessneg150.sum()) # 937\n",
    "print(\"in between -150% and -100% : \", neg150_100.sum()) # 406\n",
    "print(\"in between -100% and -50% : \", neg100_50.sum()) # 854\n",
    "print(\"in between -50% and 0% : \", neg50_0.sum()) # 2275\n",
    "print(\"in between 0% and 50% : \", in0_50.sum()) # 1666\n",
    "print(\"in between 50% and 100% : \", in50_100.sum()) # 558\n",
    "print(\"above 100% : \", ab100.sum()) # 0\n",
    "print(\"mean : \", mean) # -63.52\n",
    "print(\"median : \", median) # -18.568\n",
    "print(\"std : \", std) # 172.123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e130a0-6f9e-446b-9ccd-d89fd82feb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_unet.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 937\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 406\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 1412\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 1755\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 2186\n",
    "print(\"mean : \", mean) # 84.385\n",
    "print(\"median : \", median) # 37.68\n",
    "print(\"std : \", std) # 162.911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9eefe-b827-462e-96a5-ea2d1c07f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_comb_unet.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Average Emiss']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 0.065\n",
    "print(\"max : \", max_true) # 30.114\n",
    "print(\"mean : \", mean_true) # 7.221\n",
    "print(\"std : \", std_true) # 7.931\n",
    "print(\"median : \", median_true) # 2.042\n",
    "print(\"range : \", range_true) # 30.049\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 0.265\n",
    "print(\"max : \", max_pred) # 26.135\n",
    "print(\"mean : \", mean_pred) # 7.689\n",
    "print(\"std : \", std_pred) # 8.304\n",
    "print(\"median : \", median_pred) # 2.07\n",
    "print(\"range : \", range_pred) # 25.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc459a-d28d-48f6-8890-9af5bdc798f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Dropout, BatchNormalization, MaxPool2D, Flatten, Dense, Conv2DTranspose, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def unetreg(input_shape, dropout_rate = 0.2):\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "    \n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(inputs) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c1) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    d1 = Dropout(dropout_rate)(c1) \n",
    "    p1 = MaxPool2D(pool_size = (2, 2))(d1) # Shape : (32, 32, 64) \n",
    "    \n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p1) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c2) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    d2 = Dropout(dropout_rate)(c2)\n",
    "    p2 = MaxPool2D(pool_size = (2, 2))(d2) # Shape : (16, 16, 128)\n",
    "    \n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p2) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c3) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    d3 = Dropout(dropout_rate)(c3)\n",
    "    p3 = MaxPool2D(pool_size = (2, 2))(d3) # Shape : (8, 8, 256)\n",
    "\n",
    "    # Bottleneck\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p3) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c4) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    d4 = Dropout(dropout_rate)(c4)\n",
    "    \n",
    "    # Decoder (upsampling path)\n",
    "    u5 = Conv2DTranspose(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d4) # Shape : (8, 8, 256)\n",
    "    u5 = UpSampling2D((2, 2))(u5) # Shape : (16, 16, 256)\n",
    "    concat5 = Concatenate()([u5, c3]) # Shape : (16, 16, 256)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    d5 = Dropout(dropout_rate)(c5)\n",
    "    \n",
    "    u6 = Conv2DTranspose(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d5) # Shape : (16, 16, 128)\n",
    "    u6 = UpSampling2D((2, 2))(u6) # Shape : (32, 32, 128)\n",
    "    concat6 = Concatenate()([u6, c2]) # Shape : (32, 32, 128)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    d6 = Dropout(dropout_rate)(c6)\n",
    "    \n",
    "    u7 = Conv2DTranspose(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d6) # Shape : (32, 32, 64)\n",
    "    u7 = UpSampling2D((2, 2))(u7) # Shape : (64, 64, 64)\n",
    "    concat7 = Concatenate()([u7, c1])  # Shape : (64, 64, 64)\n",
    "    c7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat7) # Shape : (64, 64, 64)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Conv2D(1, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_uniform')(c7) # Shape : (64, 64, 1)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    \n",
    "    # Output layer\n",
    "    flat = Flatten()(c7) # Shape : 64*64*1\n",
    "    output = Dense(1, activation = 'linear')(flat) # Shape : 1\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    return model\n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return tf.sqrt(mse)\n",
    "    \n",
    "train_inputs = np.load('train_inputs.npy')\n",
    "train_outputs = np.load('train_outputs.npy')\n",
    "val_inputs = np.load('valid_inputs.npy')\n",
    "val_outputs = np.load('valid_outputs.npy')\n",
    "test_inputs = np.load('test_inputs.npy')\n",
    "test_outputs = np.load('test_outputs.npy')\n",
    "\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "check = os.path.join(checkpoint_dir, 'unet_reg_saudi.h5')\n",
    "check_call = ModelCheckpoint(filepath = check, save_weights_only = True, verbose = 1)\n",
    "\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "model = unetreg(input_shape = (64, 64, 4))\n",
    "# model.load_weights('saved_models/unet_reg_saudi.h5')\n",
    "optimizer = Adam(learning_rate = 1e-3)\n",
    "lr_check = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.75, patience = 30, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-6)\n",
    "model.compile(optimizer = optimizer, loss = 'mse')\n",
    "# tf.keras.losses.Huber()\n",
    "model.summary() # Around 5 million params\n",
    "\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 150, steps_per_epoch = len(train_inputs) // 32,\n",
    "                    validation_data = (val_inputs, val_outputs), validation_batch_size = 32,\n",
    "                    callbacks = [lr_check, check_call])\n",
    "\n",
    "test_loss = model.evaluate(test_inputs, test_outputs, batch_size = None)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "del train_inputs, train_outputs, val_inputs, val_outputs\n",
    "\n",
    "predicted_emissions = []\n",
    "true_emissions = []\n",
    "for i in range(len(test_inputs)):\n",
    "    inputs = np.expand_dims(test_inputs[i], axis = 0)\n",
    "    prediction = model.predict(inputs)\n",
    "    true_emissions.append(test_outputs[i])\n",
    "    predicted_emissions.append(prediction)\n",
    "\n",
    "del test_inputs, test_outputs\n",
    "\n",
    "true_emissions = np.array(true_emissions)\n",
    "predicted_emissions = np.array(predicted_emissions)\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape)\n",
    "\n",
    "#csv_file = 'emiss_saudi_unet.csv'\n",
    "#if os.path.exists(csv_file):\n",
    "#    df = pd.read_csv(csv_file)\n",
    "    \n",
    "df = pd.DataFrame({'True Emissions': true_emissions.flatten(), 'Predicted Emissions': predicted_emissions.flatten()})\n",
    "#df['Pred Emiss 2'] = predicted_emissions.flatten()\n",
    "df.to_csv('emiss_saudi_unet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279b0cd-66e8-4598-b15d-e4b10220f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_unet.csv')\n",
    "#  + df['Pred Emiss 3']\n",
    "df['Average Emiss'] = ((df['Predicted Emissions'] + df['Pred Emiss 2'] + df['Pred Emiss 1'])/3).round(3) \n",
    "\n",
    "df['error'] = df['True Emissions'] - df['Average Emiss'] \n",
    "\n",
    "df['absolute_error'] = (abs(df['error'])).round(3)\n",
    "\n",
    "df['relative_error'] = ((df['error'] / (df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "df.to_csv('emiss_saudi_unet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a303c-bac1-495a-bcea-9918bca3b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_unet.csv')\n",
    "\n",
    "percentiles = df['absolute_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 0.264, 50% : 0.568, 75% : 1.005 (All are in Mt/yr)\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fcf35-9042-4e20-a61a-9a45819ca2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_unet.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "percentiles = df['abs_rel_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 21.358, 50% : 46.738, 75% : 89.396 (Unitless)\n",
    "print(percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabbfd2-af8d-4d94-8a4b-0a3682e670d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_unet.csv')\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['absolute_error'], shade = True, color = 'b')\n",
    "plt.title('Kernel Density Estimate of Absolute Error')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0, 25)\n",
    "plt.savefig('Figures/Intern/absolute_error_plot_unet_saudi.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df['relative_error'], shade = True, color = 'b')\n",
    "plt.title('Kernel Density Estimate of Relative Error')\n",
    "plt.xlabel('Relative Error')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(-180, 180)\n",
    "plt.savefig('Figures/Intern/relative_error_plot_unet_saudi.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e373a1-8630-4b51-874b-92434073fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_unet.csv')\n",
    "\n",
    "abs = df['absolute_error']\n",
    "\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 3863\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 243\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 216\n",
    "print(\"above 10 : \", ab10.sum()) # 70\n",
    "print(\"mean : \", mean) # 1.22\n",
    "print(\"median : \", median) # 0.568\n",
    "print(\"std : \", std) # 2.153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc0338-d6f1-4084-884f-98f2a5da3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_unet.csv')\n",
    "\n",
    "rel = df['relative_error']\n",
    "\n",
    "lessneg150 = (rel <= -150)\n",
    "neg150_100 = (rel > -150) & (rel <= -100)\n",
    "neg100_50 = (rel > -100) & (rel <= -50)\n",
    "neg50_0 = (rel > -50) & (rel <= 0)\n",
    "in0_50 = (rel > 0) & (rel <= 50)\n",
    "in50_100 = (rel > 50) & (rel <= 100)\n",
    "ab100 = rel > 100\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"less than - 150% : \", lessneg150.sum()) # 631\n",
    "print(\"in between -150% and -100% : \", neg150_100.sum()) # 350\n",
    "print(\"in between -100% and -50% : \", neg100_50.sum()) # 541\n",
    "print(\"in between -50% and 0% : \", neg50_0.sum()) # 964\n",
    "print(\"in between 0% and 50% : \", in0_50.sum()) # 1333\n",
    "print(\"in between 50% and 100% : \", in50_100.sum()) # 573\n",
    "print(\"above 100% : \", ab100.sum()) # 0\n",
    "print(\"mean : \", mean) # -48.342\n",
    "print(\"median : \", median) # -11.966\n",
    "print(\"std : \", std) # 126.753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f08ae-08a0-41a8-a3fc-16f3ba812251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_unet.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 631\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 350\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 1114\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 1276\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 1021\n",
    "print(\"mean : \", mean) # 81.15\n",
    "print(\"median : \", median) # 46.738\n",
    "print(\"std : \", std) # 108.705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9818d9-f683-4c7c-813e-49ec4ecad2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_unet.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Average Emiss']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 0.065\n",
    "print(\"max : \", max_true) # 22.805\n",
    "print(\"mean : \", mean_true) # 2.043\n",
    "print(\"std : \", std_true) # 2.7\n",
    "print(\"median : \", median_true) # 1.118\n",
    "print(\"range : \", range_true) # 22.74\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 0.37\n",
    "print(\"max : \", max_pred) # 13.024\n",
    "print(\"mean : \", mean_pred) # 1.503\n",
    "print(\"std : \", std_pred) # 0.889\n",
    "print(\"median : \", median_pred) # 1.323\n",
    "print(\"range : \", range_pred) # 12.654\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef9b77-21fb-4d46-a6c8-ea21a98f2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.read_csv('emiss_comb_cnn.csv') # Reading emiss_results\n",
    "df2 = pd.read_csv('emiss_saudi_cnn.csv') # Reading emiss_data\n",
    "df3 = pd.read_csv('emiss_comb_unet.csv')\n",
    "df4 = pd.read_csv('emiss_saudi_unet.csv')\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df1['absolute_error'], fill = True, color = 'b', label = 'Using CNN regression on Combined dataset') # Using seaborn's kdeplot.\n",
    "sns.kdeplot(df2['absolute_error'], fill = True, color = 'r', label = 'Using CNN regression on curated dataset')\n",
    "sns.kdeplot(df3['absolute_error'], fill = True, color = 'g', label = 'Using CNN regression on original dataset')\n",
    "sns.kdeplot(df4['absolute_error'], fill = True, color = 'yellow', label = 'Using U-Net regression on original dataset')\n",
    "plt.title('Kernel Density Estimate of Absolute Error for Lippendorf')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.xlim(0, 25)\n",
    "plt.savefig('absolute_error_comb.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.kdeplot(df1['relative_error'], fill = True, color = 'b', label = 'Using U-Net regression on curated dataset') # Using seaborn's kdeplot.\n",
    "sns.kdeplot(df2['relative_error'], fill = True, color = 'r', label = 'Using CNN regression on curated dataset')\n",
    "sns.kdeplot(df3['relative_error'], fill = True, color = 'g', label = 'Using CNN regression on original dataset')\n",
    "sns.kdeplot(df4['relative_error'], fill = True, color = 'yellow', label = 'Using U-Net regression on original dataset')\n",
    "plt.title('Kernel Density Estimate of Relative Error (%) for Lippendorf')\n",
    "plt.xlabel('Relative Error (%)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.xlim(-180, 180)\n",
    "plt.savefig('relative_error_comb.png', dpi = 300, bbox_inches = 'tight') # Saving the file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f55234-1af0-4646-8576-0ab4f300f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ANNUAL data (Y)\n",
    "Y = np.array([1.93, 0.95, 1.72, 1.34, 0.81, 0.28, 1.01, 3.71, 9.89, 0.86, 1.34, 0.68], dtype = np.float32).reshape(-1, 1)\n",
    "\n",
    "# ANNUAL PRED UNET data (X)\n",
    "X = np.array([1.42, 0.83, 4.60, 2.37, 1.73, 1.76, 2.15, 6.63, 6.90, 1.58, 1.67, 1.61], dtype = np.float32).reshape(-1, 1)\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1]), dtype = np.float32, name = 'weight')\n",
    "b = tf.Variable(tf.random.normal([1]), dtype = np.float32, name = 'bias')\n",
    "\n",
    "def linear_regression(X):\n",
    "    return W * X + b\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate = 0.01)\n",
    "\n",
    "def train_step(X_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = linear_regression(X_batch)\n",
    "        loss = mean_squared_error(y_batch, y_pred)\n",
    "    gradients = tape.gradient(loss, [W, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    return loss\n",
    "\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step(X, Y)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "Y_hat = linear_regression(X).numpy()\n",
    "\n",
    "error_X_Y = np.abs(X.flatten() - Y.flatten())\n",
    "error_Y_Y_hat = np.abs(Y.flatten() - Y_hat.flatten())\n",
    "mean_error_X_Y = np.mean(error_X_Y)\n",
    "mean_error_Y_Y_hat = np.mean(error_Y_Y_hat)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'X (Actual)': X.flatten(),\n",
    "    'Y (Initial)': Y.flatten(),\n",
    "    'Y_hat (Predicted)': Y_hat.flatten(),\n",
    "    'Error X - Y': error_X_Y,\n",
    "    'Error Y - Y_hat': error_Y_Y_hat\n",
    "})\n",
    "\n",
    "print(\"\\nTable of X, Y, and Y_hat:\")\n",
    "print(df)\n",
    "\n",
    "print(f\"\\nMean Error between X and Y: {mean_error_X_Y:.2f}\")\n",
    "print(f\"Mean Error between Y and Y_hat: {mean_error_Y_Y_hat:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc983b-778e-4aeb-9525-ebd81a603e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ANNUAL data (Y)\n",
    "Y = np.array([1.93, 0.95, 1.72, 1.34, 0.81, 0.28, 1.01, 3.71, 9.89, 0.86, 1.34, 0.68], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "# ANNUAL PRED UNET data (X)\n",
    "X = np.array([1.42, 0.83, 4.60, 2.37, 1.73, 1.76, 2.15, 6.63, 6.90, 1.58, 1.67, 1.61], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "# Define the neural network model\n",
    "model_nn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X.shape[1],)),  # Input layer\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # First hidden layer\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Second hidden layer\n",
    "    tf.keras.layers.Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_nn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model_nn.fit(X, Y, epochs = 5000, verbose = 0)\n",
    "\n",
    "# Make predictions\n",
    "Y_hat = model_nn.predict(X)\n",
    "\n",
    "# Calculate errors\n",
    "error_X_Y = np.abs(X.flatten() - Y.flatten())\n",
    "error_Y_Y_hat = np.abs(Y.flatten() - Y_hat.flatten())\n",
    "mean_error_X_Y = np.mean(error_X_Y)\n",
    "mean_error_Y_Y_hat = np.mean(error_Y_Y_hat)\n",
    "\n",
    "# Create DataFrame with errors\n",
    "df = pd.DataFrame({\n",
    "    'X (Actual)': X.flatten(),\n",
    "    'Y (Initial)': Y.flatten(),\n",
    "    'Y_hat (Predicted)': Y_hat.flatten(),\n",
    "    'Absolute Error X - Y': error_X_Y,\n",
    "    'Absolute Error Y - Y_hat': error_Y_Y_hat\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"\\nTable of X, Y, Y_hat with Absolute Errors:\")\n",
    "print(df)\n",
    "\n",
    "# Print mean absolute errors\n",
    "print(f\"\\nMean Absolute Error between X and Y: {mean_error_X_Y:.2f}\")\n",
    "print(f\"Mean Absolute Error between Y and Y_hat: {mean_error_Y_Y_hat:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a05ec-609e-4f4c-9036-cc5c09d15521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "df = pd.read_csv('emiss_saudi_unet.csv')\n",
    "\n",
    "X = df['Pred Emiss 2'].values.reshape(-1, 1)\n",
    "Y = df['True Emissions'].values.reshape(-1, 1)\n",
    "\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size = 0.4, random_state = 42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size = 0.4, random_state = 42)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation = 'relu', input_shape = (X_train.shape[1],)),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(64, activation = 'relu'),\n",
    "    Dense(1)])\n",
    "\n",
    "optimizer = Adam(learning_rate = 0.001)\n",
    "model.compile(optimizer = optimizer, loss = 'mean_squared_error')\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = 100, validation_data = (X_val, Y_val), verbose = 0)\n",
    "\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "results_df = pd.DataFrame({'True Emissions': Y.flatten(), 'Predicted Emissions': Y_pred.flatten()})\n",
    "\n",
    "results_df.to_csv('emiss_saudi_reg.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf11bcb-5781-4c49-a2d2-9009710d87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_inputs = np.load('test_inputs.npy')\n",
    "test_outputs = np.load('test_outputs.npy')\n",
    "\n",
    "shape = test_outputs.shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be7419-6cc1-4234-a6bf-fdd840c2350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_inputs_box = np.load('train_inputs_box.npy')\n",
    "train_inputs_lip = np.load('train_inputs_lip.npy')\n",
    "valid_inputs_box = np.load('valid_inputs_box.npy')\n",
    "valid_inputs_lip = np.load('valid_inputs_lip.npy')\n",
    "test_inputs_box = np.load('test_inputs_box.npy')\n",
    "test_inputs_lip = np.load('test_inputs_lip.npy')\n",
    "\n",
    "train_outputs_box = np.load('train_outputs_box.npy')\n",
    "train_outputs_lip = np.load('train_outputs_lip.npy')\n",
    "valid_outputs_box = np.load('valid_outputs_box.npy')\n",
    "valid_outputs_lip = np.load('valid_outputs_lip.npy')\n",
    "test_outputs_box = np.load('test_outputs_box.npy')\n",
    "test_outputs_lip = np.load('test_outputs_lip.npy')\n",
    "\n",
    "tr_in = np.concatenate((train_inputs_box, train_inputs_lip), axis = 0)\n",
    "va_in = np.concatenate((valid_inputs_box, valid_inputs_lip), axis = 0)\n",
    "te_in = np.concatenate((test_inputs_box, test_inputs_lip), axis = 0)\n",
    "\n",
    "tr_out = np.concatenate((train_outputs_box, train_outputs_lip), axis = 0)\n",
    "va_out = np.concatenate((valid_outputs_box, valid_outputs_lip), axis = 0)\n",
    "te_out = np.concatenate((test_outputs_box, test_outputs_lip), axis = 0)\n",
    "\n",
    "np.save('tr_out.npy', tr_out)\n",
    "np.save('va_out.npy', va_out)\n",
    "np.save('te_out.npy', te_out)\n",
    "\n",
    "np.save('tr_in.npy', tr_in)\n",
    "np.save('va_in.npy', va_in)\n",
    "np.save('te_in.npy', te_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1b416-fb88-4b06-a4b0-73d2485d38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Dropout, BatchNormalization, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# (Number of samples, 64, 64, 4) xco2, no2, u, v\n",
    "# emission rate co2\n",
    "def model_arch(input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1, input_shape = input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation = \"elu\", strides = 1))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2), padding = \"valid\", strides = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(LeakyReLU(alpha = 0.3))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return tf.sqrt(mse)\n",
    "    \n",
    "train_inputs = np.load('tr_in.npy')\n",
    "train_outputs = np.load('tr_out.npy')\n",
    "val_inputs = np.load('va_in.npy')\n",
    "val_outputs = np.load('va_out.npy')\n",
    "test_inputs = np.load('te_in.npy')\n",
    "test_outputs = np.load('te_out.npy')\n",
    "\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "check = os.path.join(checkpoint_dir, 'cnn_reg_sim.h5')\n",
    "check_call = ModelCheckpoint(filepath = check, save_weights_only = True, verbose = 1)\n",
    "\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "model = model_arch(input_shape = (64, 64, 4))\n",
    "# model.load_weights('saved_models/cnn_reg_sim.h5')\n",
    "optimizer = Adam(learning_rate = 1e-3)\n",
    "lr_check = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.75, patience = 20, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-6)\n",
    "model.compile(optimizer = optimizer, loss = tf.keras.losses.Huber())\n",
    "#\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 200, steps_per_epoch = len(train_outputs) // 32,\n",
    "                    validation_data = (val_inputs, val_outputs), validation_batch_size = 32,\n",
    "                    callbacks = [lr_check, check_call])\n",
    "\n",
    "del train_inputs, train_outputs, val_inputs, val_outputs\n",
    "\n",
    "test_loss = model.evaluate(test_inputs, test_outputs, batch_size = None)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "predicted_emissions = []\n",
    "true_emissions = []\n",
    "\n",
    "for i in range(len(test_inputs)):\n",
    "    inputs = np.expand_dims(test_inputs[i], axis = 0)\n",
    "    prediction = model.predict(inputs)\n",
    "    true_emissions.append(test_outputs[i])\n",
    "    predicted_emissions.append(prediction)\n",
    "\n",
    "true_emissions = np.array(true_emissions)\n",
    "predicted_emissions = np.array(predicted_emissions)\n",
    "\n",
    "del test_inputs, test_outputs\n",
    "\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape)\n",
    "\n",
    "csv_file = 'emiss_sim_cnn.csv'\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "#df = pd.DataFrame({'True Emissions': true_emissions.flatten(), 'Predicted Emissions': predicted_emissions.flatten()})\n",
    "df['Pred Emiss 2'] = predicted_emissions.flatten()\n",
    "df.to_csv('emiss_sim_cnn.csv', index = False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691f055-d489-487f-997c-7f58059d5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_cnn.csv')\n",
    "#\n",
    "df['Average Emiss'] = ((df['Predicted Emissions'] + df['Pred Emiss 2'] + df['Pred Emiss 1']  + df['Pred Emiss 3'])/4).round(3) \n",
    "\n",
    "df['error'] = df['True Emissions'] - df['Average Emiss'] \n",
    "\n",
    "df['absolute_error'] = (abs(df['error'])).round(3)\n",
    "\n",
    "df['relative_error'] = ((df['error'] / (df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "df.to_csv('emiss_sim_cnn.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3679be82-1848-475f-912c-44295d639c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_cnn.csv')\n",
    "\n",
    "percentiles = df['absolute_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 1.201, 50% : 2.674, 75% : 4.700 (All are in Mt/yr)\n",
    "print(percentiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d4245-e32e-4737-ba31-75b3b9a37d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_cnn.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "percentiles = df['abs_rel_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 7.367, 50% : 16.011, 75% : 27.763 (Unitless)\n",
    "print(percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a2465-6ff9-4ca3-b4a6-e2a2a25450f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_cnn.csv')\n",
    "\n",
    "abs = df['absolute_error']\n",
    "\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 897\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 904\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 463\n",
    "print(\"above 10 : \", ab10.sum()) # 40\n",
    "print(\"mean : \", mean) # 3.218\n",
    "print(\"median : \", median) # 2.674\n",
    "print(\"std : \", std) # 2.489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec1ea6-8065-4435-84cc-5d3edcf32f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_cnn.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 0\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 4\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 128\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 791\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 1381\n",
    "print(\"mean : \", mean) # 19.966\n",
    "print(\"median : \", median) # 16.011\n",
    "print(\"std : \", std) # 17.098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2efc4c-dbda-4105-83f7-2cdc027e8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_cnn.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Average Emiss']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 7.493\n",
    "print(\"max : \", max_true) # 30.114\n",
    "print(\"mean : \", mean_true) # 17.09\n",
    "print(\"std : \", std_true) # 4.517\n",
    "print(\"median : \", median_true) # 16.725\n",
    "print(\"range : \", range_true) # 22.621\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 14.071\n",
    "print(\"max : \", max_pred) # 20.571\n",
    "print(\"mean : \", mean_pred) # 16.614\n",
    "print(\"std : \", std_pred) # 1.735\n",
    "print(\"median : \", median_pred) # 16.412\n",
    "print(\"range : \", range_pred) # 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733f619-ef89-4fb8-962b-018bc518d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Dropout, BatchNormalization, MaxPool2D, Flatten, Dense, Conv2DTranspose, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def unetreg(input_shape, dropout_rate = 0.2):\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "    \n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(inputs) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    c1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c1) # Shape : (64, 64, 64)\n",
    "    c1 = BatchNormalization()(c1)\n",
    "    d1 = Dropout(dropout_rate)(c1) \n",
    "    p1 = MaxPool2D(pool_size = (2, 2))(d1) # Shape : (32, 32, 64) \n",
    "    \n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p1) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    c2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c2) # Shape : (32, 32, 128)\n",
    "    c2 = BatchNormalization()(c2)\n",
    "    d2 = Dropout(dropout_rate)(c2)\n",
    "    p2 = MaxPool2D(pool_size = (2, 2))(d2) # Shape : (16, 16, 128)\n",
    "    \n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p2) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    c3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c3) # Shape : (16, 16, 256)\n",
    "    c3 = BatchNormalization()(c3)\n",
    "    d3 = Dropout(dropout_rate)(c3)\n",
    "    p3 = MaxPool2D(pool_size = (2, 2))(d3) # Shape : (8, 8, 256)\n",
    "\n",
    "    # Bottleneck\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(p3) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    c4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c4) # Shape : (8, 8, 512)\n",
    "    c4 = BatchNormalization()(c4)\n",
    "    d4 = Dropout(dropout_rate)(c4)\n",
    "    \n",
    "    # Decoder (upsampling path)\n",
    "    u5 = Conv2DTranspose(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d4) # Shape : (8, 8, 256)\n",
    "    u5 = UpSampling2D((2, 2))(u5) # Shape : (16, 16, 256)\n",
    "    concat5 = Concatenate()([u5, c3]) # Shape : (16, 16, 256)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    c5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c5) # Shape : (16, 16, 256)\n",
    "    c5 = BatchNormalization()(c5)\n",
    "    d5 = Dropout(dropout_rate)(c5)\n",
    "    \n",
    "    u6 = Conv2DTranspose(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d5) # Shape : (16, 16, 128)\n",
    "    u6 = UpSampling2D((2, 2))(u6) # Shape : (32, 32, 128)\n",
    "    concat6 = Concatenate()([u6, c2]) # Shape : (32, 32, 128)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    c6 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(c6) # Shape : (32, 32, 128)\n",
    "    c6 = BatchNormalization()(c6)\n",
    "    d6 = Dropout(dropout_rate)(c6)\n",
    "    \n",
    "    u7 = Conv2DTranspose(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(d6) # Shape : (32, 32, 64)\n",
    "    u7 = UpSampling2D((2, 2))(u7) # Shape : (64, 64, 64)\n",
    "    concat7 = Concatenate()([u7, c1])  # Shape : (64, 64, 64)\n",
    "    c7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_uniform')(concat7) # Shape : (64, 64, 64)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    c7 = Conv2D(1, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_uniform')(c7) # Shape : (64, 64, 1)\n",
    "    c7 = BatchNormalization()(c7)\n",
    "    \n",
    "    # Output layer\n",
    "    flat = Flatten()(c7) # Shape : 64*64*1\n",
    "    output = Dense(1, activation = 'linear')(flat) # Shape : 1\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    return model\n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return tf.sqrt(mse)\n",
    "    \n",
    "train_inputs = np.load('tr_in.npy')\n",
    "train_outputs = np.load('tr_out.npy')\n",
    "val_inputs = np.load('va_in.npy')\n",
    "val_outputs = np.load('va_out.npy')\n",
    "test_inputs = np.load('te_in.npy')\n",
    "test_outputs = np.load('te_out.npy')\n",
    "\n",
    "checkpoint_dir = './saved_models/' \n",
    "os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "\n",
    "check = os.path.join(checkpoint_dir, 'unet_reg_sim.h5')\n",
    "check_call = ModelCheckpoint(filepath = check, save_weights_only = True, verbose = 1)\n",
    "\n",
    "data_augmentation = ImageDataGenerator(rotation_range = 180, width_shift_range = 0.0, height_shift_range = 0.0, shear_range = 90, zoom_range = 0.2, horizontal_flip = True, vertical_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "model = unetreg(input_shape = (64, 64, 4))\n",
    "# model.load_weights('saved_models/unet_reg_sim.h5')\n",
    "optimizer = Adam(learning_rate = 1e-3)\n",
    "lr_check = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.75, patience = 30, verbose = 1, min_delta = 5e-3, cooldown = 0, min_lr = 5e-6)\n",
    "model.compile(optimizer = optimizer, loss = tf.keras.losses.Huber())\n",
    "# tf.keras.losses.Huber()\n",
    "model.summary() # Around 5 million params\n",
    "\n",
    "history = model.fit(data_augmentation.flow(train_inputs, train_outputs, batch_size = 32, shuffle = True), epochs = 150, steps_per_epoch = len(train_inputs) // 32,\n",
    "                    validation_data = (val_inputs, val_outputs), validation_batch_size = 32,\n",
    "                    callbacks = [lr_check, check_call])\n",
    "\n",
    "test_loss = model.evaluate(test_inputs, test_outputs, batch_size = None)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "del train_inputs, train_outputs, val_inputs, val_outputs\n",
    "\n",
    "predicted_emissions = []\n",
    "true_emissions = []\n",
    "for i in range(len(test_inputs)):\n",
    "    inputs = np.expand_dims(test_inputs[i], axis = 0)\n",
    "    prediction = model.predict(inputs)\n",
    "    true_emissions.append(test_outputs[i])\n",
    "    predicted_emissions.append(prediction)\n",
    "\n",
    "del test_inputs, test_outputs\n",
    "\n",
    "true_emissions = np.array(true_emissions)\n",
    "predicted_emissions = np.array(predicted_emissions)\n",
    "true_emissions = true_emissions.reshape(predicted_emissions.shape)\n",
    "\n",
    "#csv_file = 'emiss_sim_unet.csv'\n",
    "#if os.path.exists(csv_file):\n",
    "#    df = pd.read_csv(csv_file)\n",
    "    \n",
    "df = pd.DataFrame({'True Emissions': true_emissions.flatten(), 'Predicted Emissions': predicted_emissions.flatten()})\n",
    "#df['Pred Emiss 2'] = predicted_emissions.flatten()\n",
    "df.to_csv('emiss_sim_unet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07105af3-d94b-4417-948c-95a3b0caebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_unet.csv')\n",
    "# \n",
    "df['Average Emiss'] = ((df['Predicted Emissions'] + df['Pred Emiss 2'] + + df['Pred Emiss 3']+ df['Pred Emiss 1'])/4).round(3) \n",
    "\n",
    "df['error'] = df['True Emissions'] - df['Average Emiss'] \n",
    "\n",
    "df['absolute_error'] = (abs(df['error'])).round(3)\n",
    "\n",
    "df['relative_error'] = ((df['error'] / (df['True Emissions'] + 1e-15)) * 100).round(3)\n",
    "\n",
    "df.to_csv('emiss_sim_unet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d62f6b-0358-41dd-af9a-d0d835fa2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('emiss_sim_unet.csv')\n",
    "\n",
    "percentiles = df['absolute_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 1.129, 50% : 2.351, 75% : 3.997 (All are in Mt/yr)\n",
    "print(percentiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110edea-5cd6-4b17-a9a5-f96738753dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_unet.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "percentiles = df['abs_rel_error'].describe(percentiles = [0.25, 0.5, 0.75])\n",
    "percentiles = percentiles.loc[['25%', '50%', '75%']].apply(abs) # 25% : 7.084, 50% : 14.141, 75% : 23.980 (Unitless)\n",
    "print(percentiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc15cfd-75e9-46e2-8181-246b1c372984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_unet.csv')\n",
    "\n",
    "abs = df['absolute_error']\n",
    "\n",
    "in0_2 = (abs >= 0) & (abs <= 2)\n",
    "in2_5 = (abs > 2) & (abs <= 5)\n",
    "in5_10 = (abs > 5) & (abs <= 10)\n",
    "ab10 = abs > 10\n",
    "mean = round(abs.mean(), 3)\n",
    "median = round(abs.median(), 3)\n",
    "std = round(abs.std(), 3)\n",
    "\n",
    "print(\"in between 0 and 2 : \", in0_2.sum()) # 1016\n",
    "print(\"in between 2 and 5 : \", in2_5.sum()) # 914\n",
    "print(\"in between 5 and 10 : \", in5_10.sum()) # 340\n",
    "print(\"above 10 : \", ab10.sum()) # 34\n",
    "print(\"mean : \", mean) # 2.896\n",
    "print(\"median : \", median) # 2.352\n",
    "print(\"std : \", std) # 2.361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4418d-5126-4fe2-a946-740a05f6a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_unet.csv')\n",
    "df['abs_rel_error'] = abs(df['relative_error'])\n",
    "\n",
    "rel = df['abs_rel_error']\n",
    "\n",
    "\n",
    "above150 = (rel > 150)\n",
    "in150_100 = (rel > 100) & (rel <= 150)\n",
    "in100_50 = (rel > 50) & (rel <= 100)\n",
    "in50_20 = (rel > 20) & (rel <= 50)\n",
    "in20_0 = (rel > 0) & (rel <= 20)\n",
    "\n",
    "mean = round(rel.mean(), 3)\n",
    "median = round(rel.median(), 3)\n",
    "std = round(rel.std(), 3)\n",
    "\n",
    "print(\"above 150% : \", above150.sum()) # 0\n",
    "print(\"in between 100% and 150% : \", in150_100.sum()) # 3\n",
    "print(\"in between 50% and 100% : \", in100_50.sum()) # 40\n",
    "print(\"in between 20% and 50% : \", in50_20.sum()) # 740\n",
    "print(\"in between 0% and 20% : \", in20_0.sum()) # 1521\n",
    "print(\"mean : \", mean) # 16.907\n",
    "print(\"median : \", median) # 14.141\n",
    "print(\"std : \", std) # 13.277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66088181-1859-4025-b329-05320aa9cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('emiss_sim_unet.csv')\n",
    "\n",
    "true = df['True Emissions']\n",
    "min_true = round(true.min(), 3)\n",
    "max_true = round(true.max(), 3)\n",
    "mean_true = round(true.mean(), 3)\n",
    "std_true = round(true.std(), 3)\n",
    "median_true = round(true.median(), 3)\n",
    "range_true = round(max_true-min_true, 3)\n",
    "\n",
    "pred = df['Average Emiss']\n",
    "min_pred = round(pred.min(), 3)\n",
    "max_pred = round(pred.max(), 3)\n",
    "mean_pred = round(pred.mean(), 3)\n",
    "std_pred = round(pred.std(), 3)\n",
    "median_pred = round(pred.median(), 3)\n",
    "range_pred = round(max_pred-min_pred, 3)\n",
    "\n",
    "print(\"True statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_true) # 7.493\n",
    "print(\"max : \", max_true) # 30.114\n",
    "print(\"mean : \", mean_true) # 17.09\n",
    "print(\"std : \", std_true) # 4.517\n",
    "print(\"median : \", median_true) # 16.725\n",
    "print(\"range : \", range_true) # 22.621\n",
    "print()\n",
    "print(\"Predicted statistics : \")\n",
    "print()\n",
    "print(\"min : \", min_pred) # 9.528\n",
    "print(\"max : \", max_pred) # 24.886\n",
    "print(\"mean : \", mean_pred) # 15.819\n",
    "print(\"std : \", std_pred) # 2.28\n",
    "print(\"median : \", median_pred) # 15.564\n",
    "print(\"range : \", range_pred) # 15.358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f0a70-3b64-4373-83d4-ebeb4c783995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
